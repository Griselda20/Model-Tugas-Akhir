{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vp4-hpoJ0C99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87abc50d-dd72-4790-c65f-f13e9cbd05aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-05 02:19:36--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs231n.stanford.edu/tiny-imagenet-200.zip [following]\n",
            "--2025-01-05 02:19:36--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  18.8MB/s    in 10s     \n",
            "\n",
            "2025-01-05 02:19:47 (22.9 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (11.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=b74a935f6fdc180173b01d44357399d279d7f5dc8c3bf1f48ab94f57a76d71f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=7bc9cd54dc5aad9e4657db77cb51df1eaea1d2cd64239c4030b155aa200327c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip\n",
        "!pip install fvcore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import functional as F, ToPILImage\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset, random_split, Subset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import os, glob\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "4dVE5NtM2D6V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "else:\n",
        "    print(\"Using CPU\")"
      ],
      "metadata": {
        "id": "P2lUMjFt2wcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc693a7-9b06-4137-9401-faf33def5701"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_dict = {}\n",
        "for i, line in enumerate(open('/content/tiny-imagenet-200/wnids.txt', 'r')):\n",
        "  id_dict[line.replace('\\n', '')] = i\n",
        "\n",
        "class TrainTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, id, transform=None):\n",
        "        self.filenames = glob.glob(\"/content/tiny-imagenet-200/train/*/*/*.JPEG\")\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = read_image(img_path)\n",
        "        if image.shape[0] == 1:\n",
        "          image = read_image(img_path,ImageReadMode.RGB)\n",
        "        label = self.id_dict[img_path.split('/')[4]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image.type(torch.FloatTensor))\n",
        "        return image, label\n",
        "\n",
        "class TestTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, id, transform=None):\n",
        "        self.filenames = glob.glob(\"/content/tiny-imagenet-200/val/images/*.JPEG\")\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "        self.cls_dic = {}\n",
        "        for i, line in enumerate(open('/content/tiny-imagenet-200/val/val_annotations.txt', 'r')):\n",
        "            a = line.split('\\t')\n",
        "            img, cls_id = a[0],a[1]\n",
        "            self.cls_dic[img] = self.id_dict[cls_id]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = read_image(img_path)\n",
        "        if image.shape[0] == 1:\n",
        "          image = read_image(img_path,ImageReadMode.RGB)\n",
        "        label = self.cls_dic[img_path.split('/')[-1]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image.type(torch.FloatTensor))\n",
        "        return image, label\n",
        "\n",
        "transform = transforms.Normalize((122.4786, 114.2755, 101.3963), (70.4924, 68.5679, 71.8127))\n",
        "\n",
        "train_dataset = TrainTinyImageNetDataset(id=id_dict, transform = transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = TestTinyImageNetDataset(id=id_dict, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "WVdMxSTQTZd3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################## Coatnet #########################\n",
        "\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
        "    stride = 1 if downsample == False else 2\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.GELU()\n",
        "    )\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, norm):\n",
        "        super().__init__()\n",
        "        self.norm = norm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class SE(nn.Module):\n",
        "    def __init__(self, inp, oup, expansion=0.25):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        stride = 1 if self.downsample == False else 2\n",
        "        hidden_dim = int(inp * expansion)\n",
        "\n",
        "        if self.downsample:\n",
        "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "\n",
        "        if expansion == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
        "                          1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                # down-sample in the first conv\n",
        "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
        "                          groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                SE(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.downsample:\n",
        "            return self.proj(self.pool(x)) + self.conv(x)\n",
        "        else:\n",
        "            return x + self.conv(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == inp)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        # parameter table of relative position bias\n",
        "        self.relative_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
        "\n",
        "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "\n",
        "        relative_coords[0] += self.ih - 1\n",
        "        relative_coords[1] += self.iw - 1\n",
        "        relative_coords[0] *= 2 * self.iw - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, oup),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(\n",
        "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        # Use \"gather\" for more efficiency on GPUs\n",
        "        relative_bias = self.relative_bias_table.gather(\n",
        "            0, self.relative_index.repeat(1, self.heads))\n",
        "        relative_bias = rearrange(\n",
        "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
        "        dots = dots + relative_bias\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(inp * 4)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if self.downsample:\n",
        "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
        "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "\n",
        "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
        "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
        "\n",
        "        self.attn = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "        )\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.downsample:\n",
        "            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
        "        else:\n",
        "            x = x + self.attn(x)\n",
        "        x = x + self.ff(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CoAtNet(nn.Module):\n",
        "    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n",
        "        super().__init__()\n",
        "        ih, iw = image_size\n",
        "        block = {'C': MBConv, 'T': Transformer}\n",
        "\n",
        "        self.s0 = self._make_layer(\n",
        "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
        "        self.s1 = self._make_layer(\n",
        "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
        "        self.s2 = self._make_layer(\n",
        "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
        "        self.s3 = self._make_layer(\n",
        "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
        "        self.s4 = self._make_layer(\n",
        "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
        "\n",
        "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.s0(x)\n",
        "        x = self.s1(x)\n",
        "        x = self.s2(x)\n",
        "        x = self.s3(x)\n",
        "        x = self.s4(x)\n",
        "\n",
        "        x = self.pool(x).view(-1, x.shape[1])\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
        "        layers = nn.ModuleList([])\n",
        "        for i in range(depth):\n",
        "            if i == 0:\n",
        "                layers.append(block(inp, oup, image_size, downsample=True))\n",
        "            else:\n",
        "                layers.append(block(oup, oup, image_size))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def coatnet_0():\n",
        "    num_blocks = [2, 2, 3, 5, 2]            # L\n",
        "    channels = [64, 96, 192, 384, 768]      # D\n",
        "    return CoAtNet((64, 64), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_1():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [64, 96, 192, 384, 768]      # D\n",
        "    return CoAtNet((64, 64), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_2():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [128, 128, 256, 512, 1026]   # D\n",
        "    return CoAtNet((64, 64), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_3():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [192, 192, 384, 768, 1536]   # D\n",
        "    return CoAtNet((64, 64), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_4():\n",
        "    num_blocks = [2, 2, 12, 28, 2]          # L\n",
        "    channels = [192, 192, 384, 768, 1536]   # D\n",
        "    return CoAtNet((64, 64), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    img = torch.randn(1, 3, 64, 64)\n",
        "\n",
        "    net = coatnet_0()\n",
        "    out = net(img)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatnet_1()\n",
        "    out = net(img)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatnet_2()\n",
        "    out = net(img)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatnet_3()\n",
        "    out = net(img)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatnet_4()\n",
        "    out = net(img)\n",
        "    print(out.shape, count_parameters(net))\n"
      ],
      "metadata": {
        "id": "_xhZNoPD3VfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade2db0b-4cce-4e95-abb5-fcb61f0bbffa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000]) 17759864\n",
            "torch.Size([1, 1000]) 33091904\n",
            "torch.Size([1, 1000]) 55688844\n",
            "torch.Size([1, 1000]) 117645760\n",
            "torch.Size([1, 1000]) 203805488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 200\n",
        "\n",
        "model = coatnet_3()\n",
        "model.num_classes = num_classes\n",
        "\n",
        "# Sesuaikan dengan ukuran gambar dataset 64 or 224\n",
        "x = torch.randn(1, 3, 64, 64)\n",
        "\n",
        "model = model.to(device)\n",
        "x = x.to(device)\n",
        "output = model(x)\n",
        "\n",
        "print(output.shape)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "TE4yfNSWQFxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19132caa-8879-46b2-934f-d76414bea83e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000])\n",
            "CoAtNet(\n",
            "  (s0): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(3, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "    )\n",
            "  )\n",
            "  (s1): Sequential(\n",
            "    (0): MBConv(\n",
            "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (proj): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (conv): PreNorm(\n",
            "        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (fn): Sequential(\n",
            "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): SE(\n",
            "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=48, bias=False)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Linear(in_features=48, out_features=768, bias=False)\n",
            "              (3): Sigmoid()\n",
            "            )\n",
            "          )\n",
            "          (7): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): MBConv(\n",
            "      (conv): PreNorm(\n",
            "        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (fn): Sequential(\n",
            "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): SE(\n",
            "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=48, bias=False)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Linear(in_features=48, out_features=768, bias=False)\n",
            "              (3): Sigmoid()\n",
            "            )\n",
            "          )\n",
            "          (7): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (s2): Sequential(\n",
            "    (0): MBConv(\n",
            "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (proj): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (conv): PreNorm(\n",
            "        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (fn): Sequential(\n",
            "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): SE(\n",
            "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=48, bias=False)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Linear(in_features=48, out_features=768, bias=False)\n",
            "              (3): Sigmoid()\n",
            "            )\n",
            "          )\n",
            "          (7): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (8): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): MBConv(\n",
            "      (conv): PreNorm(\n",
            "        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (fn): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
            "          (4): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): SE(\n",
            "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc): Sequential(\n",
            "              (0): Linear(in_features=1536, out_features=96, bias=False)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Linear(in_features=96, out_features=1536, bias=False)\n",
            "              (3): Sigmoid()\n",
            "            )\n",
            "          )\n",
            "          (7): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (8): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): MBConv(\n",
            "      (conv): PreNorm(\n",
            "        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (fn): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
            "          (4): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): SE(\n",
            "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc): Sequential(\n",
            "              (0): Linear(in_features=1536, out_features=96, bias=False)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Linear(in_features=96, out_features=1536, bias=False)\n",
            "              (3): Sigmoid()\n",
            "            )\n",
            "          )\n",
            "          (7): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (8): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): MBConv(\n",
            "      (conv): PreNorm(\n",
            "        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (fn): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
            "          (4): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): SE(\n",
            "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc): Sequential(\n",
            "              (0): Linear(in_features=1536, out_features=96, bias=False)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Linear(in_features=96, out_features=1536, bias=False)\n",
            "              (3): Sigmoid()\n",
            "            )\n",
            "          )\n",
            "          (7): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (8): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (4): MBConv(\n",
            "      (conv): PreNorm(\n",
            "        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (fn): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
            "          (4): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): SE(\n",
            "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc): Sequential(\n",
            "              (0): Linear(in_features=1536, out_features=96, bias=False)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Linear(in_features=96, out_features=1536, bias=False)\n",
            "              (3): Sigmoid()\n",
            "            )\n",
            "          )\n",
            "          (7): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (8): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (5): MBConv(\n",
            "      (conv): PreNorm(\n",
            "        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (fn): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
            "          (4): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): SE(\n",
            "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "            (fc): Sequential(\n",
            "              (0): Linear(in_features=1536, out_features=96, bias=False)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Linear(in_features=96, out_features=1536, bias=False)\n",
            "              (3): Sigmoid()\n",
            "            )\n",
            "          )\n",
            "          (7): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (8): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (s3): Sequential(\n",
            "    (0): Transformer(\n",
            "      (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (pool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (proj): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=384, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=1536, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=1536, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (1): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (2): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (3): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (4): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (5): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (6): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (7): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (8): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (9): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (10): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (11): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (12): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "    (13): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=768, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=4, iw=4)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (s4): Sequential(\n",
            "    (0): Transformer(\n",
            "      (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (pool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (proj): Conv2d(768, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=1536, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=2, iw=2)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=1536, out_features=3072, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=3072, out_features=1536, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=2, iw=2)\n",
            "      )\n",
            "    )\n",
            "    (1): Transformer(\n",
            "      (attn): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): Attention(\n",
            "            (attend): Softmax(dim=-1)\n",
            "            (to_qkv): Linear(in_features=1536, out_features=768, bias=False)\n",
            "            (to_out): Sequential(\n",
            "              (0): Linear(in_features=256, out_features=1536, bias=True)\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=2, iw=2)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Rearrange('b c ih iw -> b (ih iw) c')\n",
            "        (1): PreNorm(\n",
            "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (net): Sequential(\n",
            "              (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): Dropout(p=0.0, inplace=False)\n",
            "              (3): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "              (4): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): Rearrange('b (ih iw) c -> b c ih iw', ih=2, iw=2)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pool): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "  (fc): Linear(in_features=1536, out_features=1000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "lr_val = 0.001\n",
        "patience = 10\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr_val, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "best_model_path = 'best_coatnet_model.pth'\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "train_losses, train_acc, val_losses, val_acc = [], [], [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=\"Train Progress\", leave=True)\n",
        "\n",
        "    for inputs, labels in train_loader_tqdm:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct_train += predicted.eq(labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    test_loader_tqdm = tqdm(test_loader, desc=\"Validation Progress\", leave=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader_tqdm:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct_val += predicted.eq(labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "            test_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "    val_loss /= len(test_loader.dataset)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_acc.append(train_accuracy)\n",
        "    val_acc.append(val_accuracy)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Validation loss improved. Saving model to {best_model_path}.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement in validation loss. Counter: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print(f\"Training complete. Best model loaded from {best_model_path}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRLXFsss3Kyj",
        "outputId": "32c5bf12-942d-4952-90bf-a97768c2e108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Progress:  11%|█         | 11107/100000 [19:48<2:56:05,  8.41it/s, loss=2.19e+3]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBrJczYh5sye",
        "outputId": "11bc6c1a-d6d3-4530-d933-8afb06a56d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "test_loss = 0.0\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "\n",
        "test_loader_tqdm = tqdm(test_loader, desc=\"Test Progress\", leave=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader_tqdm:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct_test += predicted.eq(labels).sum().item()\n",
        "        total_test += labels.size(0)\n",
        "\n",
        "        test_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Top-1 Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "# FLOPs calculation\n",
        "dummy_input = torch.randn(1, 3, 64, 64).to(device)\n",
        "\n",
        "flops = FlopCountAnalysis(model, dummy_input)\n",
        "print(f\"FLOPs: {flops.total()}\")\n",
        "\n",
        "# print FLOPs per layer\n",
        "for name, count in flops.by_module().items():\n",
        "    print(f\"{name}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7S02k1QCGwU",
        "outputId": "46319cca-e935-4698-d228-d3f889b57cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test Progress: 100%|██████████| 157/157 [00:18<00:00,  8.38it/s, loss=4.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 5.2174, Top-1 Accuracy: 4.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 42 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 6 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sigmoid encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 184 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 56 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul_ encountered 48 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::repeat encountered 16 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::softmax encountered 16 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::avg_pool2d encountered 1 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 2261629952\n",
            ": 2261629952\n",
            "s0: 345833472\n",
            "s0.0: 5701632\n",
            "s0.0.0: 5308416\n",
            "s0.0.1: 393216\n",
            "s0.0.2: 0\n",
            "s0.1: 340131840\n",
            "s0.1.0: 339738624\n",
            "s0.1.1: 393216\n",
            "s0.1.2: 0\n",
            "s1: 166772736\n",
            "s1.0: 88252416\n",
            "s1.0.pool: 0\n",
            "s1.0.proj: 9437184\n",
            "s1.0.conv: 78815232\n",
            "s1.0.conv.norm: 393216\n",
            "s1.0.conv.fn: 78422016\n",
            "s1.0.conv.fn.0: 37748736\n",
            "s1.0.conv.fn.1: 393216\n",
            "s1.0.conv.fn.2: 0\n",
            "s1.0.conv.fn.3: 1769472\n",
            "s1.0.conv.fn.4: 393216\n",
            "s1.0.conv.fn.5: 0\n",
            "s1.0.conv.fn.6: 270336\n",
            "s1.0.conv.fn.6.avg_pool: 196608\n",
            "s1.0.conv.fn.6.fc: 73728\n",
            "s1.0.conv.fn.6.fc.0: 36864\n",
            "s1.0.conv.fn.6.fc.1: 0\n",
            "s1.0.conv.fn.6.fc.2: 36864\n",
            "s1.0.conv.fn.6.fc.3: 0\n",
            "s1.0.conv.fn.7: 37748736\n",
            "s1.0.conv.fn.8: 98304\n",
            "s1.1: 78520320\n",
            "s1.1.conv: 78520320\n",
            "s1.1.conv.norm: 98304\n",
            "s1.1.conv.fn: 78422016\n",
            "s1.1.conv.fn.0: 37748736\n",
            "s1.1.conv.fn.1: 393216\n",
            "s1.1.conv.fn.2: 0\n",
            "s1.1.conv.fn.3: 1769472\n",
            "s1.1.conv.fn.4: 393216\n",
            "s1.1.conv.fn.5: 0\n",
            "s1.1.conv.fn.6: 270336\n",
            "s1.1.conv.fn.6.avg_pool: 196608\n",
            "s1.1.conv.fn.6.fc: 73728\n",
            "s1.1.conv.fn.6.fc.0: 36864\n",
            "s1.1.conv.fn.6.fc.1: 0\n",
            "s1.1.conv.fn.6.fc.2: 36864\n",
            "s1.1.conv.fn.6.fc.3: 0\n",
            "s1.1.conv.fn.7: 37748736\n",
            "s1.1.conv.fn.8: 98304\n",
            "s2: 420274176\n",
            "s2.0: 33939456\n",
            "s2.0.pool: 0\n",
            "s2.0.proj: 4718592\n",
            "s2.0.conv: 29220864\n",
            "s2.0.conv.norm: 98304\n",
            "s2.0.conv.fn: 29122560\n",
            "s2.0.conv.fn.0: 9437184\n",
            "s2.0.conv.fn.1: 98304\n",
            "s2.0.conv.fn.2: 0\n",
            "s2.0.conv.fn.3: 442368\n",
            "s2.0.conv.fn.4: 98304\n",
            "s2.0.conv.fn.5: 0\n",
            "s2.0.conv.fn.6: 122880\n",
            "s2.0.conv.fn.6.avg_pool: 49152\n",
            "s2.0.conv.fn.6.fc: 73728\n",
            "s2.0.conv.fn.6.fc.0: 36864\n",
            "s2.0.conv.fn.6.fc.1: 0\n",
            "s2.0.conv.fn.6.fc.2: 36864\n",
            "s2.0.conv.fn.6.fc.3: 0\n",
            "s2.0.conv.fn.7: 18874368\n",
            "s2.0.conv.fn.8: 49152\n",
            "s2.1: 77266944\n",
            "s2.1.conv: 77266944\n",
            "s2.1.conv.norm: 49152\n",
            "s2.1.conv.fn: 77217792\n",
            "s2.1.conv.fn.0: 37748736\n",
            "s2.1.conv.fn.1: 196608\n",
            "s2.1.conv.fn.2: 0\n",
            "s2.1.conv.fn.3: 884736\n",
            "s2.1.conv.fn.4: 196608\n",
            "s2.1.conv.fn.5: 0\n",
            "s2.1.conv.fn.6: 393216\n",
            "s2.1.conv.fn.6.avg_pool: 98304\n",
            "s2.1.conv.fn.6.fc: 294912\n",
            "s2.1.conv.fn.6.fc.0: 147456\n",
            "s2.1.conv.fn.6.fc.1: 0\n",
            "s2.1.conv.fn.6.fc.2: 147456\n",
            "s2.1.conv.fn.6.fc.3: 0\n",
            "s2.1.conv.fn.7: 37748736\n",
            "s2.1.conv.fn.8: 49152\n",
            "s2.2: 77266944\n",
            "s2.2.conv: 77266944\n",
            "s2.2.conv.norm: 49152\n",
            "s2.2.conv.fn: 77217792\n",
            "s2.2.conv.fn.0: 37748736\n",
            "s2.2.conv.fn.1: 196608\n",
            "s2.2.conv.fn.2: 0\n",
            "s2.2.conv.fn.3: 884736\n",
            "s2.2.conv.fn.4: 196608\n",
            "s2.2.conv.fn.5: 0\n",
            "s2.2.conv.fn.6: 393216\n",
            "s2.2.conv.fn.6.avg_pool: 98304\n",
            "s2.2.conv.fn.6.fc: 294912\n",
            "s2.2.conv.fn.6.fc.0: 147456\n",
            "s2.2.conv.fn.6.fc.1: 0\n",
            "s2.2.conv.fn.6.fc.2: 147456\n",
            "s2.2.conv.fn.6.fc.3: 0\n",
            "s2.2.conv.fn.7: 37748736\n",
            "s2.2.conv.fn.8: 49152\n",
            "s2.3: 77266944\n",
            "s2.3.conv: 77266944\n",
            "s2.3.conv.norm: 49152\n",
            "s2.3.conv.fn: 77217792\n",
            "s2.3.conv.fn.0: 37748736\n",
            "s2.3.conv.fn.1: 196608\n",
            "s2.3.conv.fn.2: 0\n",
            "s2.3.conv.fn.3: 884736\n",
            "s2.3.conv.fn.4: 196608\n",
            "s2.3.conv.fn.5: 0\n",
            "s2.3.conv.fn.6: 393216\n",
            "s2.3.conv.fn.6.avg_pool: 98304\n",
            "s2.3.conv.fn.6.fc: 294912\n",
            "s2.3.conv.fn.6.fc.0: 147456\n",
            "s2.3.conv.fn.6.fc.1: 0\n",
            "s2.3.conv.fn.6.fc.2: 147456\n",
            "s2.3.conv.fn.6.fc.3: 0\n",
            "s2.3.conv.fn.7: 37748736\n",
            "s2.3.conv.fn.8: 49152\n",
            "s2.4: 77266944\n",
            "s2.4.conv: 77266944\n",
            "s2.4.conv.norm: 49152\n",
            "s2.4.conv.fn: 77217792\n",
            "s2.4.conv.fn.0: 37748736\n",
            "s2.4.conv.fn.1: 196608\n",
            "s2.4.conv.fn.2: 0\n",
            "s2.4.conv.fn.3: 884736\n",
            "s2.4.conv.fn.4: 196608\n",
            "s2.4.conv.fn.5: 0\n",
            "s2.4.conv.fn.6: 393216\n",
            "s2.4.conv.fn.6.avg_pool: 98304\n",
            "s2.4.conv.fn.6.fc: 294912\n",
            "s2.4.conv.fn.6.fc.0: 147456\n",
            "s2.4.conv.fn.6.fc.1: 0\n",
            "s2.4.conv.fn.6.fc.2: 147456\n",
            "s2.4.conv.fn.6.fc.3: 0\n",
            "s2.4.conv.fn.7: 37748736\n",
            "s2.4.conv.fn.8: 49152\n",
            "s2.5: 77266944\n",
            "s2.5.conv: 77266944\n",
            "s2.5.conv.norm: 49152\n",
            "s2.5.conv.fn: 77217792\n",
            "s2.5.conv.fn.0: 37748736\n",
            "s2.5.conv.fn.1: 196608\n",
            "s2.5.conv.fn.2: 0\n",
            "s2.5.conv.fn.3: 884736\n",
            "s2.5.conv.fn.4: 196608\n",
            "s2.5.conv.fn.5: 0\n",
            "s2.5.conv.fn.6: 393216\n",
            "s2.5.conv.fn.6.avg_pool: 98304\n",
            "s2.5.conv.fn.6.fc: 294912\n",
            "s2.5.conv.fn.6.fc.0: 147456\n",
            "s2.5.conv.fn.6.fc.1: 0\n",
            "s2.5.conv.fn.6.fc.2: 147456\n",
            "s2.5.conv.fn.6.fc.3: 0\n",
            "s2.5.conv.fn.7: 37748736\n",
            "s2.5.conv.fn.8: 49152\n",
            "s3: 1198901248\n",
            "s3.0: 50554880\n",
            "s3.0.pool1: 0\n",
            "s3.0.pool2: 0\n",
            "s3.0.proj: 4718592\n",
            "s3.0.attn: 8026112\n",
            "s3.0.attn.0: 0\n",
            "s3.0.attn.1: 8026112\n",
            "s3.0.attn.1.norm: 30720\n",
            "s3.0.attn.1.fn: 7995392\n",
            "s3.0.attn.1.fn.attend: 0\n",
            "s3.0.attn.1.fn.to_qkv: 4718592\n",
            "s3.0.attn.1.fn.to_out: 3145728\n",
            "s3.0.attn.1.fn.to_out.0: 3145728\n",
            "s3.0.attn.1.fn.to_out.1: 0\n",
            "s3.0.attn.2: 0\n",
            "s3.0.ff: 37810176\n",
            "s3.0.ff.0: 0\n",
            "s3.0.ff.1: 37810176\n",
            "s3.0.ff.1.norm: 61440\n",
            "s3.0.ff.1.fn: 37748736\n",
            "s3.0.ff.1.fn.net: 37748736\n",
            "s3.0.ff.1.fn.net.0: 18874368\n",
            "s3.0.ff.1.fn.net.1: 0\n",
            "s3.0.ff.1.fn.net.2: 0\n",
            "s3.0.ff.1.fn.net.3: 18874368\n",
            "s3.0.ff.1.fn.net.4: 0\n",
            "s3.0.ff.2: 0\n",
            "s3.1: 88334336\n",
            "s3.1.attn: 12775424\n",
            "s3.1.attn.0: 0\n",
            "s3.1.attn.1: 12775424\n",
            "s3.1.attn.1.norm: 61440\n",
            "s3.1.attn.1.fn: 12713984\n",
            "s3.1.attn.1.fn.attend: 0\n",
            "s3.1.attn.1.fn.to_qkv: 9437184\n",
            "s3.1.attn.1.fn.to_out: 3145728\n",
            "s3.1.attn.1.fn.to_out.0: 3145728\n",
            "s3.1.attn.1.fn.to_out.1: 0\n",
            "s3.1.attn.2: 0\n",
            "s3.1.ff: 75558912\n",
            "s3.1.ff.0: 0\n",
            "s3.1.ff.1: 75558912\n",
            "s3.1.ff.1.norm: 61440\n",
            "s3.1.ff.1.fn: 75497472\n",
            "s3.1.ff.1.fn.net: 75497472\n",
            "s3.1.ff.1.fn.net.0: 37748736\n",
            "s3.1.ff.1.fn.net.1: 0\n",
            "s3.1.ff.1.fn.net.2: 0\n",
            "s3.1.ff.1.fn.net.3: 37748736\n",
            "s3.1.ff.1.fn.net.4: 0\n",
            "s3.1.ff.2: 0\n",
            "s3.2: 88334336\n",
            "s3.2.attn: 12775424\n",
            "s3.2.attn.0: 0\n",
            "s3.2.attn.1: 12775424\n",
            "s3.2.attn.1.norm: 61440\n",
            "s3.2.attn.1.fn: 12713984\n",
            "s3.2.attn.1.fn.attend: 0\n",
            "s3.2.attn.1.fn.to_qkv: 9437184\n",
            "s3.2.attn.1.fn.to_out: 3145728\n",
            "s3.2.attn.1.fn.to_out.0: 3145728\n",
            "s3.2.attn.1.fn.to_out.1: 0\n",
            "s3.2.attn.2: 0\n",
            "s3.2.ff: 75558912\n",
            "s3.2.ff.0: 0\n",
            "s3.2.ff.1: 75558912\n",
            "s3.2.ff.1.norm: 61440\n",
            "s3.2.ff.1.fn: 75497472\n",
            "s3.2.ff.1.fn.net: 75497472\n",
            "s3.2.ff.1.fn.net.0: 37748736\n",
            "s3.2.ff.1.fn.net.1: 0\n",
            "s3.2.ff.1.fn.net.2: 0\n",
            "s3.2.ff.1.fn.net.3: 37748736\n",
            "s3.2.ff.1.fn.net.4: 0\n",
            "s3.2.ff.2: 0\n",
            "s3.3: 88334336\n",
            "s3.3.attn: 12775424\n",
            "s3.3.attn.0: 0\n",
            "s3.3.attn.1: 12775424\n",
            "s3.3.attn.1.norm: 61440\n",
            "s3.3.attn.1.fn: 12713984\n",
            "s3.3.attn.1.fn.attend: 0\n",
            "s3.3.attn.1.fn.to_qkv: 9437184\n",
            "s3.3.attn.1.fn.to_out: 3145728\n",
            "s3.3.attn.1.fn.to_out.0: 3145728\n",
            "s3.3.attn.1.fn.to_out.1: 0\n",
            "s3.3.attn.2: 0\n",
            "s3.3.ff: 75558912\n",
            "s3.3.ff.0: 0\n",
            "s3.3.ff.1: 75558912\n",
            "s3.3.ff.1.norm: 61440\n",
            "s3.3.ff.1.fn: 75497472\n",
            "s3.3.ff.1.fn.net: 75497472\n",
            "s3.3.ff.1.fn.net.0: 37748736\n",
            "s3.3.ff.1.fn.net.1: 0\n",
            "s3.3.ff.1.fn.net.2: 0\n",
            "s3.3.ff.1.fn.net.3: 37748736\n",
            "s3.3.ff.1.fn.net.4: 0\n",
            "s3.3.ff.2: 0\n",
            "s3.4: 88334336\n",
            "s3.4.attn: 12775424\n",
            "s3.4.attn.0: 0\n",
            "s3.4.attn.1: 12775424\n",
            "s3.4.attn.1.norm: 61440\n",
            "s3.4.attn.1.fn: 12713984\n",
            "s3.4.attn.1.fn.attend: 0\n",
            "s3.4.attn.1.fn.to_qkv: 9437184\n",
            "s3.4.attn.1.fn.to_out: 3145728\n",
            "s3.4.attn.1.fn.to_out.0: 3145728\n",
            "s3.4.attn.1.fn.to_out.1: 0\n",
            "s3.4.attn.2: 0\n",
            "s3.4.ff: 75558912\n",
            "s3.4.ff.0: 0\n",
            "s3.4.ff.1: 75558912\n",
            "s3.4.ff.1.norm: 61440\n",
            "s3.4.ff.1.fn: 75497472\n",
            "s3.4.ff.1.fn.net: 75497472\n",
            "s3.4.ff.1.fn.net.0: 37748736\n",
            "s3.4.ff.1.fn.net.1: 0\n",
            "s3.4.ff.1.fn.net.2: 0\n",
            "s3.4.ff.1.fn.net.3: 37748736\n",
            "s3.4.ff.1.fn.net.4: 0\n",
            "s3.4.ff.2: 0\n",
            "s3.5: 88334336\n",
            "s3.5.attn: 12775424\n",
            "s3.5.attn.0: 0\n",
            "s3.5.attn.1: 12775424\n",
            "s3.5.attn.1.norm: 61440\n",
            "s3.5.attn.1.fn: 12713984\n",
            "s3.5.attn.1.fn.attend: 0\n",
            "s3.5.attn.1.fn.to_qkv: 9437184\n",
            "s3.5.attn.1.fn.to_out: 3145728\n",
            "s3.5.attn.1.fn.to_out.0: 3145728\n",
            "s3.5.attn.1.fn.to_out.1: 0\n",
            "s3.5.attn.2: 0\n",
            "s3.5.ff: 75558912\n",
            "s3.5.ff.0: 0\n",
            "s3.5.ff.1: 75558912\n",
            "s3.5.ff.1.norm: 61440\n",
            "s3.5.ff.1.fn: 75497472\n",
            "s3.5.ff.1.fn.net: 75497472\n",
            "s3.5.ff.1.fn.net.0: 37748736\n",
            "s3.5.ff.1.fn.net.1: 0\n",
            "s3.5.ff.1.fn.net.2: 0\n",
            "s3.5.ff.1.fn.net.3: 37748736\n",
            "s3.5.ff.1.fn.net.4: 0\n",
            "s3.5.ff.2: 0\n",
            "s3.6: 88334336\n",
            "s3.6.attn: 12775424\n",
            "s3.6.attn.0: 0\n",
            "s3.6.attn.1: 12775424\n",
            "s3.6.attn.1.norm: 61440\n",
            "s3.6.attn.1.fn: 12713984\n",
            "s3.6.attn.1.fn.attend: 0\n",
            "s3.6.attn.1.fn.to_qkv: 9437184\n",
            "s3.6.attn.1.fn.to_out: 3145728\n",
            "s3.6.attn.1.fn.to_out.0: 3145728\n",
            "s3.6.attn.1.fn.to_out.1: 0\n",
            "s3.6.attn.2: 0\n",
            "s3.6.ff: 75558912\n",
            "s3.6.ff.0: 0\n",
            "s3.6.ff.1: 75558912\n",
            "s3.6.ff.1.norm: 61440\n",
            "s3.6.ff.1.fn: 75497472\n",
            "s3.6.ff.1.fn.net: 75497472\n",
            "s3.6.ff.1.fn.net.0: 37748736\n",
            "s3.6.ff.1.fn.net.1: 0\n",
            "s3.6.ff.1.fn.net.2: 0\n",
            "s3.6.ff.1.fn.net.3: 37748736\n",
            "s3.6.ff.1.fn.net.4: 0\n",
            "s3.6.ff.2: 0\n",
            "s3.7: 88334336\n",
            "s3.7.attn: 12775424\n",
            "s3.7.attn.0: 0\n",
            "s3.7.attn.1: 12775424\n",
            "s3.7.attn.1.norm: 61440\n",
            "s3.7.attn.1.fn: 12713984\n",
            "s3.7.attn.1.fn.attend: 0\n",
            "s3.7.attn.1.fn.to_qkv: 9437184\n",
            "s3.7.attn.1.fn.to_out: 3145728\n",
            "s3.7.attn.1.fn.to_out.0: 3145728\n",
            "s3.7.attn.1.fn.to_out.1: 0\n",
            "s3.7.attn.2: 0\n",
            "s3.7.ff: 75558912\n",
            "s3.7.ff.0: 0\n",
            "s3.7.ff.1: 75558912\n",
            "s3.7.ff.1.norm: 61440\n",
            "s3.7.ff.1.fn: 75497472\n",
            "s3.7.ff.1.fn.net: 75497472\n",
            "s3.7.ff.1.fn.net.0: 37748736\n",
            "s3.7.ff.1.fn.net.1: 0\n",
            "s3.7.ff.1.fn.net.2: 0\n",
            "s3.7.ff.1.fn.net.3: 37748736\n",
            "s3.7.ff.1.fn.net.4: 0\n",
            "s3.7.ff.2: 0\n",
            "s3.8: 88334336\n",
            "s3.8.attn: 12775424\n",
            "s3.8.attn.0: 0\n",
            "s3.8.attn.1: 12775424\n",
            "s3.8.attn.1.norm: 61440\n",
            "s3.8.attn.1.fn: 12713984\n",
            "s3.8.attn.1.fn.attend: 0\n",
            "s3.8.attn.1.fn.to_qkv: 9437184\n",
            "s3.8.attn.1.fn.to_out: 3145728\n",
            "s3.8.attn.1.fn.to_out.0: 3145728\n",
            "s3.8.attn.1.fn.to_out.1: 0\n",
            "s3.8.attn.2: 0\n",
            "s3.8.ff: 75558912\n",
            "s3.8.ff.0: 0\n",
            "s3.8.ff.1: 75558912\n",
            "s3.8.ff.1.norm: 61440\n",
            "s3.8.ff.1.fn: 75497472\n",
            "s3.8.ff.1.fn.net: 75497472\n",
            "s3.8.ff.1.fn.net.0: 37748736\n",
            "s3.8.ff.1.fn.net.1: 0\n",
            "s3.8.ff.1.fn.net.2: 0\n",
            "s3.8.ff.1.fn.net.3: 37748736\n",
            "s3.8.ff.1.fn.net.4: 0\n",
            "s3.8.ff.2: 0\n",
            "s3.9: 88334336\n",
            "s3.9.attn: 12775424\n",
            "s3.9.attn.0: 0\n",
            "s3.9.attn.1: 12775424\n",
            "s3.9.attn.1.norm: 61440\n",
            "s3.9.attn.1.fn: 12713984\n",
            "s3.9.attn.1.fn.attend: 0\n",
            "s3.9.attn.1.fn.to_qkv: 9437184\n",
            "s3.9.attn.1.fn.to_out: 3145728\n",
            "s3.9.attn.1.fn.to_out.0: 3145728\n",
            "s3.9.attn.1.fn.to_out.1: 0\n",
            "s3.9.attn.2: 0\n",
            "s3.9.ff: 75558912\n",
            "s3.9.ff.0: 0\n",
            "s3.9.ff.1: 75558912\n",
            "s3.9.ff.1.norm: 61440\n",
            "s3.9.ff.1.fn: 75497472\n",
            "s3.9.ff.1.fn.net: 75497472\n",
            "s3.9.ff.1.fn.net.0: 37748736\n",
            "s3.9.ff.1.fn.net.1: 0\n",
            "s3.9.ff.1.fn.net.2: 0\n",
            "s3.9.ff.1.fn.net.3: 37748736\n",
            "s3.9.ff.1.fn.net.4: 0\n",
            "s3.9.ff.2: 0\n",
            "s3.10: 88334336\n",
            "s3.10.attn: 12775424\n",
            "s3.10.attn.0: 0\n",
            "s3.10.attn.1: 12775424\n",
            "s3.10.attn.1.norm: 61440\n",
            "s3.10.attn.1.fn: 12713984\n",
            "s3.10.attn.1.fn.attend: 0\n",
            "s3.10.attn.1.fn.to_qkv: 9437184\n",
            "s3.10.attn.1.fn.to_out: 3145728\n",
            "s3.10.attn.1.fn.to_out.0: 3145728\n",
            "s3.10.attn.1.fn.to_out.1: 0\n",
            "s3.10.attn.2: 0\n",
            "s3.10.ff: 75558912\n",
            "s3.10.ff.0: 0\n",
            "s3.10.ff.1: 75558912\n",
            "s3.10.ff.1.norm: 61440\n",
            "s3.10.ff.1.fn: 75497472\n",
            "s3.10.ff.1.fn.net: 75497472\n",
            "s3.10.ff.1.fn.net.0: 37748736\n",
            "s3.10.ff.1.fn.net.1: 0\n",
            "s3.10.ff.1.fn.net.2: 0\n",
            "s3.10.ff.1.fn.net.3: 37748736\n",
            "s3.10.ff.1.fn.net.4: 0\n",
            "s3.10.ff.2: 0\n",
            "s3.11: 88334336\n",
            "s3.11.attn: 12775424\n",
            "s3.11.attn.0: 0\n",
            "s3.11.attn.1: 12775424\n",
            "s3.11.attn.1.norm: 61440\n",
            "s3.11.attn.1.fn: 12713984\n",
            "s3.11.attn.1.fn.attend: 0\n",
            "s3.11.attn.1.fn.to_qkv: 9437184\n",
            "s3.11.attn.1.fn.to_out: 3145728\n",
            "s3.11.attn.1.fn.to_out.0: 3145728\n",
            "s3.11.attn.1.fn.to_out.1: 0\n",
            "s3.11.attn.2: 0\n",
            "s3.11.ff: 75558912\n",
            "s3.11.ff.0: 0\n",
            "s3.11.ff.1: 75558912\n",
            "s3.11.ff.1.norm: 61440\n",
            "s3.11.ff.1.fn: 75497472\n",
            "s3.11.ff.1.fn.net: 75497472\n",
            "s3.11.ff.1.fn.net.0: 37748736\n",
            "s3.11.ff.1.fn.net.1: 0\n",
            "s3.11.ff.1.fn.net.2: 0\n",
            "s3.11.ff.1.fn.net.3: 37748736\n",
            "s3.11.ff.1.fn.net.4: 0\n",
            "s3.11.ff.2: 0\n",
            "s3.12: 88334336\n",
            "s3.12.attn: 12775424\n",
            "s3.12.attn.0: 0\n",
            "s3.12.attn.1: 12775424\n",
            "s3.12.attn.1.norm: 61440\n",
            "s3.12.attn.1.fn: 12713984\n",
            "s3.12.attn.1.fn.attend: 0\n",
            "s3.12.attn.1.fn.to_qkv: 9437184\n",
            "s3.12.attn.1.fn.to_out: 3145728\n",
            "s3.12.attn.1.fn.to_out.0: 3145728\n",
            "s3.12.attn.1.fn.to_out.1: 0\n",
            "s3.12.attn.2: 0\n",
            "s3.12.ff: 75558912\n",
            "s3.12.ff.0: 0\n",
            "s3.12.ff.1: 75558912\n",
            "s3.12.ff.1.norm: 61440\n",
            "s3.12.ff.1.fn: 75497472\n",
            "s3.12.ff.1.fn.net: 75497472\n",
            "s3.12.ff.1.fn.net.0: 37748736\n",
            "s3.12.ff.1.fn.net.1: 0\n",
            "s3.12.ff.1.fn.net.2: 0\n",
            "s3.12.ff.1.fn.net.3: 37748736\n",
            "s3.12.ff.1.fn.net.4: 0\n",
            "s3.12.ff.2: 0\n",
            "s3.13: 88334336\n",
            "s3.13.attn: 12775424\n",
            "s3.13.attn.0: 0\n",
            "s3.13.attn.1: 12775424\n",
            "s3.13.attn.1.norm: 61440\n",
            "s3.13.attn.1.fn: 12713984\n",
            "s3.13.attn.1.fn.attend: 0\n",
            "s3.13.attn.1.fn.to_qkv: 9437184\n",
            "s3.13.attn.1.fn.to_out: 3145728\n",
            "s3.13.attn.1.fn.to_out.0: 3145728\n",
            "s3.13.attn.1.fn.to_out.1: 0\n",
            "s3.13.attn.2: 0\n",
            "s3.13.ff: 75558912\n",
            "s3.13.ff.0: 0\n",
            "s3.13.ff.1: 75558912\n",
            "s3.13.ff.1.norm: 61440\n",
            "s3.13.ff.1.fn: 75497472\n",
            "s3.13.ff.1.fn.net: 75497472\n",
            "s3.13.ff.1.fn.net.0: 37748736\n",
            "s3.13.ff.1.fn.net.1: 0\n",
            "s3.13.ff.1.fn.net.2: 0\n",
            "s3.13.ff.1.fn.net.3: 37748736\n",
            "s3.13.ff.1.fn.net.4: 0\n",
            "s3.13.ff.2: 0\n",
            "s4: 128312320\n",
            "s4.0: 46453760\n",
            "s4.0.pool1: 0\n",
            "s4.0.pool2: 0\n",
            "s4.0.proj: 4718592\n",
            "s4.0.attn: 3955712\n",
            "s4.0.attn.0: 0\n",
            "s4.0.attn.1: 3955712\n",
            "s4.0.attn.1.norm: 15360\n",
            "s4.0.attn.1.fn: 3940352\n",
            "s4.0.attn.1.fn.attend: 0\n",
            "s4.0.attn.1.fn.to_qkv: 2359296\n",
            "s4.0.attn.1.fn.to_out: 1572864\n",
            "s4.0.attn.1.fn.to_out.0: 1572864\n",
            "s4.0.attn.1.fn.to_out.1: 0\n",
            "s4.0.attn.2: 0\n",
            "s4.0.ff: 37779456\n",
            "s4.0.ff.0: 0\n",
            "s4.0.ff.1: 37779456\n",
            "s4.0.ff.1.norm: 30720\n",
            "s4.0.ff.1.fn: 37748736\n",
            "s4.0.ff.1.fn.net: 37748736\n",
            "s4.0.ff.1.fn.net.0: 18874368\n",
            "s4.0.ff.1.fn.net.1: 0\n",
            "s4.0.ff.1.fn.net.2: 0\n",
            "s4.0.ff.1.fn.net.3: 18874368\n",
            "s4.0.ff.1.fn.net.4: 0\n",
            "s4.0.ff.2: 0\n",
            "s4.1: 81858560\n",
            "s4.1.attn: 6330368\n",
            "s4.1.attn.0: 0\n",
            "s4.1.attn.1: 6330368\n",
            "s4.1.attn.1.norm: 30720\n",
            "s4.1.attn.1.fn: 6299648\n",
            "s4.1.attn.1.fn.attend: 0\n",
            "s4.1.attn.1.fn.to_qkv: 4718592\n",
            "s4.1.attn.1.fn.to_out: 1572864\n",
            "s4.1.attn.1.fn.to_out.0: 1572864\n",
            "s4.1.attn.1.fn.to_out.1: 0\n",
            "s4.1.attn.2: 0\n",
            "s4.1.ff: 75528192\n",
            "s4.1.ff.0: 0\n",
            "s4.1.ff.1: 75528192\n",
            "s4.1.ff.1.norm: 30720\n",
            "s4.1.ff.1.fn: 75497472\n",
            "s4.1.ff.1.fn.net: 75497472\n",
            "s4.1.ff.1.fn.net.0: 37748736\n",
            "s4.1.ff.1.fn.net.1: 0\n",
            "s4.1.ff.1.fn.net.2: 0\n",
            "s4.1.ff.1.fn.net.3: 37748736\n",
            "s4.1.ff.1.fn.net.4: 0\n",
            "s4.1.ff.2: 0\n",
            "pool: 0\n",
            "fc: 1536000\n"
          ]
        }
      ]
    }
  ]
}