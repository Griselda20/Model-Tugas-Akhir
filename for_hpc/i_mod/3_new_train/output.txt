nohup: ignoring input
/home/tasi2425111/venvmaestro/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/tasi2425111/venvmaestro/lib/python3.8/site-packages/timm/models/features.py:4: FutureWarning: Importing from timm.models.features is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/tasi2425111/venvmaestro/lib/python3.8/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/tasi2425111/venvmaestro/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Training with a single process on 1 device (cuda:1).
Model modification_294m created, param count:11103020
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
	crop_mode: center
Created AdamW (AdamW) optimizer: lr: 0.001, betas: (0.9, 0.999), eps: 1e-08, weight_decay: 0.05, amsgrad: False, foreach: None, maximize: False, capturable: False
AMP not enabled. Training in torch.float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/home/tasi2425111/for_hpc/baru/ti_mod/4_new_train/output/train/20250405-152714-modification_294m-224/checkpoint-75.pth.tar' (epoch 75)
Scheduled epochs: 300 (epochs + cooldown_epochs). Warmup within epochs when warmup_prefix=False. LR stepped per epoch.
block: 12544, cnn-drop 0.0000, mlp-drop 0.0000
block: 12544, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 16, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 24, token: 192
block: 3136, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 24, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 24, token: 192
block: 3136, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 24, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 48, token: 192
block: 784, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 48, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 48, token: 192
block: 784, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 48, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 96, token: 192
block: 196, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 96, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 96, token: 192
block: 196, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 96, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 128, token: 192
block: 196, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 128, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 128, token: 192
block: 196, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 128, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 192, token: 192
block: 49, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 192, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 192, token: 192
block: 49, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 192, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 192, token: 192
L2G: 2 heads, inp: 192, token: 192
Modification(
  (tokens): Embedding(6, 192)
  (stem): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (features): Sequential(
    (0): DnaBlock3(
      (conv): Sequential(
        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Sequential()
        (4): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): DnaBlock3(
      (conv1): Sequential(
        (0): Conv2d(16, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=192, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (conv3): Sequential(
        (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=192, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv4): Sequential(
        (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act4): DyReLU(
        (act): Sequential()
      )
      (hyper4): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=16, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=16, bias=True)
        (proj): Linear(in_features=16, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=24, bias=True)
        (proj): Linear(in_features=192, out_features=24, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (2): DnaBlock(
      (conv1): Sequential(
        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=192, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=192, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=24, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=24, bias=True)
        (proj): Linear(in_features=24, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=24, bias=True)
        (proj): Linear(in_features=192, out_features=24, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (3): DnaBlock3(
      (conv1): Sequential(
        (0): Conv2d(24, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=288, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (conv3): Sequential(
        (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=384, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv4): Sequential(
        (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act4): DyReLU(
        (act): Sequential()
      )
      (hyper4): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=24, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=24, bias=True)
        (proj): Linear(in_features=24, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=48, bias=True)
        (proj): Linear(in_features=192, out_features=48, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (4): DnaBlock(
      (conv1): Sequential(
        (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=384, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=384, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=48, bias=True)
        (proj): Linear(in_features=48, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=48, bias=True)
        (proj): Linear(in_features=192, out_features=48, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (5): DnaBlock3(
      (conv1): Sequential(
        (0): Conv2d(48, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=576, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (conv3): Sequential(
        (0): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=768, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv4): Sequential(
        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act4): DyReLU(
        (act): Sequential()
      )
      (hyper4): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=48, bias=True)
        (proj): Linear(in_features=48, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=96, bias=True)
        (proj): Linear(in_features=192, out_features=96, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (6): DnaBlock(
      (conv1): Sequential(
        (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=768, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=768, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=96, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=96, bias=True)
        (proj): Linear(in_features=96, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=96, bias=True)
        (proj): Linear(in_features=192, out_features=96, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (7): DnaBlock(
      (conv1): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1152, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1152, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=96, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=96, bias=True)
        (proj): Linear(in_features=96, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=128, bias=True)
        (proj): Linear(in_features=192, out_features=128, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (8): DnaBlock(
      (conv1): Sequential(
        (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1536, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1536, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=128, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=128, bias=True)
        (proj): Linear(in_features=128, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=128, bias=True)
        (proj): Linear(in_features=192, out_features=128, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (9): DnaBlock3(
      (conv1): Sequential(
        (0): Conv2d(128, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1536, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (conv3): Sequential(
        (0): Conv2d(192, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1536, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv4): Sequential(
        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act4): DyReLU(
        (act): Sequential()
      )
      (hyper4): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=128, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=128, bias=True)
        (proj): Linear(in_features=128, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=192, bias=True)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (10): DnaBlock(
      (conv1): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2304, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2304, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=192, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=192, bias=True)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=192, bias=True)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (11): DnaBlock(
      (conv1): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2304, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2304, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=192, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=192, bias=True)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU(approximate=none)
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (relative_attn): RelativeAttention(
          (Q): Linear(in_features=192, out_features=192, bias=False)
          (K): Linear(in_features=192, out_features=192, bias=False)
          (V): Linear(in_features=192, out_features=192, bias=False)
          (ff): Linear(in_features=192, out_features=192, bias=True)
          (attn_dropout): Dropout2d(p=0.1, inplace=False)
          (ff_dropout): Dropout(p=0.1, inplace=False)
        )
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=192, bias=True)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
  )
  (local_global): Local2Global(
    (alpha): Sequential(
      (0): Linear(in_features=192, out_features=192, bias=True)
      (1): h_sigmoid(
        (relu): ReLU6(inplace=True)
      )
    )
    (q): Linear(in_features=192, out_features=192, bias=True)
    (proj): Linear(in_features=192, out_features=192, bias=True)
    (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (drop_path): DropPath(drop_prob=0.000)
  )
  (classifier): MergeClassifier(
    (conv): Sequential(
      (0): Sequential()
      (1): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (act): DyReLU(
      (act): ReLU6(inplace=True)
    )
    (hyper): Sequential()
    (avgpool): Sequential(
      (0): AdaptiveAvgPool2d(output_size=(1, 1))
      (1): h_swish(
        (sigmoid): h_sigmoid(
          (relu): ReLU6(inplace=True)
        )
      )
    )
    (fc): Sequential(
      (0): Linear(in_features=1344, out_features=1920, bias=True)
      (1): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): h_swish(
        (sigmoid): h_sigmoid(
          (relu): ReLU6(inplace=True)
        )
      )
    )
    (classifier): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=1920, out_features=200, bias=True)
    )
  )
)
Train: 76 [   0/5000 (  0%)]  Loss: 3.50 (3.50)  Time: 1.902s,   10.51/s  (1.902s,   10.51/s)  LR: 1.752e-05  Data: 0.523 (0.523)  Elapsed/ETA: 1.9s / 9508.8s
Train: 76 [  50/5000 (  1%)]  Loss: 3.24 (4.15)  Time: 0.433s,   46.20/s  (0.499s,   40.04/s)  LR: 1.752e-05  Data: 0.008 (0.019)  Elapsed/ETA: 25.5s / 2471.8s
Train: 76 [ 100/5000 (  2%)]  Loss: 4.59 (4.08)  Time: 0.568s,   35.24/s  (0.471s,   42.49/s)  LR: 1.752e-05  Data: 0.010 (0.014)  Elapsed/ETA: 47.5s / 2306.2s
Train: 76 [ 150/5000 (  3%)]  Loss: 4.21 (4.08)  Time: 0.427s,   46.80/s  (0.469s,   42.61/s)  LR: 1.752e-05  Data: 0.008 (0.012)  Elapsed/ETA: 70.9s / 2276.0s
Train: 76 [ 200/5000 (  4%)]  Loss: 4.51 (4.11)  Time: 0.433s,   46.24/s  (0.460s,   43.51/s)  LR: 1.752e-05  Data: 0.010 (0.011)  Elapsed/ETA: 92.4s / 2205.8s
Train: 76 [ 250/5000 (  5%)]  Loss: 3.78 (4.12)  Time: 0.437s,   45.76/s  (0.458s,   43.64/s)  LR: 1.752e-05  Data: 0.009 (0.011)  Elapsed/ETA: 115.0s / 2176.3s
Train: 76 [ 300/5000 (  6%)]  Loss: 3.90 (4.11)  Time: 0.432s,   46.33/s  (0.454s,   44.04/s)  LR: 1.752e-05  Data: 0.009 (0.010)  Elapsed/ETA: 136.7s / 2133.9s
Train: 76 [ 350/5000 (  7%)]  Loss: 3.50 (4.11)  Time: 0.428s,   46.72/s  (0.451s,   44.35/s)  LR: 1.752e-05  Data: 0.008 (0.010)  Elapsed/ETA: 158.3s / 2096.6s
Train: 76 [ 400/5000 (  8%)]  Loss: 3.97 (4.11)  Time: 0.431s,   46.45/s  (0.448s,   44.60/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 179.8s / 2062.4s
Train: 76 [ 450/5000 (  9%)]  Loss: 3.49 (4.11)  Time: 0.429s,   46.65/s  (0.446s,   44.80/s)  LR: 1.752e-05  Data: 0.008 (0.010)  Elapsed/ETA: 201.3s / 2030.7s
Train: 76 [ 500/5000 ( 10%)]  Loss: 4.70 (4.12)  Time: 0.432s,   46.32/s  (0.445s,   44.97/s)  LR: 1.752e-05  Data: 0.009 (0.010)  Elapsed/ETA: 222.8s / 2001.1s
Train: 76 [ 550/5000 ( 11%)]  Loss: 4.40 (4.11)  Time: 0.470s,   42.55/s  (0.447s,   44.79/s)  LR: 1.752e-05  Data: 0.008 (0.010)  Elapsed/ETA: 246.0s / 1986.7s
Train: 76 [ 600/5000 ( 12%)]  Loss: 4.67 (4.11)  Time: 0.569s,   35.18/s  (0.450s,   44.40/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 270.7s / 1981.4s
Train: 76 [ 650/5000 ( 13%)]  Loss: 3.19 (4.10)  Time: 0.561s,   35.67/s  (0.459s,   43.55/s)  LR: 1.752e-05  Data: 0.008 (0.009)  Elapsed/ETA: 298.9s / 1997.0s
Train: 76 [ 700/5000 ( 14%)]  Loss: 4.61 (4.10)  Time: 0.567s,   35.26/s  (0.467s,   42.85/s)  LR: 1.752e-05  Data: 0.010 (0.009)  Elapsed/ETA: 327.2s / 2006.7s
Train: 76 [ 750/5000 ( 15%)]  Loss: 4.44 (4.10)  Time: 0.428s,   46.71/s  (0.467s,   42.80/s)  LR: 1.752e-05  Data: 0.008 (0.009)  Elapsed/ETA: 350.9s / 1985.3s
Train: 76 [ 800/5000 ( 16%)]  Loss: 3.36 (4.10)  Time: 0.432s,   46.31/s  (0.465s,   43.02/s)  LR: 1.752e-05  Data: 0.010 (0.009)  Elapsed/ETA: 372.4s / 1952.3s
Train: 76 [ 850/5000 ( 17%)]  Loss: 4.38 (4.10)  Time: 0.428s,   46.74/s  (0.463s,   43.21/s)  LR: 1.752e-05  Data: 0.008 (0.009)  Elapsed/ETA: 393.9s / 1920.3s
Train: 76 [ 900/5000 ( 18%)]  Loss: 3.85 (4.11)  Time: 0.432s,   46.26/s  (0.461s,   43.38/s)  LR: 1.752e-05  Data: 0.010 (0.009)  Elapsed/ETA: 415.4s / 1889.6s
Train: 76 [ 950/5000 ( 19%)]  Loss: 4.40 (4.11)  Time: 0.430s,   46.52/s  (0.459s,   43.54/s)  LR: 1.752e-05  Data: 0.008 (0.009)  Elapsed/ETA: 436.9s / 1860.0s
Train: 76 [1000/5000 ( 20%)]  Loss: 4.13 (4.11)  Time: 0.435s,   45.94/s  (0.459s,   43.57/s)  LR: 1.752e-05  Data: 0.011 (0.009)  Elapsed/ETA: 459.5s / 1835.8s
Train: 76 [1050/5000 ( 21%)]  Loss: 3.65 (4.12)  Time: 0.431s,   46.40/s  (0.461s,   43.34/s)  LR: 1.752e-05  Data: 0.009 (0.009)  Elapsed/ETA: 485.0s / 1822.3s
Train: 76 [1100/5000 ( 22%)]  Loss: 3.74 (4.11)  Time: 0.434s,   46.06/s  (0.460s,   43.46/s)  LR: 1.752e-05  Data: 0.011 (0.009)  Elapsed/ETA: 506.7s / 1794.3s
Train: 76 [1150/5000 ( 23%)]  Loss: 4.60 (4.12)  Time: 0.435s,   45.98/s  (0.459s,   43.54/s)  LR: 1.752e-05  Data: 0.010 (0.009)  Elapsed/ETA: 528.7s / 1768.0s
Train: 76 [1200/5000 ( 24%)]  Loss: 4.15 (4.11)  Time: 0.437s,   45.81/s  (0.458s,   43.64/s)  LR: 1.752e-05  Data: 0.011 (0.009)  Elapsed/ETA: 550.5s / 1741.2s
Train: 76 [1250/5000 ( 25%)]  Loss: 4.72 (4.11)  Time: 0.434s,   46.04/s  (0.457s,   43.72/s)  LR: 1.752e-05  Data: 0.010 (0.009)  Elapsed/ETA: 572.2s / 1714.8s
Train: 76 [1300/5000 ( 26%)]  Loss: 3.86 (4.11)  Time: 0.484s,   41.29/s  (0.459s,   43.61/s)  LR: 1.752e-05  Data: 0.011 (0.009)  Elapsed/ETA: 596.7s / 1696.5s
Train: 76 [1350/5000 ( 27%)]  Loss: 3.68 (4.11)  Time: 0.436s,   45.87/s  (0.459s,   43.62/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 619.5s / 1673.1s
Train: 76 [1400/5000 ( 28%)]  Loss: 4.49 (4.11)  Time: 0.454s,   44.10/s  (0.459s,   43.61/s)  LR: 1.752e-05  Data: 0.014 (0.010)  Elapsed/ETA: 642.5s / 1650.4s
Train: 76 [1450/5000 ( 29%)]  Loss: 4.56 (4.12)  Time: 0.438s,   45.70/s  (0.459s,   43.61/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 665.5s / 1627.7s
Train: 76 [1500/5000 ( 30%)]  Loss: 3.25 (4.12)  Time: 0.451s,   44.37/s  (0.459s,   43.58/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 688.8s / 1605.8s
Train: 76 [1550/5000 ( 31%)]  Loss: 4.24 (4.12)  Time: 0.534s,   37.43/s  (0.459s,   43.56/s)  LR: 1.752e-05  Data: 0.015 (0.010)  Elapsed/ETA: 712.2s / 1583.7s
Train: 76 [1600/5000 ( 32%)]  Loss: 4.47 (4.12)  Time: 0.581s,   34.44/s  (0.459s,   43.55/s)  LR: 1.752e-05  Data: 0.015 (0.010)  Elapsed/ETA: 735.2s / 1560.9s
Train: 76 [1650/5000 ( 33%)]  Loss: 3.54 (4.12)  Time: 0.576s,   34.70/s  (0.460s,   43.43/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 760.3s / 1542.2s
Train: 76 [1700/5000 ( 34%)]  Loss: 3.59 (4.11)  Time: 0.445s,   44.92/s  (0.460s,   43.43/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 783.3s / 1519.2s
Train: 76 [1750/5000 ( 35%)]  Loss: 4.32 (4.11)  Time: 0.449s,   44.58/s  (0.462s,   43.32/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 808.4s / 1499.9s
Train: 76 [1800/5000 ( 36%)]  Loss: 4.70 (4.12)  Time: 0.451s,   44.36/s  (0.461s,   43.35/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 830.9s / 1475.8s
Train: 76 [1850/5000 ( 37%)]  Loss: 4.01 (4.12)  Time: 0.450s,   44.47/s  (0.461s,   43.36/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 853.7s / 1452.4s
Train: 76 [1900/5000 ( 38%)]  Loss: 3.67 (4.11)  Time: 0.456s,   43.85/s  (0.461s,   43.36/s)  LR: 1.752e-05  Data: 0.012 (0.010)  Elapsed/ETA: 876.8s / 1429.3s
Train: 76 [1950/5000 ( 39%)]  Loss: 4.02 (4.11)  Time: 0.440s,   45.50/s  (0.462s,   43.31/s)  LR: 1.752e-05  Data: 0.009 (0.010)  Elapsed/ETA: 900.9s / 1407.9s
Train: 76 [2000/5000 ( 40%)]  Loss: 4.46 (4.11)  Time: 0.440s,   45.49/s  (0.463s,   43.18/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 926.8s / 1389.1s
Train: 76 [2050/5000 ( 41%)]  Loss: 4.58 (4.11)  Time: 0.435s,   45.97/s  (0.463s,   43.24/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 948.7s / 1364.1s
Train: 76 [2100/5000 ( 42%)]  Loss: 4.39 (4.11)  Time: 0.434s,   46.05/s  (0.462s,   43.28/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 970.8s / 1339.5s
Train: 76 [2150/5000 ( 43%)]  Loss: 3.87 (4.11)  Time: 0.438s,   45.65/s  (0.462s,   43.34/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 992.7s / 1314.8s
Train: 76 [2200/5000 ( 44%)]  Loss: 3.12 (4.11)  Time: 0.442s,   45.28/s  (0.461s,   43.38/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1014.7s / 1290.4s
Train: 76 [2250/5000 ( 45%)]  Loss: 4.99 (4.11)  Time: 0.435s,   45.94/s  (0.461s,   43.43/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1036.7s / 1266.1s
Train: 76 [2300/5000 ( 46%)]  Loss: 4.30 (4.11)  Time: 0.436s,   45.86/s  (0.461s,   43.42/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1059.8s / 1243.2s
Train: 76 [2350/5000 ( 47%)]  Loss: 4.56 (4.11)  Time: 0.436s,   45.87/s  (0.460s,   43.47/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1081.7s / 1218.8s
Train: 76 [2400/5000 ( 48%)]  Loss: 4.50 (4.12)  Time: 0.438s,   45.64/s  (0.460s,   43.46/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1104.9s / 1196.1s
Train: 76 [2450/5000 ( 49%)]  Loss: 2.90 (4.12)  Time: 0.477s,   41.94/s  (0.460s,   43.49/s)  LR: 1.752e-05  Data: 0.009 (0.010)  Elapsed/ETA: 1127.2s / 1172.2s
Train: 76 [2500/5000 ( 50%)]  Loss: 4.90 (4.12)  Time: 0.437s,   45.76/s  (0.460s,   43.51/s)  LR: 1.752e-05  Data: 0.008 (0.010)  Elapsed/ETA: 1149.7s / 1148.8s
Train: 76 [2550/5000 ( 51%)]  Loss: 3.24 (4.12)  Time: 0.441s,   45.38/s  (0.461s,   43.42/s)  LR: 1.752e-05  Data: 0.012 (0.010)  Elapsed/ETA: 1175.0s / 1128.1s
Train: 76 [2600/5000 ( 52%)]  Loss: 4.39 (4.12)  Time: 0.456s,   43.85/s  (0.460s,   43.44/s)  LR: 1.752e-05  Data: 0.015 (0.010)  Elapsed/ETA: 1197.5s / 1104.5s
Train: 76 [2650/5000 ( 53%)]  Loss: 4.47 (4.12)  Time: 0.437s,   45.81/s  (0.461s,   43.43/s)  LR: 1.752e-05  Data: 0.009 (0.010)  Elapsed/ETA: 1220.8s / 1081.8s
Train: 76 [2700/5000 ( 54%)]  Loss: 4.10 (4.11)  Time: 0.439s,   45.59/s  (0.461s,   43.40/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1244.6s / 1059.3s
Train: 76 [2750/5000 ( 55%)]  Loss: 3.32 (4.11)  Time: 0.453s,   44.19/s  (0.460s,   43.44/s)  LR: 1.752e-05  Data: 0.012 (0.010)  Elapsed/ETA: 1266.6s / 1035.5s
Train: 76 [2800/5000 ( 56%)]  Loss: 4.59 (4.11)  Time: 0.436s,   45.86/s  (0.461s,   43.41/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1290.5s / 1013.1s
Train: 76 [2850/5000 ( 57%)]  Loss: 5.24 (4.11)  Time: 0.589s,   33.97/s  (0.461s,   43.34/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1315.6s / 991.7s
Train: 76 [2900/5000 ( 58%)]  Loss: 3.90 (4.11)  Time: 0.446s,   44.88/s  (0.462s,   43.30/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1340.1s / 969.6s
Train: 76 [2950/5000 ( 59%)]  Loss: 4.48 (4.11)  Time: 0.441s,   45.31/s  (0.462s,   43.31/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1362.7s / 946.2s
Train: 76 [3000/5000 ( 60%)]  Loss: 3.99 (4.11)  Time: 0.576s,   34.74/s  (0.462s,   43.32/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1385.6s / 923.0s
Train: 76 [3050/5000 ( 61%)]  Loss: 4.33 (4.11)  Time: 0.437s,   45.73/s  (0.462s,   43.33/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1408.3s / 899.6s
Train: 76 [3100/5000 ( 62%)]  Loss: 3.65 (4.11)  Time: 0.459s,   43.56/s  (0.461s,   43.34/s)  LR: 1.752e-05  Data: 0.016 (0.010)  Elapsed/ETA: 1431.0s / 876.3s
Train: 76 [3150/5000 ( 63%)]  Loss: 3.83 (4.11)  Time: 0.583s,   34.30/s  (0.462s,   43.30/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1455.4s / 854.0s
Train: 76 [3200/5000 ( 64%)]  Loss: 4.17 (4.11)  Time: 0.438s,   45.68/s  (0.463s,   43.24/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1480.5s / 832.1s
Train: 76 [3250/5000 ( 65%)]  Loss: 4.28 (4.11)  Time: 0.438s,   45.69/s  (0.462s,   43.27/s)  LR: 1.752e-05  Data: 0.009 (0.010)  Elapsed/ETA: 1502.7s / 808.4s
Train: 76 [3300/5000 ( 66%)]  Loss: 3.78 (4.11)  Time: 0.486s,   41.16/s  (0.463s,   43.19/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1528.8s / 786.8s
Train: 76 [3350/5000 ( 67%)]  Loss: 4.05 (4.11)  Time: 0.443s,   45.18/s  (0.463s,   43.22/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1550.8s / 763.2s
Train: 76 [3400/5000 ( 68%)]  Loss: 4.34 (4.11)  Time: 0.434s,   46.09/s  (0.463s,   43.23/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1573.4s / 739.8s
Train: 76 [3450/5000 ( 69%)]  Loss: 4.71 (4.11)  Time: 0.444s,   45.06/s  (0.462s,   43.26/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1595.6s / 716.2s
Train: 76 [3500/5000 ( 70%)]  Loss: 3.57 (4.11)  Time: 0.440s,   45.45/s  (0.462s,   43.29/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1617.5s / 692.6s
Train: 76 [3550/5000 ( 71%)]  Loss: 3.98 (4.12)  Time: 0.435s,   46.02/s  (0.462s,   43.32/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1639.4s / 669.0s
Train: 76 [3600/5000 ( 72%)]  Loss: 3.05 (4.11)  Time: 0.440s,   45.44/s  (0.461s,   43.35/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1661.2s / 645.4s
Train: 76 [3650/5000 ( 73%)]  Loss: 4.63 (4.11)  Time: 0.572s,   34.95/s  (0.462s,   43.33/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1685.1s / 622.6s
Train: 76 [3700/5000 ( 74%)]  Loss: 4.51 (4.11)  Time: 0.576s,   34.71/s  (0.463s,   43.19/s)  LR: 1.752e-05  Data: 0.013 (0.010)  Elapsed/ETA: 1713.8s / 601.5s
Train: 76 [3750/5000 ( 75%)]  Loss: 3.59 (4.11)  Time: 0.436s,   45.85/s  (0.463s,   43.16/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1738.1s / 578.7s
Train: 76 [3800/5000 ( 76%)]  Loss: 4.61 (4.11)  Time: 0.441s,   45.40/s  (0.463s,   43.19/s)  LR: 1.752e-05  Data: 0.012 (0.010)  Elapsed/ETA: 1760.1s / 555.2s
Train: 76 [3850/5000 ( 77%)]  Loss: 4.26 (4.11)  Time: 0.435s,   45.95/s  (0.463s,   43.17/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1784.3s / 532.4s
Train: 76 [3900/5000 ( 78%)]  Loss: 4.49 (4.11)  Time: 0.436s,   45.87/s  (0.463s,   43.20/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1806.1s / 508.8s
Train: 76 [3950/5000 ( 79%)]  Loss: 3.97 (4.11)  Time: 0.471s,   42.48/s  (0.463s,   43.22/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1828.5s / 485.5s
Train: 76 [4000/5000 ( 80%)]  Loss: 4.19 (4.11)  Time: 0.440s,   45.49/s  (0.463s,   43.24/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1850.7s / 462.1s
Train: 76 [4050/5000 ( 81%)]  Loss: 4.31 (4.11)  Time: 0.441s,   45.30/s  (0.463s,   43.23/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1874.3s / 439.1s
Train: 76 [4100/5000 ( 82%)]  Loss: 4.21 (4.12)  Time: 0.493s,   40.55/s  (0.463s,   43.23/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 1897.1s / 415.9s
Train: 76 [4150/5000 ( 83%)]  Loss: 3.35 (4.11)  Time: 0.596s,   33.58/s  (0.463s,   43.24/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1920.2s / 392.7s
Train: 76 [4200/5000 ( 84%)]  Loss: 3.73 (4.11)  Time: 0.470s,   42.58/s  (0.463s,   43.23/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1943.4s / 369.6s
Train: 76 [4250/5000 ( 85%)]  Loss: 3.54 (4.11)  Time: 0.439s,   45.58/s  (0.462s,   43.25/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 1965.9s / 346.4s
Train: 76 [4300/5000 ( 86%)]  Loss: 4.32 (4.11)  Time: 0.448s,   44.65/s  (0.463s,   43.21/s)  LR: 1.752e-05  Data: 0.013 (0.010)  Elapsed/ETA: 1991.0s / 323.6s
Train: 76 [4350/5000 ( 87%)]  Loss: 3.92 (4.11)  Time: 0.444s,   45.06/s  (0.463s,   43.21/s)  LR: 1.752e-05  Data: 0.012 (0.010)  Elapsed/ETA: 2013.7s / 300.4s
Train: 76 [4400/5000 ( 88%)]  Loss: 4.63 (4.11)  Time: 0.438s,   45.66/s  (0.463s,   43.21/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 2036.9s / 277.2s
Train: 76 [4450/5000 ( 89%)]  Loss: 4.00 (4.11)  Time: 0.450s,   44.42/s  (0.464s,   43.13/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 2064.0s / 254.6s
Train: 76 [4500/5000 ( 90%)]  Loss: 3.76 (4.11)  Time: 0.448s,   44.66/s  (0.464s,   43.08/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 2089.6s / 231.7s
Train: 76 [4550/5000 ( 91%)]  Loss: 3.75 (4.12)  Time: 0.542s,   36.88/s  (0.464s,   43.07/s)  LR: 1.752e-05  Data: 0.012 (0.010)  Elapsed/ETA: 2113.2s / 208.5s
Train: 76 [4600/5000 ( 92%)]  Loss: 3.43 (4.12)  Time: 0.448s,   44.63/s  (0.465s,   43.03/s)  LR: 1.752e-05  Data: 0.014 (0.010)  Elapsed/ETA: 2138.4s / 185.4s
Train: 76 [4650/5000 ( 93%)]  Loss: 3.96 (4.12)  Time: 0.440s,   45.43/s  (0.465s,   43.04/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 2161.3s / 162.2s
Train: 76 [4700/5000 ( 94%)]  Loss: 4.13 (4.12)  Time: 0.436s,   45.85/s  (0.464s,   43.06/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 2183.4s / 138.9s
Train: 76 [4750/5000 ( 95%)]  Loss: 4.25 (4.12)  Time: 0.435s,   45.98/s  (0.464s,   43.09/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 2205.3s / 115.6s
Train: 76 [4800/5000 ( 96%)]  Loss: 4.40 (4.12)  Time: 0.436s,   45.87/s  (0.464s,   43.11/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 2227.2s / 92.3s
Train: 76 [4850/5000 ( 97%)]  Loss: 4.50 (4.12)  Time: 0.437s,   45.77/s  (0.464s,   43.14/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 2249.0s / 69.1s
Train: 76 [4900/5000 ( 98%)]  Loss: 4.91 (4.12)  Time: 0.481s,   41.54/s  (0.464s,   43.13/s)  LR: 1.752e-05  Data: 0.010 (0.010)  Elapsed/ETA: 2272.5s / 45.9s
Train: 76 [4950/5000 ( 99%)]  Loss: 4.02 (4.12)  Time: 0.482s,   41.52/s  (0.464s,   43.14/s)  LR: 1.752e-05  Data: 0.011 (0.010)  Elapsed/ETA: 2295.4s / 22.7s
Test: [   0/499]  Time: 0.731 (0.731)  Loss:   0.771 ( 0.771)  Acc@1:  85.000 ( 85.000)  Acc@5:  95.000 ( 95.000)
Test: [  50/499]  Time: 0.091 (0.098)  Loss:   2.002 ( 2.297)  Acc@1:  65.000 ( 50.196)  Acc@5:  75.000 ( 73.922)
Test: [ 100/499]  Time: 0.082 (0.091)  Loss:   2.352 ( 2.332)  Acc@1:  55.000 ( 47.426)  Acc@5:  70.000 ( 73.515)
Test: [ 150/499]  Time: 0.082 (0.089)  Loss:   2.133 ( 2.291)  Acc@1:  55.000 ( 48.510)  Acc@5:  85.000 ( 74.371)
Test: [ 200/499]  Time: 0.083 (0.088)  Loss:   3.657 ( 2.379)  Acc@1:  10.000 ( 46.219)  Acc@5:  40.000 ( 72.861)
Test: [ 250/499]  Time: 0.086 (0.087)  Loss:   3.581 ( 2.397)  Acc@1:  10.000 ( 45.916)  Acc@5:  40.000 ( 72.430)
Test: [ 300/499]  Time: 0.082 (0.087)  Loss:   2.816 ( 2.378)  Acc@1:  35.000 ( 46.744)  Acc@5:  60.000 ( 72.658)
Test: [ 350/499]  Time: 0.083 (0.086)  Loss:   2.684 ( 2.427)  Acc@1:  35.000 ( 45.328)  Acc@5:  70.000 ( 71.496)
Test: [ 400/499]  Time: 0.086 (0.086)  Loss:   3.286 ( 2.420)  Acc@1:  40.000 ( 45.923)  Acc@5:  55.000 ( 71.633)
Test: [ 450/499]  Time: 0.085 (0.086)  Loss:   3.042 ( 2.413)  Acc@1:  40.000 ( 46.042)  Acc@5:  70.000 ( 71.619)
Test: [ 499/499]  Time: 0.074 (0.085)  Loss:   2.881 ( 2.386)  Acc@1:  50.000 ( 46.650)  Acc@5:  80.000 ( 72.440)
Current checkpoints:
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-76.pth.tar', 46.65)

Train: 77 [   0/5000 (  0%)]  Loss: 4.47 (4.47)  Time: 0.848s,   23.59/s  (0.848s,   23.59/s)  LR: 1.762e-05  Data: 0.314 (0.314)  Elapsed/ETA: 0.8s / 4240.0s
Train: 77 [  50/5000 (  1%)]  Loss: 4.94 (4.10)  Time: 0.478s,   41.85/s  (0.508s,   39.35/s)  LR: 1.762e-05  Data: 0.011 (0.017)  Elapsed/ETA: 25.9s / 2515.2s
Train: 77 [ 100/5000 (  2%)]  Loss: 4.16 (4.16)  Time: 0.435s,   45.99/s  (0.485s,   41.25/s)  LR: 1.762e-05  Data: 0.010 (0.014)  Elapsed/ETA: 49.0s / 2375.3s
Train: 77 [ 150/5000 (  3%)]  Loss: 4.07 (4.15)  Time: 0.434s,   46.04/s  (0.487s,   41.10/s)  LR: 1.762e-05  Data: 0.011 (0.013)  Elapsed/ETA: 73.5s / 2359.9s
Train: 77 [ 200/5000 (  4%)]  Loss: 3.03 (4.15)  Time: 0.436s,   45.84/s  (0.497s,   40.23/s)  LR: 1.762e-05  Data: 0.011 (0.013)  Elapsed/ETA: 99.9s / 2385.6s
Train: 77 [ 250/5000 (  5%)]  Loss: 4.09 (4.14)  Time: 0.438s,   45.65/s  (0.487s,   41.07/s)  LR: 1.762e-05  Data: 0.011 (0.012)  Elapsed/ETA: 122.2s / 2312.6s
Train: 77 [ 300/5000 (  6%)]  Loss: 4.57 (4.15)  Time: 0.440s,   45.45/s  (0.482s,   41.50/s)  LR: 1.762e-05  Data: 0.013 (0.012)  Elapsed/ETA: 145.0s / 2264.3s
Train: 77 [ 350/5000 (  7%)]  Loss: 3.91 (4.14)  Time: 0.435s,   45.98/s  (0.475s,   42.08/s)  LR: 1.762e-05  Data: 0.011 (0.012)  Elapsed/ETA: 166.8s / 2209.4s
Train: 77 [ 400/5000 (  8%)]  Loss: 4.17 (4.14)  Time: 0.566s,   35.33/s  (0.477s,   41.90/s)  LR: 1.762e-05  Data: 0.010 (0.012)  Elapsed/ETA: 191.4s / 2195.4s
Train: 77 [ 450/5000 (  9%)]  Loss: 4.43 (4.14)  Time: 0.437s,   45.77/s  (0.473s,   42.29/s)  LR: 1.762e-05  Data: 0.010 (0.012)  Elapsed/ETA: 213.3s / 2151.3s
Train: 77 [ 500/5000 ( 10%)]  Loss: 4.33 (4.14)  Time: 0.482s,   41.50/s  (0.471s,   42.48/s)  LR: 1.762e-05  Data: 0.011 (0.012)  Elapsed/ETA: 235.9s / 2118.1s
Train: 77 [ 550/5000 ( 11%)]  Loss: 4.40 (4.14)  Time: 0.439s,   45.60/s  (0.471s,   42.42/s)  LR: 1.762e-05  Data: 0.011 (0.012)  Elapsed/ETA: 259.8s / 2097.6s
Train: 77 [ 600/5000 ( 12%)]  Loss: 4.20 (4.13)  Time: 0.434s,   46.04/s  (0.469s,   42.60/s)  LR: 1.762e-05  Data: 0.011 (0.012)  Elapsed/ETA: 282.2s / 2065.2s
Train: 77 [ 650/5000 ( 13%)]  Loss: 3.55 (4.13)  Time: 0.442s,   45.22/s  (0.468s,   42.76/s)  LR: 1.762e-05  Data: 0.011 (0.012)  Elapsed/ETA: 304.5s / 2034.0s
Train: 77 [ 700/5000 ( 14%)]  Loss: 3.98 (4.13)  Time: 0.440s,   45.43/s  (0.466s,   42.89/s)  LR: 1.762e-05  Data: 0.011 (0.012)  Elapsed/ETA: 326.8s / 2004.4s
Train: 77 [ 750/5000 ( 15%)]  Loss: 3.15 (4.12)  Time: 0.439s,   45.53/s  (0.466s,   42.91/s)  LR: 1.762e-05  Data: 0.015 (0.012)  Elapsed/ETA: 350.0s / 1980.2s
Train: 77 [ 800/5000 ( 16%)]  Loss: 4.30 (4.12)  Time: 0.435s,   45.99/s  (0.466s,   42.88/s)  LR: 1.762e-05  Data: 0.010 (0.012)  Elapsed/ETA: 373.6s / 1958.4s
Train: 77 [ 850/5000 ( 17%)]  Loss: 4.06 (4.12)  Time: 0.441s,   45.40/s  (0.467s,   42.83/s)  LR: 1.762e-05  Data: 0.013 (0.012)  Elapsed/ETA: 397.4s / 1937.3s
Train: 77 [ 900/5000 ( 18%)]  Loss: 3.08 (4.12)  Time: 0.584s,   34.26/s  (0.468s,   42.73/s)  LR: 1.762e-05  Data: 0.011 (0.012)  Elapsed/ETA: 421.7s / 1918.6s
Train: 77 [ 950/5000 ( 19%)]  Loss: 4.62 (4.12)  Time: 0.435s,   45.93/s  (0.468s,   42.74/s)  LR: 1.762e-05  Data: 0.011 (0.012)  Elapsed/ETA: 445.1s / 1894.9s
Train: 77 [1000/5000 ( 20%)]  Loss: 4.67 (4.12)  Time: 0.475s,   42.09/s  (0.466s,   42.88/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 466.9s / 1865.4s
Train: 77 [1050/5000 ( 21%)]  Loss: 3.26 (4.11)  Time: 0.432s,   46.26/s  (0.465s,   43.00/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 488.9s / 1836.9s
Train: 77 [1100/5000 ( 22%)]  Loss: 3.37 (4.11)  Time: 0.435s,   45.96/s  (0.466s,   42.95/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 512.7s / 1815.5s
Train: 77 [1150/5000 ( 23%)]  Loss: 4.71 (4.11)  Time: 0.431s,   46.44/s  (0.465s,   43.02/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 535.1s / 1789.6s
Train: 77 [1200/5000 ( 24%)]  Loss: 3.78 (4.12)  Time: 0.448s,   44.60/s  (0.466s,   42.96/s)  LR: 1.762e-05  Data: 0.014 (0.011)  Elapsed/ETA: 559.1s / 1768.6s
Train: 77 [1250/5000 ( 25%)]  Loss: 4.65 (4.12)  Time: 0.447s,   44.70/s  (0.465s,   42.97/s)  LR: 1.762e-05  Data: 0.013 (0.011)  Elapsed/ETA: 582.2s / 1744.8s
Train: 77 [1300/5000 ( 26%)]  Loss: 4.72 (4.12)  Time: 0.437s,   45.72/s  (0.467s,   42.83/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 607.5s / 1727.3s
Train: 77 [1350/5000 ( 27%)]  Loss: 4.14 (4.12)  Time: 0.575s,   34.78/s  (0.468s,   42.74/s)  LR: 1.762e-05  Data: 0.012 (0.011)  Elapsed/ETA: 632.2s / 1707.4s
Train: 77 [1400/5000 ( 28%)]  Loss: 4.39 (4.12)  Time: 0.440s,   45.44/s  (0.469s,   42.68/s)  LR: 1.762e-05  Data: 0.012 (0.011)  Elapsed/ETA: 656.6s / 1686.6s
Train: 77 [1450/5000 ( 29%)]  Loss: 4.12 (4.12)  Time: 0.448s,   44.65/s  (0.468s,   42.70/s)  LR: 1.762e-05  Data: 0.013 (0.011)  Elapsed/ETA: 679.7s / 1662.4s
Train: 77 [1500/5000 ( 30%)]  Loss: 4.75 (4.12)  Time: 0.442s,   45.28/s  (0.468s,   42.76/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 702.1s / 1636.8s
Train: 77 [1550/5000 ( 31%)]  Loss: 4.52 (4.12)  Time: 0.581s,   34.41/s  (0.468s,   42.71/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 726.2s / 1615.0s
Train: 77 [1600/5000 ( 32%)]  Loss: 3.83 (4.12)  Time: 0.440s,   45.50/s  (0.469s,   42.62/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 751.3s / 1595.1s
Train: 77 [1650/5000 ( 33%)]  Loss: 3.77 (4.12)  Time: 0.440s,   45.50/s  (0.469s,   42.62/s)  LR: 1.762e-05  Data: 0.014 (0.011)  Elapsed/ETA: 774.7s / 1571.5s
Train: 77 [1700/5000 ( 34%)]  Loss: 4.41 (4.12)  Time: 0.445s,   44.91/s  (0.469s,   42.64/s)  LR: 1.762e-05  Data: 0.008 (0.011)  Elapsed/ETA: 797.8s / 1547.3s
Train: 77 [1750/5000 ( 35%)]  Loss: 4.95 (4.12)  Time: 0.443s,   45.20/s  (0.470s,   42.59/s)  LR: 1.762e-05  Data: 0.013 (0.011)  Elapsed/ETA: 822.3s / 1525.8s
Train: 77 [1800/5000 ( 36%)]  Loss: 4.23 (4.11)  Time: 0.579s,   34.57/s  (0.470s,   42.53/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 846.9s / 1504.2s
Train: 77 [1850/5000 ( 37%)]  Loss: 3.55 (4.11)  Time: 0.431s,   46.45/s  (0.470s,   42.58/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 869.4s / 1479.1s
Train: 77 [1900/5000 ( 38%)]  Loss: 3.56 (4.11)  Time: 0.447s,   44.79/s  (0.469s,   42.60/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 892.4s / 1454.8s
Train: 77 [1950/5000 ( 39%)]  Loss: 4.11 (4.11)  Time: 0.441s,   45.40/s  (0.469s,   42.61/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 915.8s / 1431.2s
Train: 77 [2000/5000 ( 40%)]  Loss: 4.13 (4.11)  Time: 0.485s,   41.24/s  (0.470s,   42.55/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 940.4s / 1409.5s
Train: 77 [2050/5000 ( 41%)]  Loss: 2.97 (4.11)  Time: 0.476s,   41.99/s  (0.470s,   42.59/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 963.1s / 1384.8s
Train: 77 [2100/5000 ( 42%)]  Loss: 3.35 (4.11)  Time: 0.584s,   34.24/s  (0.470s,   42.51/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 988.4s / 1363.8s
Train: 77 [2150/5000 ( 43%)]  Loss: 4.30 (4.11)  Time: 0.436s,   45.90/s  (0.471s,   42.49/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1012.5s / 1341.1s
Train: 77 [2200/5000 ( 44%)]  Loss: 3.99 (4.11)  Time: 0.539s,   37.09/s  (0.470s,   42.54/s)  LR: 1.762e-05  Data: 0.013 (0.011)  Elapsed/ETA: 1034.9s / 1316.1s
Train: 77 [2250/5000 ( 45%)]  Loss: 3.80 (4.11)  Time: 0.430s,   46.54/s  (0.470s,   42.58/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1057.4s / 1291.3s
Train: 77 [2300/5000 ( 46%)]  Loss: 3.67 (4.11)  Time: 0.450s,   44.46/s  (0.469s,   42.60/s)  LR: 1.762e-05  Data: 0.014 (0.011)  Elapsed/ETA: 1080.3s / 1267.1s
Train: 77 [2350/5000 ( 47%)]  Loss: 3.18 (4.10)  Time: 0.459s,   43.53/s  (0.469s,   42.62/s)  LR: 1.762e-05  Data: 0.013 (0.011)  Elapsed/ETA: 1103.2s / 1243.0s
Train: 77 [2400/5000 ( 48%)]  Loss: 3.84 (4.11)  Time: 0.438s,   45.71/s  (0.469s,   42.69/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1125.0s / 1217.8s
Train: 77 [2450/5000 ( 49%)]  Loss: 4.33 (4.11)  Time: 0.439s,   45.57/s  (0.468s,   42.75/s)  LR: 1.762e-05  Data: 0.015 (0.011)  Elapsed/ETA: 1146.8s / 1192.6s
Train: 77 [2500/5000 ( 50%)]  Loss: 3.95 (4.11)  Time: 0.432s,   46.31/s  (0.467s,   42.80/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1168.6s / 1167.7s
Train: 77 [2550/5000 ( 51%)]  Loss: 3.65 (4.10)  Time: 0.526s,   37.99/s  (0.467s,   42.85/s)  LR: 1.762e-05  Data: 0.013 (0.011)  Elapsed/ETA: 1190.6s / 1143.0s
Train: 77 [2600/5000 ( 52%)]  Loss: 4.20 (4.11)  Time: 0.435s,   45.99/s  (0.467s,   42.79/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1215.7s / 1121.3s
Train: 77 [2650/5000 ( 53%)]  Loss: 4.74 (4.11)  Time: 0.575s,   34.78/s  (0.468s,   42.71/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1241.4s / 1099.9s
Train: 77 [2700/5000 ( 54%)]  Loss: 3.81 (4.11)  Time: 0.430s,   46.50/s  (0.469s,   42.67/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1266.0s / 1077.5s
Train: 77 [2750/5000 ( 55%)]  Loss: 4.56 (4.11)  Time: 0.434s,   46.08/s  (0.468s,   42.72/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1287.8s / 1052.8s
Train: 77 [2800/5000 ( 56%)]  Loss: 4.98 (4.11)  Time: 0.444s,   45.09/s  (0.468s,   42.74/s)  LR: 1.762e-05  Data: 0.013 (0.011)  Elapsed/ETA: 1310.8s / 1029.1s
Train: 77 [2850/5000 ( 57%)]  Loss: 4.10 (4.11)  Time: 0.437s,   45.79/s  (0.468s,   42.77/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1333.2s / 1004.9s
Train: 77 [2900/5000 ( 58%)]  Loss: 2.84 (4.11)  Time: 0.432s,   46.32/s  (0.468s,   42.74/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1357.5s / 982.2s
Train: 77 [2950/5000 ( 59%)]  Loss: 3.48 (4.11)  Time: 0.440s,   45.45/s  (0.468s,   42.73/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1381.3s / 959.1s
Train: 77 [3000/5000 ( 60%)]  Loss: 4.55 (4.10)  Time: 0.436s,   45.86/s  (0.468s,   42.70/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1405.6s / 936.3s
Train: 77 [3050/5000 ( 61%)]  Loss: 4.11 (4.10)  Time: 0.433s,   46.19/s  (0.468s,   42.74/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1427.6s / 912.0s
Train: 77 [3100/5000 ( 62%)]  Loss: 3.67 (4.10)  Time: 0.570s,   35.08/s  (0.469s,   42.69/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1452.9s / 889.7s
Train: 77 [3150/5000 ( 63%)]  Loss: 3.76 (4.10)  Time: 0.438s,   45.66/s  (0.468s,   42.69/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1476.2s / 866.2s
Train: 77 [3200/5000 ( 64%)]  Loss: 4.72 (4.11)  Time: 0.576s,   34.71/s  (0.469s,   42.67/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1500.5s / 843.3s
Train: 77 [3250/5000 ( 65%)]  Loss: 4.48 (4.10)  Time: 0.440s,   45.41/s  (0.469s,   42.66/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1524.1s / 820.0s
Train: 77 [3300/5000 ( 66%)]  Loss: 4.79 (4.10)  Time: 0.440s,   45.49/s  (0.469s,   42.65/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1548.0s / 796.8s
Train: 77 [3350/5000 ( 67%)]  Loss: 4.54 (4.11)  Time: 0.440s,   45.45/s  (0.469s,   42.67/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1570.5s / 772.8s
Train: 77 [3400/5000 ( 68%)]  Loss: 3.85 (4.11)  Time: 0.570s,   35.07/s  (0.469s,   42.69/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1593.4s / 749.2s
Train: 77 [3450/5000 ( 69%)]  Loss: 3.98 (4.11)  Time: 0.433s,   46.14/s  (0.468s,   42.71/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1615.9s / 725.3s
Train: 77 [3500/5000 ( 70%)]  Loss: 4.46 (4.11)  Time: 0.435s,   45.96/s  (0.468s,   42.75/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1637.8s / 701.2s
Train: 77 [3550/5000 ( 71%)]  Loss: 4.48 (4.10)  Time: 0.569s,   35.12/s  (0.468s,   42.75/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1661.4s / 678.0s
Train: 77 [3600/5000 ( 72%)]  Loss: 4.77 (4.10)  Time: 0.431s,   46.36/s  (0.468s,   42.78/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1683.6s / 654.1s
Train: 77 [3650/5000 ( 73%)]  Loss: 4.79 (4.11)  Time: 0.432s,   46.31/s  (0.467s,   42.82/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1705.2s / 630.1s
Train: 77 [3700/5000 ( 74%)]  Loss: 3.71 (4.11)  Time: 0.433s,   46.15/s  (0.467s,   42.85/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1727.3s / 606.3s
Train: 77 [3750/5000 ( 75%)]  Loss: 4.26 (4.11)  Time: 0.433s,   46.24/s  (0.466s,   42.89/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1749.0s / 582.4s
Train: 77 [3800/5000 ( 76%)]  Loss: 3.89 (4.11)  Time: 0.433s,   46.20/s  (0.466s,   42.93/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1770.8s / 558.6s
Train: 77 [3850/5000 ( 77%)]  Loss: 4.30 (4.11)  Time: 0.435s,   46.02/s  (0.465s,   42.97/s)  LR: 1.762e-05  Data: 0.012 (0.011)  Elapsed/ETA: 1792.5s / 534.8s
Train: 77 [3900/5000 ( 78%)]  Loss: 3.48 (4.11)  Time: 0.433s,   46.18/s  (0.465s,   43.00/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1814.3s / 511.1s
Train: 77 [3950/5000 ( 79%)]  Loss: 3.35 (4.11)  Time: 0.430s,   46.51/s  (0.465s,   43.04/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1835.9s / 487.4s
Train: 77 [4000/5000 ( 80%)]  Loss: 4.10 (4.11)  Time: 0.433s,   46.18/s  (0.464s,   43.08/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1857.7s / 463.8s
Train: 77 [4050/5000 ( 81%)]  Loss: 4.14 (4.11)  Time: 0.436s,   45.83/s  (0.464s,   43.11/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1879.5s / 440.3s
Train: 77 [4100/5000 ( 82%)]  Loss: 4.39 (4.11)  Time: 0.433s,   46.16/s  (0.464s,   43.13/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1901.5s / 416.8s
Train: 77 [4150/5000 ( 83%)]  Loss: 4.11 (4.11)  Time: 0.441s,   45.34/s  (0.463s,   43.16/s)  LR: 1.762e-05  Data: 0.014 (0.011)  Elapsed/ETA: 1923.7s / 393.4s
Train: 77 [4200/5000 ( 84%)]  Loss: 3.36 (4.11)  Time: 0.432s,   46.34/s  (0.463s,   43.18/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1945.8s / 370.1s
Train: 77 [4250/5000 ( 85%)]  Loss: 4.50 (4.11)  Time: 0.435s,   45.97/s  (0.463s,   43.20/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1968.1s / 346.8s
Train: 77 [4300/5000 ( 86%)]  Loss: 4.37 (4.11)  Time: 0.436s,   45.83/s  (0.463s,   43.15/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1993.4s / 324.0s
Train: 77 [4350/5000 ( 87%)]  Loss: 4.66 (4.11)  Time: 0.439s,   45.54/s  (0.464s,   43.14/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 2017.0s / 300.9s
Train: 77 [4400/5000 ( 88%)]  Loss: 4.05 (4.11)  Time: 0.440s,   45.48/s  (0.464s,   43.12/s)  LR: 1.762e-05  Data: 0.015 (0.011)  Elapsed/ETA: 2041.0s / 277.8s
Train: 77 [4450/5000 ( 89%)]  Loss: 4.70 (4.11)  Time: 0.434s,   46.07/s  (0.464s,   43.06/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2067.4s / 255.0s
Train: 77 [4500/5000 ( 90%)]  Loss: 4.64 (4.11)  Time: 0.446s,   44.86/s  (0.464s,   43.07/s)  LR: 1.762e-05  Data: 0.012 (0.011)  Elapsed/ETA: 2090.2s / 231.7s
Train: 77 [4550/5000 ( 91%)]  Loss: 4.00 (4.11)  Time: 0.437s,   45.74/s  (0.464s,   43.08/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2112.7s / 208.4s
Train: 77 [4600/5000 ( 92%)]  Loss: 4.83 (4.11)  Time: 0.435s,   45.93/s  (0.464s,   43.08/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 2136.1s / 185.2s
Train: 77 [4650/5000 ( 93%)]  Loss: 3.74 (4.11)  Time: 0.444s,   45.04/s  (0.465s,   43.05/s)  LR: 1.762e-05  Data: 0.012 (0.011)  Elapsed/ETA: 2160.5s / 162.1s
Train: 77 [4700/5000 ( 94%)]  Loss: 4.07 (4.11)  Time: 0.444s,   45.00/s  (0.464s,   43.08/s)  LR: 1.762e-05  Data: 0.012 (0.011)  Elapsed/ETA: 2182.5s / 138.8s
Train: 77 [4750/5000 ( 95%)]  Loss: 4.57 (4.11)  Time: 0.438s,   45.68/s  (0.464s,   43.06/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2206.7s / 115.7s
Train: 77 [4800/5000 ( 96%)]  Loss: 4.53 (4.11)  Time: 0.438s,   45.68/s  (0.465s,   43.03/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2231.3s / 92.5s
Train: 77 [4850/5000 ( 97%)]  Loss: 3.92 (4.11)  Time: 0.583s,   34.32/s  (0.466s,   42.95/s)  LR: 1.762e-05  Data: 0.010 (0.011)  Elapsed/ETA: 2258.9s / 69.4s
Train: 77 [4900/5000 ( 98%)]  Loss: 4.02 (4.11)  Time: 0.487s,   41.09/s  (0.466s,   42.96/s)  LR: 1.762e-05  Data: 0.012 (0.011)  Elapsed/ETA: 2281.8s / 46.1s
Train: 77 [4950/5000 ( 99%)]  Loss: 4.01 (4.11)  Time: 0.445s,   44.92/s  (0.465s,   42.97/s)  LR: 1.762e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2304.5s / 22.8s
Test: [   0/499]  Time: 0.239 (0.239)  Loss:   0.929 ( 0.929)  Acc@1:  85.000 ( 85.000)  Acc@5:  95.000 ( 95.000)
Test: [  50/499]  Time: 0.085 (0.088)  Loss:   1.529 ( 2.265)  Acc@1:  80.000 ( 50.980)  Acc@5:  85.000 ( 75.392)
Test: [ 100/499]  Time: 0.088 (0.086)  Loss:   2.495 ( 2.278)  Acc@1:  30.000 ( 49.158)  Acc@5:  70.000 ( 74.851)
Test: [ 150/499]  Time: 0.083 (0.086)  Loss:   1.858 ( 2.236)  Acc@1:  65.000 ( 50.033)  Acc@5:  85.000 ( 75.397)
Test: [ 200/499]  Time: 0.085 (0.086)  Loss:   3.891 ( 2.346)  Acc@1:   5.000 ( 47.363)  Acc@5:  35.000 ( 73.607)
Test: [ 250/499]  Time: 0.090 (0.086)  Loss:   3.572 ( 2.379)  Acc@1:  15.000 ( 46.733)  Acc@5:  50.000 ( 73.008)
Test: [ 300/499]  Time: 0.090 (0.087)  Loss:   2.994 ( 2.371)  Acc@1:  40.000 ( 47.226)  Acc@5:  55.000 ( 73.189)
Test: [ 350/499]  Time: 0.089 (0.087)  Loss:   3.054 ( 2.434)  Acc@1:  30.000 ( 45.556)  Acc@5:  65.000 ( 71.752)
Test: [ 400/499]  Time: 0.085 (0.087)  Loss:   3.721 ( 2.434)  Acc@1:  25.000 ( 45.973)  Acc@5:  50.000 ( 71.945)
Test: [ 450/499]  Time: 0.090 (0.086)  Loss:   3.607 ( 2.434)  Acc@1:  20.000 ( 45.876)  Acc@5:  50.000 ( 71.851)
Test: [ 499/499]  Time: 0.074 (0.086)  Loss:   2.811 ( 2.400)  Acc@1:  55.000 ( 46.760)  Acc@5:  70.000 ( 72.760)
Current checkpoints:
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-77.pth.tar', 46.76)
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-76.pth.tar', 46.65)

Train: 78 [   0/5000 (  0%)]  Loss: 4.17 (4.17)  Time: 0.799s,   25.05/s  (0.799s,   25.05/s)  LR: 1.772e-05  Data: 0.333 (0.333)  Elapsed/ETA: 0.8s / 3992.7s
Train: 78 [  50/5000 (  1%)]  Loss: 3.26 (4.00)  Time: 0.434s,   46.09/s  (0.446s,   44.81/s)  LR: 1.772e-05  Data: 0.011 (0.018)  Elapsed/ETA: 22.8s / 2208.8s
Train: 78 [ 100/5000 (  2%)]  Loss: 4.10 (4.00)  Time: 0.437s,   45.72/s  (0.443s,   45.18/s)  LR: 1.772e-05  Data: 0.011 (0.015)  Elapsed/ETA: 44.7s / 2168.7s
Train: 78 [ 150/5000 (  3%)]  Loss: 4.54 (4.02)  Time: 0.442s,   45.24/s  (0.443s,   45.16/s)  LR: 1.772e-05  Data: 0.011 (0.014)  Elapsed/ETA: 66.9s / 2147.3s
Train: 78 [ 200/5000 (  4%)]  Loss: 4.46 (4.02)  Time: 0.439s,   45.56/s  (0.450s,   44.47/s)  LR: 1.772e-05  Data: 0.011 (0.013)  Elapsed/ETA: 90.4s / 2158.5s
Train: 78 [ 250/5000 (  5%)]  Loss: 3.92 (4.05)  Time: 0.436s,   45.92/s  (0.450s,   44.49/s)  LR: 1.772e-05  Data: 0.011 (0.013)  Elapsed/ETA: 112.8s / 2134.7s
Train: 78 [ 300/5000 (  6%)]  Loss: 3.79 (4.08)  Time: 0.437s,   45.77/s  (0.453s,   44.17/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 136.3s / 2127.5s
Train: 78 [ 350/5000 (  7%)]  Loss: 4.08 (4.07)  Time: 0.436s,   45.85/s  (0.459s,   43.55/s)  LR: 1.772e-05  Data: 0.009 (0.012)  Elapsed/ETA: 161.2s / 2134.9s
Train: 78 [ 400/5000 (  8%)]  Loss: 3.60 (4.09)  Time: 0.436s,   45.84/s  (0.459s,   43.58/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 184.0s / 2110.5s
Train: 78 [ 450/5000 (  9%)]  Loss: 3.90 (4.08)  Time: 0.449s,   44.57/s  (0.457s,   43.79/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 206.0s / 2077.7s
Train: 78 [ 500/5000 ( 10%)]  Loss: 4.13 (4.08)  Time: 0.577s,   34.64/s  (0.464s,   43.08/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 232.6s / 2088.9s
Train: 78 [ 550/5000 ( 11%)]  Loss: 4.07 (4.08)  Time: 0.437s,   45.78/s  (0.463s,   43.18/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 255.2s / 2060.7s
Train: 78 [ 600/5000 ( 12%)]  Loss: 4.77 (4.08)  Time: 0.437s,   45.81/s  (0.461s,   43.38/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 277.1s / 2028.3s
Train: 78 [ 650/5000 ( 13%)]  Loss: 4.42 (4.08)  Time: 0.436s,   45.87/s  (0.460s,   43.52/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 299.2s / 1998.7s
Train: 78 [ 700/5000 ( 14%)]  Loss: 4.70 (4.09)  Time: 0.445s,   44.91/s  (0.460s,   43.52/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 322.1s / 1975.6s
Train: 78 [ 750/5000 ( 15%)]  Loss: 3.96 (4.09)  Time: 0.442s,   45.22/s  (0.460s,   43.51/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 345.2s / 1952.9s
Train: 78 [ 800/5000 ( 16%)]  Loss: 4.44 (4.09)  Time: 0.437s,   45.73/s  (0.458s,   43.64/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 367.1s / 1924.2s
Train: 78 [ 850/5000 ( 17%)]  Loss: 3.91 (4.09)  Time: 0.443s,   45.12/s  (0.457s,   43.74/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 389.1s / 1897.1s
Train: 78 [ 900/5000 ( 18%)]  Loss: 3.63 (4.10)  Time: 0.577s,   34.66/s  (0.459s,   43.55/s)  LR: 1.772e-05  Data: 0.009 (0.012)  Elapsed/ETA: 413.8s / 1882.4s
Train: 78 [ 950/5000 ( 19%)]  Loss: 3.12 (4.10)  Time: 0.447s,   44.71/s  (0.460s,   43.43/s)  LR: 1.772e-05  Data: 0.010 (0.012)  Elapsed/ETA: 437.9s / 1864.5s
Train: 78 [1000/5000 ( 20%)]  Loss: 5.02 (4.10)  Time: 0.436s,   45.86/s  (0.462s,   43.31/s)  LR: 1.772e-05  Data: 0.010 (0.012)  Elapsed/ETA: 462.2s / 1846.6s
Train: 78 [1050/5000 ( 21%)]  Loss: 3.10 (4.11)  Time: 0.441s,   45.33/s  (0.464s,   43.09/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 487.8s / 1833.0s
Train: 78 [1100/5000 ( 22%)]  Loss: 4.53 (4.11)  Time: 0.436s,   45.91/s  (0.463s,   43.20/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 509.8s / 1805.3s
Train: 78 [1150/5000 ( 23%)]  Loss: 3.42 (4.11)  Time: 0.441s,   45.32/s  (0.464s,   43.09/s)  LR: 1.772e-05  Data: 0.010 (0.012)  Elapsed/ETA: 534.3s / 1786.6s
Train: 78 [1200/5000 ( 24%)]  Loss: 3.46 (4.11)  Time: 0.574s,   34.83/s  (0.464s,   43.10/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 557.3s / 1762.7s
Train: 78 [1250/5000 ( 25%)]  Loss: 4.36 (4.10)  Time: 0.573s,   34.91/s  (0.467s,   42.87/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 583.7s / 1749.1s
Train: 78 [1300/5000 ( 26%)]  Loss: 4.21 (4.10)  Time: 0.440s,   45.47/s  (0.467s,   42.80/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 607.9s / 1728.5s
Train: 78 [1350/5000 ( 27%)]  Loss: 3.26 (4.10)  Time: 0.435s,   46.03/s  (0.470s,   42.59/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 634.5s / 1713.7s
Train: 78 [1400/5000 ( 28%)]  Loss: 3.30 (4.10)  Time: 0.468s,   42.76/s  (0.469s,   42.61/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 657.5s / 1689.1s
Train: 78 [1450/5000 ( 29%)]  Loss: 3.72 (4.10)  Time: 0.438s,   45.70/s  (0.470s,   42.58/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 681.5s / 1666.8s
Train: 78 [1500/5000 ( 30%)]  Loss: 4.07 (4.11)  Time: 0.442s,   45.29/s  (0.469s,   42.67/s)  LR: 1.772e-05  Data: 0.013 (0.011)  Elapsed/ETA: 703.5s / 1640.0s
Train: 78 [1550/5000 ( 31%)]  Loss: 3.70 (4.11)  Time: 0.439s,   45.54/s  (0.468s,   42.73/s)  LR: 1.772e-05  Data: 0.011 (0.011)  Elapsed/ETA: 726.0s / 1614.5s
Train: 78 [1600/5000 ( 32%)]  Loss: 3.56 (4.11)  Time: 0.511s,   39.14/s  (0.468s,   42.74/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 749.2s / 1590.5s
Train: 78 [1650/5000 ( 33%)]  Loss: 4.42 (4.11)  Time: 0.438s,   45.67/s  (0.475s,   42.11/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 784.2s / 1590.6s
Train: 78 [1700/5000 ( 34%)]  Loss: 3.91 (4.11)  Time: 0.435s,   46.00/s  (0.474s,   42.18/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 806.6s / 1564.4s
Train: 78 [1750/5000 ( 35%)]  Loss: 4.68 (4.11)  Time: 0.468s,   42.78/s  (0.476s,   42.04/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 832.9s / 1545.5s
Train: 78 [1800/5000 ( 36%)]  Loss: 4.60 (4.11)  Time: 0.436s,   45.88/s  (0.475s,   42.14/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 854.9s / 1518.5s
Train: 78 [1850/5000 ( 37%)]  Loss: 4.05 (4.11)  Time: 0.442s,   45.29/s  (0.474s,   42.22/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 876.8s / 1491.6s
Train: 78 [1900/5000 ( 38%)]  Loss: 3.35 (4.11)  Time: 0.448s,   44.62/s  (0.473s,   42.25/s)  LR: 1.772e-05  Data: 0.009 (0.012)  Elapsed/ETA: 899.9s / 1467.0s
Train: 78 [1950/5000 ( 39%)]  Loss: 3.37 (4.11)  Time: 0.577s,   34.66/s  (0.475s,   42.07/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 927.4s / 1449.3s
Train: 78 [2000/5000 ( 40%)]  Loss: 3.44 (4.11)  Time: 0.440s,   45.46/s  (0.475s,   42.10/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 950.6s / 1424.7s
Train: 78 [2050/5000 ( 41%)]  Loss: 4.02 (4.11)  Time: 0.440s,   45.48/s  (0.474s,   42.18/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 972.6s / 1398.4s
Train: 78 [2100/5000 ( 42%)]  Loss: 4.64 (4.11)  Time: 0.435s,   45.97/s  (0.474s,   42.22/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 995.4s / 1373.4s
Train: 78 [2150/5000 ( 43%)]  Loss: 4.07 (4.11)  Time: 0.436s,   45.86/s  (0.473s,   42.29/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1017.2s / 1347.3s
Train: 78 [2200/5000 ( 44%)]  Loss: 3.51 (4.11)  Time: 0.582s,   34.36/s  (0.474s,   42.22/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 1042.6s / 1325.9s
Train: 78 [2250/5000 ( 45%)]  Loss: 5.06 (4.11)  Time: 0.445s,   44.92/s  (0.475s,   42.13/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1068.5s / 1304.9s
Train: 78 [2300/5000 ( 46%)]  Loss: 4.34 (4.11)  Time: 0.574s,   34.87/s  (0.475s,   42.11/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1092.7s / 1281.7s
Train: 78 [2350/5000 ( 47%)]  Loss: 4.63 (4.11)  Time: 0.442s,   45.26/s  (0.477s,   41.94/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1121.0s / 1263.1s
Train: 78 [2400/5000 ( 48%)]  Loss: 4.56 (4.11)  Time: 0.436s,   45.85/s  (0.478s,   41.82/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1148.4s / 1243.1s
Train: 78 [2450/5000 ( 49%)]  Loss: 3.36 (4.11)  Time: 0.438s,   45.71/s  (0.478s,   41.88/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1170.5s / 1217.3s
Train: 78 [2500/5000 ( 50%)]  Loss: 2.88 (4.11)  Time: 0.481s,   41.59/s  (0.477s,   41.90/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1193.8s / 1192.9s
Train: 78 [2550/5000 ( 51%)]  Loss: 4.72 (4.11)  Time: 0.439s,   45.54/s  (0.479s,   41.79/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1220.9s / 1172.1s
Train: 78 [2600/5000 ( 52%)]  Loss: 3.38 (4.11)  Time: 0.576s,   34.73/s  (0.480s,   41.68/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1248.2s / 1151.2s
Train: 78 [2650/5000 ( 53%)]  Loss: 4.20 (4.11)  Time: 0.578s,   34.62/s  (0.482s,   41.52/s)  LR: 1.772e-05  Data: 0.010 (0.012)  Elapsed/ETA: 1277.1s / 1131.6s
Train: 78 [2700/5000 ( 54%)]  Loss: 3.92 (4.11)  Time: 0.439s,   45.54/s  (0.481s,   41.56/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1299.9s / 1106.4s
Train: 78 [2750/5000 ( 55%)]  Loss: 4.25 (4.11)  Time: 0.451s,   44.31/s  (0.481s,   41.59/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 1323.0s / 1081.6s
Train: 78 [2800/5000 ( 56%)]  Loss: 4.79 (4.11)  Time: 0.441s,   45.37/s  (0.481s,   41.61/s)  LR: 1.772e-05  Data: 0.017 (0.012)  Elapsed/ETA: 1346.3s / 1057.0s
Train: 78 [2850/5000 ( 57%)]  Loss: 3.82 (4.11)  Time: 0.436s,   45.82/s  (0.480s,   41.65/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 1368.9s / 1031.9s
Train: 78 [2900/5000 ( 58%)]  Loss: 3.74 (4.11)  Time: 0.444s,   45.00/s  (0.480s,   41.70/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1391.4s / 1006.8s
Train: 78 [2950/5000 ( 59%)]  Loss: 4.60 (4.11)  Time: 0.450s,   44.49/s  (0.479s,   41.74/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1414.1s / 981.9s
Train: 78 [3000/5000 ( 60%)]  Loss: 4.22 (4.11)  Time: 0.479s,   41.77/s  (0.479s,   41.78/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 1436.6s / 956.9s
Train: 78 [3050/5000 ( 61%)]  Loss: 3.77 (4.11)  Time: 0.460s,   43.43/s  (0.479s,   41.80/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1459.9s / 932.6s
Train: 78 [3100/5000 ( 62%)]  Loss: 3.81 (4.11)  Time: 0.448s,   44.64/s  (0.478s,   41.83/s)  LR: 1.772e-05  Data: 0.018 (0.012)  Elapsed/ETA: 1482.7s / 908.0s
Train: 78 [3150/5000 ( 63%)]  Loss: 4.34 (4.11)  Time: 0.531s,   37.68/s  (0.478s,   41.87/s)  LR: 1.772e-05  Data: 0.016 (0.012)  Elapsed/ETA: 1505.2s / 883.3s
Train: 78 [3200/5000 ( 64%)]  Loss: 4.85 (4.11)  Time: 0.446s,   44.83/s  (0.477s,   41.90/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 1527.9s / 858.7s
Train: 78 [3250/5000 ( 65%)]  Loss: 4.16 (4.11)  Time: 0.440s,   45.50/s  (0.477s,   41.92/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 1550.9s / 834.4s
Train: 78 [3300/5000 ( 66%)]  Loss: 3.87 (4.11)  Time: 0.454s,   44.01/s  (0.477s,   41.96/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1573.4s / 809.8s
Train: 78 [3350/5000 ( 67%)]  Loss: 4.45 (4.11)  Time: 0.450s,   44.45/s  (0.476s,   41.98/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 1596.3s / 785.5s
Train: 78 [3400/5000 ( 68%)]  Loss: 3.31 (4.11)  Time: 0.444s,   45.07/s  (0.476s,   42.03/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 1618.4s / 760.9s
Train: 78 [3450/5000 ( 69%)]  Loss: 4.63 (4.11)  Time: 0.443s,   45.19/s  (0.476s,   42.05/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1641.4s / 736.8s
Train: 78 [3500/5000 ( 70%)]  Loss: 3.29 (4.11)  Time: 0.597s,   33.52/s  (0.476s,   42.05/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 1665.0s / 712.9s
Train: 78 [3550/5000 ( 71%)]  Loss: 4.16 (4.11)  Time: 0.440s,   45.43/s  (0.476s,   42.03/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1689.7s / 689.5s
Train: 78 [3600/5000 ( 72%)]  Loss: 2.92 (4.11)  Time: 0.436s,   45.88/s  (0.476s,   42.04/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 1713.2s / 665.6s
Train: 78 [3650/5000 ( 73%)]  Loss: 4.03 (4.11)  Time: 0.475s,   42.08/s  (0.476s,   42.03/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 1737.3s / 641.9s
Train: 78 [3700/5000 ( 74%)]  Loss: 3.57 (4.11)  Time: 0.504s,   39.65/s  (0.476s,   42.05/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 1760.2s / 617.8s
Train: 78 [3750/5000 ( 75%)]  Loss: 3.78 (4.11)  Time: 0.442s,   45.24/s  (0.475s,   42.07/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 1783.4s / 593.8s
Train: 78 [3800/5000 ( 76%)]  Loss: 4.33 (4.11)  Time: 0.441s,   45.38/s  (0.475s,   42.07/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 1807.0s / 570.0s
Train: 78 [3850/5000 ( 77%)]  Loss: 3.16 (4.11)  Time: 0.533s,   37.55/s  (0.475s,   42.08/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 1830.4s / 546.1s
Train: 78 [3900/5000 ( 78%)]  Loss: 4.54 (4.11)  Time: 0.440s,   45.41/s  (0.475s,   42.10/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 1853.3s / 522.1s
Train: 78 [3950/5000 ( 79%)]  Loss: 4.69 (4.11)  Time: 0.446s,   44.85/s  (0.475s,   42.10/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 1876.8s / 498.3s
Train: 78 [4000/5000 ( 80%)]  Loss: 4.72 (4.11)  Time: 0.458s,   43.64/s  (0.475s,   42.11/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1900.3s / 474.5s
Train: 78 [4050/5000 ( 81%)]  Loss: 4.41 (4.11)  Time: 0.544s,   36.74/s  (0.475s,   42.10/s)  LR: 1.772e-05  Data: 0.010 (0.012)  Elapsed/ETA: 1924.5s / 450.8s
Train: 78 [4100/5000 ( 82%)]  Loss: 3.99 (4.11)  Time: 0.438s,   45.67/s  (0.475s,   42.11/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 1947.8s / 427.0s
Train: 78 [4150/5000 ( 83%)]  Loss: 4.42 (4.11)  Time: 0.451s,   44.37/s  (0.475s,   42.13/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 1970.4s / 403.0s
Train: 78 [4200/5000 ( 84%)]  Loss: 4.38 (4.11)  Time: 0.452s,   44.24/s  (0.474s,   42.16/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 1993.1s / 379.1s
Train: 78 [4250/5000 ( 85%)]  Loss: 4.20 (4.11)  Time: 0.468s,   42.75/s  (0.474s,   42.16/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 2016.6s / 355.3s
Train: 78 [4300/5000 ( 86%)]  Loss: 4.35 (4.11)  Time: 0.581s,   34.40/s  (0.475s,   42.12/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 2042.4s / 331.9s
Train: 78 [4350/5000 ( 87%)]  Loss: 3.63 (4.11)  Time: 0.453s,   44.15/s  (0.475s,   42.11/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 2066.3s / 308.2s
Train: 78 [4400/5000 ( 88%)]  Loss: 3.98 (4.11)  Time: 0.609s,   32.86/s  (0.475s,   42.09/s)  LR: 1.772e-05  Data: 0.015 (0.012)  Elapsed/ETA: 2091.2s / 284.6s
Train: 78 [4450/5000 ( 89%)]  Loss: 3.65 (4.11)  Time: 0.648s,   30.88/s  (0.477s,   41.97/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 2121.2s / 261.6s
Train: 78 [4500/5000 ( 90%)]  Loss: 4.38 (4.11)  Time: 0.449s,   44.50/s  (0.478s,   41.88/s)  LR: 1.772e-05  Data: 0.015 (0.012)  Elapsed/ETA: 2149.3s / 238.3s
Train: 78 [4550/5000 ( 91%)]  Loss: 4.07 (4.11)  Time: 0.438s,   45.68/s  (0.477s,   41.90/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 2172.1s / 214.3s
Train: 78 [4600/5000 ( 92%)]  Loss: 3.54 (4.11)  Time: 0.442s,   45.29/s  (0.477s,   41.91/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2195.8s / 190.4s
Train: 78 [4650/5000 ( 93%)]  Loss: 4.82 (4.11)  Time: 0.476s,   42.04/s  (0.477s,   41.91/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 2219.3s / 166.5s
Train: 78 [4700/5000 ( 94%)]  Loss: 4.34 (4.11)  Time: 0.440s,   45.44/s  (0.477s,   41.94/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2241.8s / 142.6s
Train: 78 [4750/5000 ( 95%)]  Loss: 4.76 (4.11)  Time: 0.603s,   33.15/s  (0.477s,   41.95/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 2265.0s / 118.7s
Train: 78 [4800/5000 ( 96%)]  Loss: 4.00 (4.11)  Time: 0.441s,   45.32/s  (0.477s,   41.94/s)  LR: 1.772e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2289.4s / 94.9s
Train: 78 [4850/5000 ( 97%)]  Loss: 4.44 (4.11)  Time: 0.443s,   45.12/s  (0.477s,   41.96/s)  LR: 1.772e-05  Data: 0.014 (0.012)  Elapsed/ETA: 2312.4s / 71.0s
Train: 78 [4900/5000 ( 98%)]  Loss: 4.10 (4.11)  Time: 0.440s,   45.47/s  (0.476s,   41.98/s)  LR: 1.772e-05  Data: 0.013 (0.012)  Elapsed/ETA: 2335.2s / 47.2s
Train: 78 [4950/5000 ( 99%)]  Loss: 4.84 (4.11)  Time: 0.452s,   44.20/s  (0.476s,   41.98/s)  LR: 1.772e-05  Data: 0.012 (0.012)  Elapsed/ETA: 2358.8s / 23.3s
Test: [   0/499]  Time: 0.269 (0.269)  Loss:   0.976 ( 0.976)  Acc@1:  80.000 ( 80.000)  Acc@5:  95.000 ( 95.000)
Test: [  50/499]  Time: 0.087 (0.091)  Loss:   1.858 ( 2.389)  Acc@1:  60.000 ( 49.216)  Acc@5:  80.000 ( 73.137)
Test: [ 100/499]  Time: 0.090 (0.094)  Loss:   2.739 ( 2.406)  Acc@1:  45.000 ( 47.723)  Acc@5:  60.000 ( 72.376)
Test: [ 150/499]  Time: 0.088 (0.092)  Loss:   1.921 ( 2.355)  Acc@1:  60.000 ( 48.278)  Acc@5:  85.000 ( 73.212)
Test: [ 200/499]  Time: 0.088 (0.090)  Loss:   3.405 ( 2.432)  Acc@1:  20.000 ( 46.194)  Acc@5:  45.000 ( 71.915)
Test: [ 250/499]  Time: 0.088 (0.089)  Loss:   3.122 ( 2.444)  Acc@1:  15.000 ( 45.618)  Acc@5:  60.000 ( 71.912)
Test: [ 300/499]  Time: 0.087 (0.088)  Loss:   2.974 ( 2.413)  Acc@1:  35.000 ( 46.528)  Acc@5:  60.000 ( 72.176)
Test: [ 350/499]  Time: 0.086 (0.088)  Loss:   2.605 ( 2.460)  Acc@1:  40.000 ( 45.342)  Acc@5:  65.000 ( 71.140)
Test: [ 400/499]  Time: 0.111 (0.091)  Loss:   3.518 ( 2.446)  Acc@1:  25.000 ( 45.923)  Acc@5:  45.000 ( 71.559)
Test: [ 450/499]  Time: 0.087 (0.091)  Loss:   3.352 ( 2.433)  Acc@1:  20.000 ( 46.286)  Acc@5:  65.000 ( 71.596)
Test: [ 499/499]  Time: 0.097 (0.092)  Loss:   2.762 ( 2.402)  Acc@1:  50.000 ( 47.000)  Acc@5:  75.000 ( 72.450)
Current checkpoints:
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-78.pth.tar', 47.0)
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-77.pth.tar', 46.76)
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-76.pth.tar', 46.65)

Train: 79 [   0/5000 (  0%)]  Loss: 5.12 (5.12)  Time: 0.814s,   24.57/s  (0.814s,   24.57/s)  LR: 1.782e-05  Data: 0.327 (0.327)  Elapsed/ETA: 0.8s / 4070.5s
Train: 79 [  50/5000 (  1%)]  Loss: 4.46 (4.07)  Time: 0.480s,   41.63/s  (0.451s,   44.31/s)  LR: 1.782e-05  Data: 0.010 (0.017)  Elapsed/ETA: 23.0s / 2233.8s
Train: 79 [ 100/5000 (  2%)]  Loss: 3.79 (4.04)  Time: 0.439s,   45.56/s  (0.447s,   44.71/s)  LR: 1.782e-05  Data: 0.014 (0.014)  Elapsed/ETA: 45.2s / 2191.7s
Train: 79 [ 150/5000 (  3%)]  Loss: 4.82 (4.05)  Time: 0.435s,   45.97/s  (0.456s,   43.85/s)  LR: 1.782e-05  Data: 0.011 (0.013)  Elapsed/ETA: 68.9s / 2211.7s
Train: 79 [ 200/5000 (  4%)]  Loss: 3.45 (4.02)  Time: 0.451s,   44.36/s  (0.452s,   44.28/s)  LR: 1.782e-05  Data: 0.016 (0.013)  Elapsed/ETA: 90.8s / 2167.7s
Train: 79 [ 250/5000 (  5%)]  Loss: 3.88 (4.03)  Time: 0.435s,   46.03/s  (0.454s,   44.04/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 114.0s / 2156.5s
Train: 79 [ 300/5000 (  6%)]  Loss: 4.90 (4.02)  Time: 0.443s,   45.15/s  (0.452s,   44.27/s)  LR: 1.782e-05  Data: 0.012 (0.012)  Elapsed/ETA: 136.0s / 2122.9s
Train: 79 [ 350/5000 (  7%)]  Loss: 4.55 (4.04)  Time: 0.436s,   45.87/s  (0.455s,   43.96/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 159.7s / 2115.0s
Train: 79 [ 400/5000 (  8%)]  Loss: 4.39 (4.05)  Time: 0.484s,   41.35/s  (0.454s,   44.08/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 181.9s / 2086.7s
Train: 79 [ 450/5000 (  9%)]  Loss: 4.69 (4.05)  Time: 0.437s,   45.78/s  (0.454s,   44.07/s)  LR: 1.782e-05  Data: 0.012 (0.012)  Elapsed/ETA: 204.7s / 2064.3s
Train: 79 [ 500/5000 ( 10%)]  Loss: 4.46 (4.06)  Time: 0.435s,   45.95/s  (0.452s,   44.22/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 226.6s / 2034.7s
Train: 79 [ 550/5000 ( 11%)]  Loss: 4.28 (4.05)  Time: 0.440s,   45.42/s  (0.451s,   44.32/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 248.6s / 2007.6s
Train: 79 [ 600/5000 ( 12%)]  Loss: 3.97 (4.06)  Time: 0.438s,   45.71/s  (0.450s,   44.44/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 270.5s / 1979.9s
Train: 79 [ 650/5000 ( 13%)]  Loss: 3.17 (4.06)  Time: 0.435s,   46.00/s  (0.449s,   44.51/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 292.5s / 1954.1s
Train: 79 [ 700/5000 ( 14%)]  Loss: 3.42 (4.06)  Time: 0.450s,   44.48/s  (0.452s,   44.27/s)  LR: 1.782e-05  Data: 0.017 (0.012)  Elapsed/ETA: 316.7s / 1942.0s
Train: 79 [ 750/5000 ( 15%)]  Loss: 3.98 (4.06)  Time: 0.526s,   38.04/s  (0.452s,   44.25/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 339.4s / 1920.4s
Train: 79 [ 800/5000 ( 16%)]  Loss: 4.56 (4.06)  Time: 0.443s,   45.12/s  (0.452s,   44.26/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 361.9s / 1897.3s
Train: 79 [ 850/5000 ( 17%)]  Loss: 4.14 (4.06)  Time: 0.442s,   45.23/s  (0.456s,   43.90/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 387.7s / 1890.2s
Train: 79 [ 900/5000 ( 18%)]  Loss: 4.19 (4.05)  Time: 0.587s,   34.08/s  (0.460s,   43.48/s)  LR: 1.782e-05  Data: 0.013 (0.012)  Elapsed/ETA: 414.4s / 1885.5s
Train: 79 [ 950/5000 ( 19%)]  Loss: 4.49 (4.06)  Time: 0.440s,   45.49/s  (0.462s,   43.25/s)  LR: 1.782e-05  Data: 0.012 (0.012)  Elapsed/ETA: 439.8s / 1872.5s
Train: 79 [1000/5000 ( 20%)]  Loss: 3.57 (4.06)  Time: 0.440s,   45.49/s  (0.462s,   43.34/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 462.0s / 1845.6s
Train: 79 [1050/5000 ( 21%)]  Loss: 4.38 (4.06)  Time: 0.439s,   45.56/s  (0.463s,   43.22/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 486.4s / 1827.5s
Train: 79 [1100/5000 ( 22%)]  Loss: 3.37 (4.06)  Time: 0.438s,   45.69/s  (0.465s,   43.02/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 511.9s / 1812.8s
Train: 79 [1150/5000 ( 23%)]  Loss: 4.54 (4.06)  Time: 0.583s,   34.29/s  (0.467s,   42.84/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 537.3s / 1796.9s
Train: 79 [1200/5000 ( 24%)]  Loss: 4.39 (4.06)  Time: 0.445s,   44.99/s  (0.467s,   42.81/s)  LR: 1.782e-05  Data: 0.012 (0.012)  Elapsed/ETA: 561.1s / 1775.0s
Train: 79 [1250/5000 ( 25%)]  Loss: 4.78 (4.06)  Time: 0.441s,   45.38/s  (0.466s,   42.88/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 583.5s / 1748.7s
Train: 79 [1300/5000 ( 26%)]  Loss: 3.98 (4.07)  Time: 0.590s,   33.92/s  (0.467s,   42.85/s)  LR: 1.782e-05  Data: 0.013 (0.012)  Elapsed/ETA: 607.2s / 1726.3s
Train: 79 [1350/5000 ( 27%)]  Loss: 4.28 (4.07)  Time: 0.438s,   45.62/s  (0.466s,   42.90/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 629.9s / 1701.3s
Train: 79 [1400/5000 ( 28%)]  Loss: 4.23 (4.07)  Time: 0.441s,   45.37/s  (0.468s,   42.76/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 655.2s / 1683.2s
Train: 79 [1450/5000 ( 29%)]  Loss: 4.75 (4.07)  Time: 0.577s,   34.67/s  (0.469s,   42.65/s)  LR: 1.782e-05  Data: 0.012 (0.012)  Elapsed/ETA: 680.4s / 1664.3s
Train: 79 [1500/5000 ( 30%)]  Loss: 3.94 (4.07)  Time: 0.491s,   40.75/s  (0.472s,   42.39/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 708.3s / 1651.0s
Train: 79 [1550/5000 ( 31%)]  Loss: 4.66 (4.07)  Time: 0.435s,   45.98/s  (0.472s,   42.41/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 731.5s / 1626.6s
Train: 79 [1600/5000 ( 32%)]  Loss: 4.59 (4.07)  Time: 0.451s,   44.31/s  (0.471s,   42.48/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 753.7s / 1600.2s
Train: 79 [1650/5000 ( 33%)]  Loss: 3.64 (4.07)  Time: 0.437s,   45.77/s  (0.470s,   42.55/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 776.1s / 1574.2s
Train: 79 [1700/5000 ( 34%)]  Loss: 3.96 (4.07)  Time: 0.445s,   44.93/s  (0.470s,   42.58/s)  LR: 1.782e-05  Data: 0.014 (0.012)  Elapsed/ETA: 799.0s / 1549.6s
Train: 79 [1750/5000 ( 35%)]  Loss: 3.91 (4.07)  Time: 0.588s,   34.00/s  (0.471s,   42.46/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 824.8s / 1530.5s
Train: 79 [1800/5000 ( 36%)]  Loss: 3.61 (4.07)  Time: 0.447s,   44.76/s  (0.471s,   42.42/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 849.1s / 1508.1s
Train: 79 [1850/5000 ( 37%)]  Loss: 4.58 (4.07)  Time: 0.459s,   43.61/s  (0.472s,   42.37/s)  LR: 1.782e-05  Data: 0.012 (0.012)  Elapsed/ETA: 873.8s / 1486.6s
Train: 79 [1900/5000 ( 38%)]  Loss: 3.56 (4.07)  Time: 0.434s,   46.06/s  (0.471s,   42.44/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 895.9s / 1460.5s
Train: 79 [1950/5000 ( 39%)]  Loss: 3.06 (4.07)  Time: 0.439s,   45.51/s  (0.470s,   42.51/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 917.9s / 1434.5s
Train: 79 [2000/5000 ( 40%)]  Loss: 3.19 (4.07)  Time: 0.432s,   46.29/s  (0.470s,   42.58/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 939.8s / 1408.5s
Train: 79 [2050/5000 ( 41%)]  Loss: 4.15 (4.07)  Time: 0.488s,   41.01/s  (0.469s,   42.60/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 962.9s / 1384.5s
Train: 79 [2100/5000 ( 42%)]  Loss: 4.24 (4.07)  Time: 0.455s,   43.91/s  (0.469s,   42.62/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 985.8s / 1360.3s
Train: 79 [2150/5000 ( 43%)]  Loss: 3.73 (4.07)  Time: 0.592s,   33.78/s  (0.469s,   42.63/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1009.2s / 1336.7s
Train: 79 [2200/5000 ( 44%)]  Loss: 4.57 (4.07)  Time: 0.440s,   45.48/s  (0.469s,   42.63/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1032.5s / 1313.0s
Train: 79 [2250/5000 ( 45%)]  Loss: 4.36 (4.07)  Time: 0.449s,   44.53/s  (0.468s,   42.69/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1054.5s / 1287.8s
Train: 79 [2300/5000 ( 46%)]  Loss: 3.24 (4.07)  Time: 0.436s,   45.91/s  (0.468s,   42.74/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 1076.6s / 1262.8s
Train: 79 [2350/5000 ( 47%)]  Loss: 4.07 (4.07)  Time: 0.442s,   45.27/s  (0.467s,   42.80/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1098.6s / 1237.9s
Train: 79 [2400/5000 ( 48%)]  Loss: 4.96 (4.07)  Time: 0.441s,   45.40/s  (0.468s,   42.69/s)  LR: 1.782e-05  Data: 0.012 (0.012)  Elapsed/ETA: 1124.8s / 1217.6s
Train: 79 [2450/5000 ( 49%)]  Loss: 4.09 (4.07)  Time: 0.582s,   34.36/s  (0.469s,   42.66/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1149.0s / 1194.9s
Train: 79 [2500/5000 ( 50%)]  Loss: 3.32 (4.08)  Time: 0.436s,   45.87/s  (0.468s,   42.70/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1171.5s / 1170.6s
Train: 79 [2550/5000 ( 51%)]  Loss: 4.34 (4.08)  Time: 0.435s,   46.02/s  (0.468s,   42.75/s)  LR: 1.782e-05  Data: 0.009 (0.012)  Elapsed/ETA: 1193.4s / 1145.7s
Train: 79 [2600/5000 ( 52%)]  Loss: 3.59 (4.08)  Time: 0.571s,   35.00/s  (0.468s,   42.73/s)  LR: 1.782e-05  Data: 0.010 (0.012)  Elapsed/ETA: 1217.5s / 1122.9s
Train: 79 [2650/5000 ( 53%)]  Loss: 4.79 (4.07)  Time: 0.436s,   45.92/s  (0.468s,   42.76/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1239.9s / 1098.6s
Train: 79 [2700/5000 ( 54%)]  Loss: 4.82 (4.07)  Time: 0.580s,   34.51/s  (0.467s,   42.80/s)  LR: 1.782e-05  Data: 0.016 (0.012)  Elapsed/ETA: 1262.1s / 1074.2s
Train: 79 [2750/5000 ( 55%)]  Loss: 3.14 (4.07)  Time: 0.441s,   45.37/s  (0.467s,   42.83/s)  LR: 1.782e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1284.5s / 1050.1s
Train: 79 [2800/5000 ( 56%)]  Loss: 4.35 (4.08)  Time: 0.440s,   45.49/s  (0.466s,   42.88/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1306.6s / 1025.7s
Train: 79 [2850/5000 ( 57%)]  Loss: 3.33 (4.08)  Time: 0.479s,   41.78/s  (0.466s,   42.92/s)  LR: 1.782e-05  Data: 0.012 (0.011)  Elapsed/ETA: 1328.5s / 1001.4s
Train: 79 [2900/5000 ( 58%)]  Loss: 3.85 (4.08)  Time: 0.439s,   45.61/s  (0.466s,   42.90/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1352.3s / 978.5s
Train: 79 [2950/5000 ( 59%)]  Loss: 4.39 (4.08)  Time: 0.436s,   45.88/s  (0.466s,   42.93/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1374.9s / 954.6s
Train: 79 [3000/5000 ( 60%)]  Loss: 4.30 (4.08)  Time: 0.440s,   45.50/s  (0.465s,   42.97/s)  LR: 1.782e-05  Data: 0.013 (0.011)  Elapsed/ETA: 1396.8s / 930.4s
Train: 79 [3050/5000 ( 61%)]  Loss: 4.85 (4.08)  Time: 0.457s,   43.73/s  (0.465s,   43.01/s)  LR: 1.782e-05  Data: 0.013 (0.011)  Elapsed/ETA: 1418.8s / 906.3s
Train: 79 [3100/5000 ( 62%)]  Loss: 4.30 (4.08)  Time: 0.436s,   45.82/s  (0.466s,   42.94/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1444.2s / 884.4s
Train: 79 [3150/5000 ( 63%)]  Loss: 4.42 (4.08)  Time: 0.439s,   45.54/s  (0.465s,   42.98/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1466.1s / 860.3s
Train: 79 [3200/5000 ( 64%)]  Loss: 4.82 (4.08)  Time: 0.441s,   45.35/s  (0.465s,   43.02/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1488.2s / 836.4s
Train: 79 [3250/5000 ( 65%)]  Loss: 3.90 (4.08)  Time: 0.435s,   45.93/s  (0.465s,   43.00/s)  LR: 1.782e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1512.0s / 813.4s
Train: 79 [3300/5000 ( 66%)]  Loss: 2.87 (4.08)  Time: 0.439s,   45.57/s  (0.465s,   43.04/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1533.8s / 789.4s
Train: 79 [3350/5000 ( 67%)]  Loss: 4.04 (4.08)  Time: 0.437s,   45.74/s  (0.464s,   43.08/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1555.7s / 765.5s
Train: 79 [3400/5000 ( 68%)]  Loss: 3.55 (4.09)  Time: 0.478s,   41.88/s  (0.464s,   43.12/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1577.6s / 741.7s
Train: 79 [3450/5000 ( 69%)]  Loss: 3.23 (4.08)  Time: 0.436s,   45.86/s  (0.464s,   43.14/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1599.8s / 718.1s
Train: 79 [3500/5000 ( 70%)]  Loss: 3.00 (4.08)  Time: 0.450s,   44.47/s  (0.463s,   43.17/s)  LR: 1.782e-05  Data: 0.014 (0.011)  Elapsed/ETA: 1622.1s / 694.5s
Train: 79 [3550/5000 ( 71%)]  Loss: 3.42 (4.09)  Time: 0.438s,   45.64/s  (0.463s,   43.17/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1645.0s / 671.3s
Train: 79 [3600/5000 ( 72%)]  Loss: 2.98 (4.09)  Time: 0.441s,   45.33/s  (0.463s,   43.20/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1667.1s / 647.7s
Train: 79 [3650/5000 ( 73%)]  Loss: 3.47 (4.09)  Time: 0.440s,   45.47/s  (0.463s,   43.24/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1688.9s / 624.0s
Train: 79 [3700/5000 ( 74%)]  Loss: 4.73 (4.09)  Time: 0.435s,   46.03/s  (0.462s,   43.26/s)  LR: 1.782e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1711.2s / 600.6s
Train: 79 [3750/5000 ( 75%)]  Loss: 4.55 (4.09)  Time: 0.583s,   34.28/s  (0.463s,   43.20/s)  LR: 1.782e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1736.6s / 578.3s
Train: 79 [3800/5000 ( 76%)]  Loss: 4.68 (4.08)  Time: 0.434s,   46.13/s  (0.463s,   43.15/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1761.8s / 555.7s
Train: 79 [3850/5000 ( 77%)]  Loss: 4.56 (4.09)  Time: 0.436s,   45.90/s  (0.463s,   43.18/s)  LR: 1.782e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1783.7s / 532.2s
Train: 79 [3900/5000 ( 78%)]  Loss: 3.51 (4.09)  Time: 0.438s,   45.68/s  (0.463s,   43.21/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1805.6s / 508.7s
Train: 79 [3950/5000 ( 79%)]  Loss: 3.84 (4.09)  Time: 0.435s,   45.95/s  (0.463s,   43.23/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1827.8s / 485.3s
Train: 79 [4000/5000 ( 80%)]  Loss: 3.54 (4.09)  Time: 0.436s,   45.92/s  (0.462s,   43.26/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1849.6s / 461.8s
Train: 79 [4050/5000 ( 81%)]  Loss: 4.76 (4.09)  Time: 0.571s,   35.04/s  (0.463s,   43.21/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1875.1s / 439.3s
Train: 79 [4100/5000 ( 82%)]  Loss: 4.69 (4.09)  Time: 0.435s,   45.93/s  (0.463s,   43.19/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1899.1s / 416.3s
Train: 79 [4150/5000 ( 83%)]  Loss: 4.13 (4.09)  Time: 0.439s,   45.57/s  (0.463s,   43.22/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1921.0s / 392.9s
Train: 79 [4200/5000 ( 84%)]  Loss: 3.56 (4.09)  Time: 0.434s,   46.03/s  (0.462s,   43.25/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1942.9s / 369.5s
Train: 79 [4250/5000 ( 85%)]  Loss: 3.72 (4.09)  Time: 0.476s,   42.05/s  (0.462s,   43.26/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1965.1s / 346.2s
Train: 79 [4300/5000 ( 86%)]  Loss: 4.40 (4.09)  Time: 0.437s,   45.77/s  (0.462s,   43.29/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1987.1s / 322.9s
Train: 79 [4350/5000 ( 87%)]  Loss: 4.55 (4.09)  Time: 0.449s,   44.54/s  (0.462s,   43.32/s)  LR: 1.782e-05  Data: 0.013 (0.011)  Elapsed/ETA: 2008.9s / 299.7s
Train: 79 [4400/5000 ( 88%)]  Loss: 4.39 (4.09)  Time: 0.434s,   46.06/s  (0.461s,   43.34/s)  LR: 1.782e-05  Data: 0.010 (0.011)  Elapsed/ETA: 2030.9s / 276.4s
Train: 79 [4450/5000 ( 89%)]  Loss: 3.33 (4.09)  Time: 0.449s,   44.58/s  (0.461s,   43.35/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2053.4s / 253.3s
Train: 79 [4500/5000 ( 90%)]  Loss: 3.84 (4.09)  Time: 0.441s,   45.31/s  (0.462s,   43.30/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2079.0s / 230.5s
Train: 79 [4550/5000 ( 91%)]  Loss: 4.77 (4.09)  Time: 0.438s,   45.71/s  (0.462s,   43.29/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2102.6s / 207.4s
Train: 79 [4600/5000 ( 92%)]  Loss: 4.49 (4.09)  Time: 0.478s,   41.87/s  (0.462s,   43.31/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2124.8s / 184.3s
Train: 79 [4650/5000 ( 93%)]  Loss: 4.65 (4.09)  Time: 0.437s,   45.73/s  (0.462s,   43.32/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2147.0s / 161.1s
Train: 79 [4700/5000 ( 94%)]  Loss: 4.30 (4.09)  Time: 0.438s,   45.66/s  (0.462s,   43.32/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2170.3s / 138.0s
Train: 79 [4750/5000 ( 95%)]  Loss: 3.34 (4.09)  Time: 0.436s,   45.91/s  (0.462s,   43.33/s)  LR: 1.782e-05  Data: 0.010 (0.011)  Elapsed/ETA: 2193.1s / 114.9s
Train: 79 [4800/5000 ( 96%)]  Loss: 4.13 (4.09)  Time: 0.440s,   45.49/s  (0.461s,   43.35/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2215.1s / 91.8s
Train: 79 [4850/5000 ( 97%)]  Loss: 3.78 (4.09)  Time: 0.437s,   45.80/s  (0.461s,   43.37/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2237.1s / 68.7s
Train: 79 [4900/5000 ( 98%)]  Loss: 4.77 (4.09)  Time: 0.448s,   44.64/s  (0.461s,   43.39/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2259.2s / 45.6s
Train: 79 [4950/5000 ( 99%)]  Loss: 4.74 (4.09)  Time: 0.581s,   34.44/s  (0.461s,   43.35/s)  LR: 1.782e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2284.4s / 22.6s
Test: [   0/499]  Time: 0.260 (0.260)  Loss:   1.056 ( 1.056)  Acc@1:  80.000 ( 80.000)  Acc@5:  95.000 ( 95.000)
Test: [  50/499]  Time: 0.086 (0.100)  Loss:   1.385 ( 2.275)  Acc@1:  80.000 ( 51.373)  Acc@5:  90.000 ( 75.686)
Test: [ 100/499]  Time: 0.090 (0.093)  Loss:   2.416 ( 2.257)  Acc@1:  45.000 ( 50.396)  Acc@5:  70.000 ( 75.644)
Test: [ 150/499]  Time: 0.095 (0.091)  Loss:   2.021 ( 2.213)  Acc@1:  50.000 ( 50.762)  Acc@5:  85.000 ( 76.093)
Test: [ 200/499]  Time: 0.085 (0.090)  Loss:   3.771 ( 2.308)  Acc@1:  10.000 ( 48.483)  Acc@5:  40.000 ( 74.229)
Test: [ 250/499]  Time: 0.085 (0.089)  Loss:   3.597 ( 2.328)  Acc@1:  15.000 ( 47.908)  Acc@5:  35.000 ( 73.984)
Test: [ 300/499]  Time: 0.086 (0.088)  Loss:   3.006 ( 2.310)  Acc@1:  35.000 ( 48.704)  Acc@5:  55.000 ( 73.987)
Test: [ 350/499]  Time: 0.089 (0.088)  Loss:   2.599 ( 2.372)  Acc@1:  40.000 ( 47.251)  Acc@5:  70.000 ( 72.536)
Test: [ 400/499]  Time: 0.086 (0.088)  Loss:   3.570 ( 2.361)  Acc@1:  30.000 ( 47.706)  Acc@5:  50.000 ( 72.818)
Test: [ 450/499]  Time: 0.086 (0.088)  Loss:   3.089 ( 2.365)  Acc@1:  40.000 ( 47.716)  Acc@5:  65.000 ( 72.772)
Test: [ 499/499]  Time: 0.076 (0.088)  Loss:   2.604 ( 2.339)  Acc@1:  35.000 ( 48.270)  Acc@5:  85.000 ( 73.630)
Current checkpoints:
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-79.pth.tar', 48.27)
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-78.pth.tar', 47.0)
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-77.pth.tar', 46.76)
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-76.pth.tar', 46.65)

Train: 80 [   0/5000 (  0%)]  Loss: 4.60 (4.60)  Time: 0.787s,   25.41/s  (0.787s,   25.41/s)  LR: 1.792e-05  Data: 0.302 (0.302)  Elapsed/ETA: 0.8s / 3935.4s
Train: 80 [  50/5000 (  1%)]  Loss: 3.79 (4.05)  Time: 0.446s,   44.81/s  (0.469s,   42.67/s)  LR: 1.792e-05  Data: 0.011 (0.017)  Elapsed/ETA: 23.9s / 2319.8s
Train: 80 [ 100/5000 (  2%)]  Loss: 4.84 (4.11)  Time: 0.579s,   34.57/s  (0.467s,   42.78/s)  LR: 1.792e-05  Data: 0.011 (0.015)  Elapsed/ETA: 47.2s / 2290.1s
Train: 80 [ 150/5000 (  3%)]  Loss: 3.87 (4.03)  Time: 0.445s,   44.96/s  (0.469s,   42.66/s)  LR: 1.792e-05  Data: 0.013 (0.014)  Elapsed/ETA: 70.8s / 2273.1s
Train: 80 [ 200/5000 (  4%)]  Loss: 3.88 (4.06)  Time: 0.445s,   44.98/s  (0.463s,   43.22/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 93.0s / 2220.5s
Train: 80 [ 250/5000 (  5%)]  Loss: 3.92 (4.05)  Time: 0.447s,   44.79/s  (0.459s,   43.57/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 115.2s / 2179.7s
Train: 80 [ 300/5000 (  6%)]  Loss: 3.76 (4.05)  Time: 0.469s,   42.63/s  (0.470s,   42.53/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 141.5s / 2209.5s
Train: 80 [ 350/5000 (  7%)]  Loss: 3.07 (4.05)  Time: 0.449s,   44.55/s  (0.467s,   42.86/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 163.8s / 2169.1s
Train: 80 [ 400/5000 (  8%)]  Loss: 4.66 (4.07)  Time: 0.448s,   44.61/s  (0.465s,   43.02/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 186.4s / 2137.9s
Train: 80 [ 450/5000 (  9%)]  Loss: 3.68 (4.07)  Time: 0.481s,   41.57/s  (0.468s,   42.72/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 211.2s / 2129.8s
Train: 80 [ 500/5000 ( 10%)]  Loss: 4.14 (4.08)  Time: 0.451s,   44.37/s  (0.466s,   42.95/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 233.3s / 2094.9s
Train: 80 [ 550/5000 ( 11%)]  Loss: 3.46 (4.07)  Time: 0.444s,   45.06/s  (0.464s,   43.12/s)  LR: 1.792e-05  Data: 0.014 (0.012)  Elapsed/ETA: 255.6s / 2063.4s
Train: 80 [ 600/5000 ( 12%)]  Loss: 4.38 (4.06)  Time: 0.443s,   45.11/s  (0.462s,   43.26/s)  LR: 1.792e-05  Data: 0.014 (0.012)  Elapsed/ETA: 277.9s / 2033.8s
Train: 80 [ 650/5000 ( 13%)]  Loss: 4.81 (4.06)  Time: 0.576s,   34.72/s  (0.466s,   42.92/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 303.4s / 2026.6s
Train: 80 [ 700/5000 ( 14%)]  Loss: 3.49 (4.07)  Time: 0.444s,   45.00/s  (0.466s,   42.88/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 327.0s / 2005.3s
Train: 80 [ 750/5000 ( 15%)]  Loss: 3.22 (4.06)  Time: 0.445s,   44.98/s  (0.467s,   42.82/s)  LR: 1.792e-05  Data: 0.012 (0.012)  Elapsed/ETA: 350.8s / 1984.6s
Train: 80 [ 800/5000 ( 16%)]  Loss: 4.20 (4.07)  Time: 0.444s,   45.05/s  (0.466s,   42.96/s)  LR: 1.792e-05  Data: 0.012 (0.012)  Elapsed/ETA: 372.9s / 1955.0s
Train: 80 [ 850/5000 ( 17%)]  Loss: 4.78 (4.06)  Time: 0.445s,   44.93/s  (0.464s,   43.08/s)  LR: 1.792e-05  Data: 0.012 (0.012)  Elapsed/ETA: 395.1s / 1926.3s
Train: 80 [ 900/5000 ( 18%)]  Loss: 3.49 (4.07)  Time: 0.447s,   44.76/s  (0.465s,   43.01/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 419.0s / 1906.1s
Train: 80 [ 950/5000 ( 19%)]  Loss: 4.14 (4.07)  Time: 0.449s,   44.51/s  (0.464s,   43.11/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 441.2s / 1878.5s
Train: 80 [1000/5000 ( 20%)]  Loss: 4.88 (4.07)  Time: 0.508s,   39.38/s  (0.464s,   43.10/s)  LR: 1.792e-05  Data: 0.023 (0.012)  Elapsed/ETA: 464.5s / 1855.6s
Train: 80 [1050/5000 ( 21%)]  Loss: 4.74 (4.08)  Time: 0.458s,   43.71/s  (0.464s,   43.10/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 487.7s / 1832.4s
Train: 80 [1100/5000 ( 22%)]  Loss: 4.11 (4.08)  Time: 0.452s,   44.26/s  (0.464s,   43.10/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 510.9s / 1809.4s
Train: 80 [1150/5000 ( 23%)]  Loss: 4.63 (4.08)  Time: 0.444s,   45.06/s  (0.463s,   43.17/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 533.3s / 1783.2s
Train: 80 [1200/5000 ( 24%)]  Loss: 4.56 (4.08)  Time: 0.443s,   45.11/s  (0.464s,   43.08/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 557.5s / 1763.6s
Train: 80 [1250/5000 ( 25%)]  Loss: 3.36 (4.08)  Time: 0.446s,   44.80/s  (0.465s,   43.02/s)  LR: 1.792e-05  Data: 0.012 (0.012)  Elapsed/ETA: 581.6s / 1742.8s
Train: 80 [1300/5000 ( 26%)]  Loss: 4.40 (4.07)  Time: 0.447s,   44.79/s  (0.464s,   43.08/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 604.0s / 1717.3s
Train: 80 [1350/5000 ( 27%)]  Loss: 4.33 (4.08)  Time: 0.450s,   44.48/s  (0.464s,   43.14/s)  LR: 1.792e-05  Data: 0.012 (0.012)  Elapsed/ETA: 626.3s / 1691.6s
Train: 80 [1400/5000 ( 28%)]  Loss: 3.97 (4.08)  Time: 0.447s,   44.78/s  (0.465s,   42.99/s)  LR: 1.792e-05  Data: 0.012 (0.014)  Elapsed/ETA: 651.8s / 1674.3s
Train: 80 [1450/5000 ( 29%)]  Loss: 3.41 (4.07)  Time: 0.459s,   43.60/s  (0.465s,   43.01/s)  LR: 1.792e-05  Data: 0.009 (0.014)  Elapsed/ETA: 674.8s / 1650.5s
Train: 80 [1500/5000 ( 30%)]  Loss: 4.21 (4.07)  Time: 0.443s,   45.19/s  (0.466s,   42.94/s)  LR: 1.792e-05  Data: 0.012 (0.014)  Elapsed/ETA: 699.1s / 1629.8s
Train: 80 [1550/5000 ( 31%)]  Loss: 4.11 (4.07)  Time: 0.442s,   45.24/s  (0.465s,   42.98/s)  LR: 1.792e-05  Data: 0.011 (0.014)  Elapsed/ETA: 721.7s / 1604.8s
Train: 80 [1600/5000 ( 32%)]  Loss: 4.30 (4.08)  Time: 0.446s,   44.86/s  (0.465s,   43.01/s)  LR: 1.792e-05  Data: 0.011 (0.014)  Elapsed/ETA: 744.4s / 1580.4s
Train: 80 [1650/5000 ( 33%)]  Loss: 4.58 (4.08)  Time: 0.459s,   43.60/s  (0.464s,   43.07/s)  LR: 1.792e-05  Data: 0.016 (0.014)  Elapsed/ETA: 766.7s / 1555.3s
Train: 80 [1700/5000 ( 34%)]  Loss: 4.78 (4.08)  Time: 0.445s,   44.98/s  (0.464s,   43.09/s)  LR: 1.792e-05  Data: 0.013 (0.014)  Elapsed/ETA: 789.4s / 1531.1s
Train: 80 [1750/5000 ( 35%)]  Loss: 4.19 (4.08)  Time: 0.585s,   34.18/s  (0.464s,   43.08/s)  LR: 1.792e-05  Data: 0.010 (0.014)  Elapsed/ETA: 813.0s / 1508.5s
Train: 80 [1800/5000 ( 36%)]  Loss: 4.03 (4.08)  Time: 0.444s,   45.05/s  (0.467s,   42.86/s)  LR: 1.792e-05  Data: 0.011 (0.014)  Elapsed/ETA: 840.4s / 1492.7s
Train: 80 [1850/5000 ( 37%)]  Loss: 3.49 (4.08)  Time: 0.446s,   44.87/s  (0.466s,   42.91/s)  LR: 1.792e-05  Data: 0.010 (0.014)  Elapsed/ETA: 862.6s / 1467.6s
Train: 80 [1900/5000 ( 38%)]  Loss: 3.53 (4.08)  Time: 0.577s,   34.63/s  (0.467s,   42.81/s)  LR: 1.792e-05  Data: 0.011 (0.014)  Elapsed/ETA: 888.2s / 1448.0s
Train: 80 [1950/5000 ( 39%)]  Loss: 4.61 (4.08)  Time: 0.485s,   41.22/s  (0.467s,   42.83/s)  LR: 1.792e-05  Data: 0.012 (0.014)  Elapsed/ETA: 911.0s / 1423.7s
Train: 80 [2000/5000 ( 40%)]  Loss: 3.46 (4.08)  Time: 0.440s,   45.42/s  (0.467s,   42.85/s)  LR: 1.792e-05  Data: 0.011 (0.014)  Elapsed/ETA: 934.0s / 1399.9s
Train: 80 [2050/5000 ( 41%)]  Loss: 3.94 (4.08)  Time: 0.443s,   45.18/s  (0.466s,   42.90/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 956.2s / 1374.9s
Train: 80 [2100/5000 ( 42%)]  Loss: 4.60 (4.08)  Time: 0.442s,   45.29/s  (0.466s,   42.91/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 979.2s / 1351.1s
Train: 80 [2150/5000 ( 43%)]  Loss: 3.53 (4.08)  Time: 0.442s,   45.29/s  (0.465s,   42.97/s)  LR: 1.792e-05  Data: 0.014 (0.013)  Elapsed/ETA: 1001.3s / 1326.2s
Train: 80 [2200/5000 ( 44%)]  Loss: 3.21 (4.08)  Time: 0.484s,   41.35/s  (0.465s,   43.01/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1023.5s / 1301.6s
Train: 80 [2250/5000 ( 45%)]  Loss: 4.14 (4.08)  Time: 0.442s,   45.27/s  (0.465s,   43.05/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 1045.7s / 1277.0s
Train: 80 [2300/5000 ( 46%)]  Loss: 4.94 (4.08)  Time: 0.444s,   45.10/s  (0.464s,   43.09/s)  LR: 1.792e-05  Data: 0.013 (0.013)  Elapsed/ETA: 1068.1s / 1252.8s
Train: 80 [2350/5000 ( 47%)]  Loss: 3.89 (4.08)  Time: 0.444s,   45.05/s  (0.464s,   43.08/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 1091.3s / 1229.7s
Train: 80 [2400/5000 ( 48%)]  Loss: 4.34 (4.08)  Time: 0.444s,   45.03/s  (0.464s,   43.12/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1113.7s / 1205.5s
Train: 80 [2450/5000 ( 49%)]  Loss: 3.08 (4.08)  Time: 0.449s,   44.55/s  (0.466s,   42.96/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 1141.2s / 1186.8s
Train: 80 [2500/5000 ( 50%)]  Loss: 4.62 (4.08)  Time: 0.486s,   41.17/s  (0.466s,   42.95/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1164.7s / 1163.7s
Train: 80 [2550/5000 ( 51%)]  Loss: 2.58 (4.08)  Time: 0.458s,   43.63/s  (0.465s,   42.97/s)  LR: 1.792e-05  Data: 0.009 (0.013)  Elapsed/ETA: 1187.3s / 1139.8s
Train: 80 [2600/5000 ( 52%)]  Loss: 3.98 (4.08)  Time: 0.452s,   44.29/s  (0.465s,   42.98/s)  LR: 1.792e-05  Data: 0.010 (0.013)  Elapsed/ETA: 1210.4s / 1116.4s
Train: 80 [2650/5000 ( 53%)]  Loss: 3.09 (4.08)  Time: 0.449s,   44.58/s  (0.465s,   43.00/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 1233.1s / 1092.6s
Train: 80 [2700/5000 ( 54%)]  Loss: 4.74 (4.08)  Time: 0.583s,   34.33/s  (0.465s,   42.98/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 1256.9s / 1069.9s
Train: 80 [2750/5000 ( 55%)]  Loss: 4.86 (4.08)  Time: 0.447s,   44.72/s  (0.465s,   42.98/s)  LR: 1.792e-05  Data: 0.010 (0.013)  Elapsed/ETA: 1280.1s / 1046.5s
Train: 80 [2800/5000 ( 56%)]  Loss: 3.23 (4.08)  Time: 0.447s,   44.75/s  (0.465s,   43.00/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1302.9s / 1022.8s
Train: 80 [2850/5000 ( 57%)]  Loss: 4.11 (4.08)  Time: 0.445s,   44.93/s  (0.465s,   43.02/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1325.4s / 999.1s
Train: 80 [2900/5000 ( 58%)]  Loss: 3.44 (4.08)  Time: 0.446s,   44.84/s  (0.465s,   42.97/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1350.4s / 977.1s
Train: 80 [2950/5000 ( 59%)]  Loss: 4.58 (4.08)  Time: 0.485s,   41.21/s  (0.465s,   42.99/s)  LR: 1.792e-05  Data: 0.009 (0.013)  Elapsed/ETA: 1373.0s / 953.3s
Train: 80 [3000/5000 ( 60%)]  Loss: 4.23 (4.08)  Time: 0.449s,   44.58/s  (0.465s,   43.01/s)  LR: 1.792e-05  Data: 0.014 (0.013)  Elapsed/ETA: 1395.4s / 929.5s
Train: 80 [3050/5000 ( 61%)]  Loss: 3.55 (4.09)  Time: 0.449s,   44.51/s  (0.465s,   43.00/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1419.0s / 906.5s
Train: 80 [3100/5000 ( 62%)]  Loss: 2.72 (4.09)  Time: 0.446s,   44.89/s  (0.465s,   43.01/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1441.9s / 883.0s
Train: 80 [3150/5000 ( 63%)]  Loss: 3.21 (4.09)  Time: 0.506s,   39.53/s  (0.465s,   43.03/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1464.5s / 859.4s
Train: 80 [3200/5000 ( 64%)]  Loss: 3.43 (4.09)  Time: 0.552s,   36.20/s  (0.464s,   43.06/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1486.9s / 835.6s
Train: 80 [3250/5000 ( 65%)]  Loss: 4.45 (4.09)  Time: 0.579s,   34.55/s  (0.465s,   43.05/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1510.5s / 812.6s
Train: 80 [3300/5000 ( 66%)]  Loss: 4.19 (4.09)  Time: 0.447s,   44.74/s  (0.465s,   43.02/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1534.5s / 789.8s
Train: 80 [3350/5000 ( 67%)]  Loss: 3.17 (4.09)  Time: 0.574s,   34.81/s  (0.466s,   42.90/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1562.1s / 768.7s
Train: 80 [3400/5000 ( 68%)]  Loss: 5.04 (4.09)  Time: 0.441s,   45.35/s  (0.466s,   42.88/s)  LR: 1.792e-05  Data: 0.009 (0.013)  Elapsed/ETA: 1586.2s / 745.8s
Train: 80 [3450/5000 ( 69%)]  Loss: 3.85 (4.09)  Time: 0.442s,   45.28/s  (0.466s,   42.88/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1609.5s / 722.4s
Train: 80 [3500/5000 ( 70%)]  Loss: 4.76 (4.09)  Time: 0.447s,   44.70/s  (0.466s,   42.91/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 1631.7s / 698.6s
Train: 80 [3550/5000 ( 71%)]  Loss: 3.71 (4.09)  Time: 0.465s,   42.97/s  (0.466s,   42.92/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 1654.7s / 675.2s
Train: 80 [3600/5000 ( 72%)]  Loss: 4.52 (4.09)  Time: 0.482s,   41.50/s  (0.466s,   42.95/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 1677.0s / 651.5s
Train: 80 [3650/5000 ( 73%)]  Loss: 3.85 (4.09)  Time: 0.452s,   44.22/s  (0.466s,   42.95/s)  LR: 1.792e-05  Data: 0.020 (0.013)  Elapsed/ETA: 1699.9s / 628.1s
Train: 80 [3700/5000 ( 74%)]  Loss: 3.33 (4.08)  Time: 0.440s,   45.49/s  (0.466s,   42.95/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1723.2s / 604.8s
Train: 80 [3750/5000 ( 75%)]  Loss: 3.19 (4.08)  Time: 0.486s,   41.17/s  (0.466s,   42.96/s)  LR: 1.792e-05  Data: 0.017 (0.013)  Elapsed/ETA: 1746.2s / 581.4s
Train: 80 [3800/5000 ( 76%)]  Loss: 3.99 (4.08)  Time: 0.442s,   45.24/s  (0.465s,   42.98/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1768.6s / 557.9s
Train: 80 [3850/5000 ( 77%)]  Loss: 3.71 (4.08)  Time: 0.444s,   45.08/s  (0.465s,   43.01/s)  LR: 1.792e-05  Data: 0.010 (0.013)  Elapsed/ETA: 1790.6s / 534.3s
Train: 80 [3900/5000 ( 78%)]  Loss: 3.94 (4.08)  Time: 0.441s,   45.38/s  (0.465s,   43.03/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1813.0s / 510.8s
Train: 80 [3950/5000 ( 79%)]  Loss: 4.51 (4.09)  Time: 0.444s,   45.01/s  (0.465s,   42.98/s)  LR: 1.792e-05  Data: 0.013 (0.013)  Elapsed/ETA: 1838.4s / 488.1s
Train: 80 [4000/5000 ( 80%)]  Loss: 4.02 (4.09)  Time: 0.446s,   44.87/s  (0.465s,   43.01/s)  LR: 1.792e-05  Data: 0.010 (0.013)  Elapsed/ETA: 1860.5s / 464.5s
Train: 80 [4050/5000 ( 81%)]  Loss: 3.89 (4.09)  Time: 0.441s,   45.31/s  (0.465s,   43.04/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 1882.6s / 441.0s
Train: 80 [4100/5000 ( 82%)]  Loss: 3.84 (4.09)  Time: 0.437s,   45.82/s  (0.464s,   43.06/s)  LR: 1.792e-05  Data: 0.011 (0.013)  Elapsed/ETA: 1904.7s / 417.5s
Train: 80 [4150/5000 ( 83%)]  Loss: 4.15 (4.09)  Time: 0.448s,   44.62/s  (0.464s,   43.09/s)  LR: 1.792e-05  Data: 0.012 (0.013)  Elapsed/ETA: 1926.7s / 394.1s
Train: 80 [4200/5000 ( 84%)]  Loss: 3.39 (4.09)  Time: 0.440s,   45.47/s  (0.464s,   43.09/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1950.0s / 370.9s
Train: 80 [4250/5000 ( 85%)]  Loss: 4.52 (4.09)  Time: 0.442s,   45.25/s  (0.464s,   43.11/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1972.1s / 347.5s
Train: 80 [4300/5000 ( 86%)]  Loss: 4.55 (4.09)  Time: 0.440s,   45.49/s  (0.464s,   43.13/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 1994.5s / 324.1s
Train: 80 [4350/5000 ( 87%)]  Loss: 4.57 (4.09)  Time: 0.440s,   45.47/s  (0.464s,   43.14/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2017.3s / 300.9s
Train: 80 [4400/5000 ( 88%)]  Loss: 3.47 (4.09)  Time: 0.439s,   45.51/s  (0.463s,   43.16/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2039.4s / 277.6s
Train: 80 [4450/5000 ( 89%)]  Loss: 4.25 (4.09)  Time: 0.440s,   45.41/s  (0.463s,   43.18/s)  LR: 1.792e-05  Data: 0.010 (0.012)  Elapsed/ETA: 2061.5s / 254.3s
Train: 80 [4500/5000 ( 90%)]  Loss: 3.64 (4.09)  Time: 0.442s,   45.28/s  (0.463s,   43.20/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2083.6s / 231.0s
Train: 80 [4550/5000 ( 91%)]  Loss: 4.12 (4.09)  Time: 0.440s,   45.46/s  (0.463s,   43.21/s)  LR: 1.792e-05  Data: 0.013 (0.012)  Elapsed/ETA: 2106.7s / 207.8s
Train: 80 [4600/5000 ( 92%)]  Loss: 3.95 (4.09)  Time: 0.439s,   45.51/s  (0.463s,   43.23/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2128.7s / 184.6s
Train: 80 [4650/5000 ( 93%)]  Loss: 4.89 (4.09)  Time: 0.439s,   45.53/s  (0.463s,   43.23/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2151.9s / 161.5s
Train: 80 [4700/5000 ( 94%)]  Loss: 3.84 (4.09)  Time: 0.438s,   45.68/s  (0.462s,   43.25/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2173.9s / 138.3s
Train: 80 [4750/5000 ( 95%)]  Loss: 4.28 (4.09)  Time: 0.438s,   45.61/s  (0.462s,   43.27/s)  LR: 1.792e-05  Data: 0.010 (0.012)  Elapsed/ETA: 2195.9s / 115.1s
Train: 80 [4800/5000 ( 96%)]  Loss: 3.22 (4.09)  Time: 0.441s,   45.35/s  (0.462s,   43.29/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2217.8s / 91.9s
Train: 80 [4850/5000 ( 97%)]  Loss: 4.28 (4.09)  Time: 0.576s,   34.71/s  (0.463s,   43.18/s)  LR: 1.792e-05  Data: 0.012 (0.012)  Elapsed/ETA: 2246.9s / 69.0s
Train: 80 [4900/5000 ( 98%)]  Loss: 3.61 (4.08)  Time: 0.439s,   45.54/s  (0.463s,   43.19/s)  LR: 1.792e-05  Data: 0.011 (0.012)  Elapsed/ETA: 2269.3s / 45.8s
Train: 80 [4950/5000 ( 99%)]  Loss: 4.71 (4.08)  Time: 0.437s,   45.80/s  (0.463s,   43.22/s)  LR: 1.792e-05  Data: 0.009 (0.012)  Elapsed/ETA: 2291.3s / 22.7s
Test: [   0/499]  Time: 0.258 (0.258)  Loss:   0.746 ( 0.746)  Acc@1:  85.000 ( 85.000)  Acc@5: 100.000 (100.000)
Test: [  50/499]  Time: 0.084 (0.088)  Loss:   1.276 ( 2.143)  Acc@1:  80.000 ( 52.451)  Acc@5:  90.000 ( 77.451)
Test: [ 100/499]  Time: 0.085 (0.090)  Loss:   2.354 ( 2.192)  Acc@1:  45.000 ( 49.851)  Acc@5:  70.000 ( 76.040)
Test: [ 150/499]  Time: 0.086 (0.089)  Loss:   1.814 ( 2.132)  Acc@1:  60.000 ( 51.689)  Acc@5:  85.000 ( 76.689)
Test: [ 200/499]  Time: 0.090 (0.089)  Loss:   3.704 ( 2.237)  Acc@1:  10.000 ( 49.254)  Acc@5:  40.000 ( 74.652)
Test: [ 250/499]  Time: 0.083 (0.088)  Loss:   3.373 ( 2.259)  Acc@1:  10.000 ( 48.606)  Acc@5:  40.000 ( 74.582)
Test: [ 300/499]  Time: 0.083 (0.088)  Loss:   3.069 ( 2.250)  Acc@1:  30.000 ( 49.236)  Acc@5:  65.000 ( 74.751)
Test: [ 350/499]  Time: 0.083 (0.087)  Loss:   2.292 ( 2.314)  Acc@1:  45.000 ( 47.664)  Acc@5:  70.000 ( 73.291)
Test: [ 400/499]  Time: 0.086 (0.092)  Loss:   3.572 ( 2.316)  Acc@1:  30.000 ( 48.017)  Acc@5:  50.000 ( 73.217)
Test: [ 450/499]  Time: 0.096 (0.092)  Loss:   3.307 ( 2.316)  Acc@1:  30.000 ( 47.949)  Acc@5:  60.000 ( 73.304)
Test: [ 499/499]  Time: 0.074 (0.094)  Loss:   2.703 ( 2.294)  Acc@1:  50.000 ( 48.460)  Acc@5:  80.000 ( 73.920)
Current checkpoints:
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-80.pth.tar', 48.46)
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-79.pth.tar', 48.27)
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-78.pth.tar', 47.0)
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-77.pth.tar', 46.76)
 ('./output/train/20250407-114808-modification_294m-224/checkpoint-76.pth.tar', 46.65)

Train: 81 [   0/5000 (  0%)]  Loss: 3.37 (3.37)  Time: 0.780s,   25.66/s  (0.780s,   25.66/s)  LR: 1.802e-05  Data: 0.294 (0.294)  Elapsed/ETA: 0.8s / 3898.2s
Train: 81 [  50/5000 (  1%)]  Loss: 4.89 (4.06)  Time: 0.439s,   45.55/s  (0.470s,   42.55/s)  LR: 1.802e-05  Data: 0.011 (0.016)  Elapsed/ETA: 24.0s / 2326.0s
Train: 81 [ 100/5000 (  2%)]  Loss: 4.40 (4.12)  Time: 0.439s,   45.53/s  (0.463s,   43.17/s)  LR: 1.802e-05  Data: 0.010 (0.013)  Elapsed/ETA: 46.8s / 2269.5s
Train: 81 [ 150/5000 (  3%)]  Loss: 3.10 (4.07)  Time: 0.439s,   45.55/s  (0.459s,   43.53/s)  LR: 1.802e-05  Data: 0.010 (0.013)  Elapsed/ETA: 69.4s / 2227.8s
Train: 81 [ 200/5000 (  4%)]  Loss: 3.43 (4.09)  Time: 0.441s,   45.39/s  (0.455s,   43.99/s)  LR: 1.802e-05  Data: 0.011 (0.012)  Elapsed/ETA: 91.4s / 2182.0s
Train: 81 [ 250/5000 (  5%)]  Loss: 3.37 (4.07)  Time: 0.439s,   45.60/s  (0.459s,   43.56/s)  LR: 1.802e-05  Data: 0.011 (0.012)  Elapsed/ETA: 115.2s / 2180.3s
Train: 81 [ 300/5000 (  6%)]  Loss: 3.78 (4.08)  Time: 0.442s,   45.29/s  (0.456s,   43.83/s)  LR: 1.802e-05  Data: 0.010 (0.012)  Elapsed/ETA: 137.3s / 2144.0s
Train: 81 [ 350/5000 (  7%)]  Loss: 4.46 (4.09)  Time: 0.441s,   45.34/s  (0.454s,   44.04/s)  LR: 1.802e-05  Data: 0.012 (0.012)  Elapsed/ETA: 159.4s / 2111.5s
Train: 81 [ 400/5000 (  8%)]  Loss: 3.66 (4.08)  Time: 0.440s,   45.43/s  (0.453s,   44.20/s)  LR: 1.802e-05  Data: 0.010 (0.012)  Elapsed/ETA: 181.5s / 2081.2s
Train: 81 [ 450/5000 (  9%)]  Loss: 3.28 (4.09)  Time: 0.439s,   45.57/s  (0.453s,   44.12/s)  LR: 1.802e-05  Data: 0.010 (0.012)  Elapsed/ETA: 204.4s / 2062.0s
Train: 81 [ 500/5000 ( 10%)]  Loss: 4.63 (4.10)  Time: 0.439s,   45.59/s  (0.452s,   44.25/s)  LR: 1.802e-05  Data: 0.010 (0.012)  Elapsed/ETA: 226.5s / 2033.5s
Train: 81 [ 550/5000 ( 11%)]  Loss: 4.18 (4.09)  Time: 0.444s,   45.07/s  (0.458s,   43.71/s)  LR: 1.802e-05  Data: 0.016 (0.012)  Elapsed/ETA: 252.1s / 2035.5s
Train: 81 [ 600/5000 ( 12%)]  Loss: 4.50 (4.10)  Time: 0.574s,   34.86/s  (0.461s,   43.37/s)  LR: 1.802e-05  Data: 0.011 (0.012)  Elapsed/ETA: 277.1s / 2028.5s
Train: 81 [ 650/5000 ( 13%)]  Loss: 3.75 (4.10)  Time: 0.577s,   34.66/s  (0.470s,   42.55/s)  LR: 1.802e-05  Data: 0.013 (0.012)  Elapsed/ETA: 306.0s / 2044.1s
Train: 81 [ 700/5000 ( 14%)]  Loss: 4.27 (4.09)  Time: 0.438s,   45.64/s  (0.475s,   42.13/s)  LR: 1.802e-05  Data: 0.011 (0.012)  Elapsed/ETA: 332.8s / 2040.7s
Train: 81 [ 750/5000 ( 15%)]  Loss: 3.38 (4.09)  Time: 0.591s,   33.87/s  (0.475s,   42.08/s)  LR: 1.802e-05  Data: 0.021 (0.012)  Elapsed/ETA: 356.9s / 2019.5s
Train: 81 [ 800/5000 ( 16%)]  Loss: 3.33 (4.09)  Time: 0.589s,   33.96/s  (0.482s,   41.51/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 385.9s / 2023.2s
Train: 81 [ 850/5000 ( 17%)]  Loss: 4.43 (4.09)  Time: 0.440s,   45.49/s  (0.481s,   41.58/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 409.3s / 1995.6s
Train: 81 [ 900/5000 ( 18%)]  Loss: 4.66 (4.09)  Time: 0.439s,   45.60/s  (0.479s,   41.73/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 431.9s / 1964.7s
Train: 81 [ 950/5000 ( 19%)]  Loss: 4.67 (4.09)  Time: 0.582s,   34.34/s  (0.481s,   41.60/s)  LR: 1.802e-05  Data: 0.012 (0.011)  Elapsed/ETA: 457.3s / 1946.8s
Train: 81 [1000/5000 ( 20%)]  Loss: 3.79 (4.09)  Time: 0.439s,   45.57/s  (0.480s,   41.67/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 480.4s / 1919.3s
Train: 81 [1050/5000 ( 21%)]  Loss: 3.72 (4.09)  Time: 0.443s,   45.12/s  (0.478s,   41.83/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 502.5s / 1888.0s
Train: 81 [1100/5000 ( 22%)]  Loss: 3.53 (4.09)  Time: 0.443s,   45.17/s  (0.477s,   41.97/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 524.7s / 1858.0s
Train: 81 [1150/5000 ( 23%)]  Loss: 4.16 (4.09)  Time: 0.448s,   44.62/s  (0.475s,   42.11/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 546.7s / 1828.2s
Train: 81 [1200/5000 ( 24%)]  Loss: 4.28 (4.09)  Time: 0.451s,   44.34/s  (0.474s,   42.23/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 568.8s / 1799.2s
Train: 81 [1250/5000 ( 25%)]  Loss: 4.96 (4.09)  Time: 0.438s,   45.70/s  (0.472s,   42.34/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 591.0s / 1771.0s
Train: 81 [1300/5000 ( 26%)]  Loss: 4.53 (4.09)  Time: 0.441s,   45.35/s  (0.471s,   42.44/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 613.1s / 1743.2s
Train: 81 [1350/5000 ( 27%)]  Loss: 4.83 (4.09)  Time: 0.444s,   45.02/s  (0.472s,   42.37/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 637.7s / 1722.4s
Train: 81 [1400/5000 ( 28%)]  Loss: 4.66 (4.09)  Time: 0.441s,   45.39/s  (0.471s,   42.47/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 659.8s / 1694.8s
Train: 81 [1450/5000 ( 29%)]  Loss: 4.31 (4.09)  Time: 0.439s,   45.53/s  (0.470s,   42.56/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 681.9s / 1667.8s
Train: 81 [1500/5000 ( 30%)]  Loss: 4.22 (4.09)  Time: 0.442s,   45.28/s  (0.470s,   42.55/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 705.6s / 1644.8s
Train: 81 [1550/5000 ( 31%)]  Loss: 4.62 (4.09)  Time: 0.440s,   45.48/s  (0.469s,   42.63/s)  LR: 1.802e-05  Data: 0.012 (0.011)  Elapsed/ETA: 727.7s / 1618.1s
Train: 81 [1600/5000 ( 32%)]  Loss: 4.37 (4.09)  Time: 0.440s,   45.46/s  (0.468s,   42.70/s)  LR: 1.802e-05  Data: 0.015 (0.011)  Elapsed/ETA: 749.9s / 1592.0s
Train: 81 [1650/5000 ( 33%)]  Loss: 4.28 (4.09)  Time: 0.439s,   45.53/s  (0.468s,   42.78/s)  LR: 1.802e-05  Data: 0.012 (0.011)  Elapsed/ETA: 771.9s / 1565.7s
Train: 81 [1700/5000 ( 34%)]  Loss: 3.86 (4.09)  Time: 0.440s,   45.50/s  (0.467s,   42.85/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 793.9s / 1539.6s
Train: 81 [1750/5000 ( 35%)]  Loss: 3.84 (4.09)  Time: 0.440s,   45.44/s  (0.466s,   42.92/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 815.9s / 1514.0s
Train: 81 [1800/5000 ( 36%)]  Loss: 3.87 (4.09)  Time: 0.442s,   45.22/s  (0.465s,   42.98/s)  LR: 1.802e-05  Data: 0.012 (0.011)  Elapsed/ETA: 838.0s / 1488.5s
Train: 81 [1850/5000 ( 37%)]  Loss: 4.47 (4.09)  Time: 0.468s,   42.76/s  (0.467s,   42.81/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 864.7s / 1471.1s
Train: 81 [1900/5000 ( 38%)]  Loss: 4.26 (4.09)  Time: 0.441s,   45.32/s  (0.466s,   42.87/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 886.8s / 1445.7s
Train: 81 [1950/5000 ( 39%)]  Loss: 4.62 (4.09)  Time: 0.454s,   44.04/s  (0.466s,   42.93/s)  LR: 1.802e-05  Data: 0.017 (0.011)  Elapsed/ETA: 909.0s / 1420.5s
Train: 81 [2000/5000 ( 40%)]  Loss: 3.68 (4.09)  Time: 0.443s,   45.13/s  (0.465s,   42.98/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 931.2s / 1395.6s
Train: 81 [2050/5000 ( 41%)]  Loss: 4.65 (4.09)  Time: 0.450s,   44.42/s  (0.466s,   42.94/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 955.2s / 1373.4s
Train: 81 [2100/5000 ( 42%)]  Loss: 4.61 (4.09)  Time: 0.437s,   45.76/s  (0.465s,   43.00/s)  LR: 1.802e-05  Data: 0.012 (0.011)  Elapsed/ETA: 977.3s / 1348.5s
Train: 81 [2150/5000 ( 43%)]  Loss: 4.51 (4.09)  Time: 0.439s,   45.58/s  (0.465s,   43.05/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 999.3s / 1323.6s
Train: 81 [2200/5000 ( 44%)]  Loss: 4.31 (4.09)  Time: 0.441s,   45.36/s  (0.464s,   43.07/s)  LR: 1.802e-05  Data: 0.012 (0.011)  Elapsed/ETA: 1022.2s / 1299.9s
Train: 81 [2250/5000 ( 45%)]  Loss: 4.41 (4.09)  Time: 0.443s,   45.16/s  (0.464s,   43.11/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1044.2s / 1275.2s
Train: 81 [2300/5000 ( 46%)]  Loss: 3.93 (4.09)  Time: 0.439s,   45.57/s  (0.463s,   43.16/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1066.3s / 1250.7s
Train: 81 [2350/5000 ( 47%)]  Loss: 4.78 (4.09)  Time: 0.439s,   45.56/s  (0.463s,   43.21/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1088.3s / 1226.2s
Train: 81 [2400/5000 ( 48%)]  Loss: 4.49 (4.09)  Time: 0.437s,   45.73/s  (0.462s,   43.25/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1110.3s / 1201.8s
Train: 81 [2450/5000 ( 49%)]  Loss: 3.77 (4.09)  Time: 0.439s,   45.57/s  (0.462s,   43.29/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1132.3s / 1177.6s
Train: 81 [2500/5000 ( 50%)]  Loss: 3.89 (4.09)  Time: 0.441s,   45.39/s  (0.462s,   43.33/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1154.5s / 1153.6s
Train: 81 [2550/5000 ( 51%)]  Loss: 4.57 (4.08)  Time: 0.436s,   45.88/s  (0.461s,   43.35/s)  LR: 1.802e-05  Data: 0.009 (0.011)  Elapsed/ETA: 1177.0s / 1129.9s
Train: 81 [2600/5000 ( 52%)]  Loss: 4.62 (4.08)  Time: 0.439s,   45.58/s  (0.461s,   43.38/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1199.1s / 1106.0s
Train: 81 [2650/5000 ( 53%)]  Loss: 3.68 (4.08)  Time: 0.446s,   44.84/s  (0.461s,   43.41/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1221.2s / 1082.1s
Train: 81 [2700/5000 ( 54%)]  Loss: 4.65 (4.08)  Time: 0.574s,   34.84/s  (0.461s,   43.43/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1244.0s / 1058.8s
Train: 81 [2750/5000 ( 55%)]  Loss: 4.02 (4.08)  Time: 0.439s,   45.56/s  (0.461s,   43.41/s)  LR: 1.802e-05  Data: 0.012 (0.011)  Elapsed/ETA: 1267.5s / 1036.2s
Train: 81 [2800/5000 ( 56%)]  Loss: 4.57 (4.08)  Time: 0.438s,   45.70/s  (0.461s,   43.40/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1290.7s / 1013.3s
Train: 81 [2850/5000 ( 57%)]  Loss: 4.53 (4.08)  Time: 0.439s,   45.55/s  (0.460s,   43.44/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1312.7s / 989.5s
Train: 81 [2900/5000 ( 58%)]  Loss: 4.64 (4.08)  Time: 0.443s,   45.17/s  (0.461s,   43.42/s)  LR: 1.802e-05  Data: 0.014 (0.011)  Elapsed/ETA: 1336.2s / 966.8s
Train: 81 [2950/5000 ( 59%)]  Loss: 3.34 (4.08)  Time: 0.436s,   45.88/s  (0.460s,   43.45/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1358.2s / 943.1s
Train: 81 [3000/5000 ( 60%)]  Loss: 4.40 (4.08)  Time: 0.439s,   45.56/s  (0.460s,   43.48/s)  LR: 1.802e-05  Data: 0.009 (0.011)  Elapsed/ETA: 1380.3s / 919.4s
Train: 81 [3050/5000 ( 61%)]  Loss: 4.39 (4.08)  Time: 0.449s,   44.57/s  (0.460s,   43.51/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1402.4s / 895.9s
Train: 81 [3100/5000 ( 62%)]  Loss: 4.97 (4.08)  Time: 0.441s,   45.32/s  (0.459s,   43.54/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1424.5s / 872.3s
Train: 81 [3150/5000 ( 63%)]  Loss: 4.62 (4.08)  Time: 0.440s,   45.42/s  (0.459s,   43.56/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1446.7s / 848.9s
Train: 81 [3200/5000 ( 64%)]  Loss: 4.50 (4.08)  Time: 0.441s,   45.32/s  (0.459s,   43.59/s)  LR: 1.802e-05  Data: 0.012 (0.011)  Elapsed/ETA: 1468.8s / 825.5s
Train: 81 [3250/5000 ( 65%)]  Loss: 3.82 (4.08)  Time: 0.483s,   41.42/s  (0.459s,   43.61/s)  LR: 1.802e-05  Data: 0.012 (0.011)  Elapsed/ETA: 1491.1s / 802.2s
Train: 81 [3300/5000 ( 66%)]  Loss: 3.66 (4.08)  Time: 0.438s,   45.71/s  (0.458s,   43.63/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1513.3s / 778.9s
Train: 81 [3350/5000 ( 67%)]  Loss: 4.77 (4.08)  Time: 0.443s,   45.19/s  (0.458s,   43.64/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1535.6s / 755.7s
Train: 81 [3400/5000 ( 68%)]  Loss: 4.31 (4.08)  Time: 0.439s,   45.61/s  (0.459s,   43.61/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1559.6s / 733.2s
Train: 81 [3450/5000 ( 69%)]  Loss: 3.28 (4.07)  Time: 0.442s,   45.27/s  (0.459s,   43.62/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1582.3s / 710.2s
Train: 81 [3500/5000 ( 70%)]  Loss: 3.93 (4.07)  Time: 0.445s,   44.90/s  (0.458s,   43.64/s)  LR: 1.802e-05  Data: 0.012 (0.011)  Elapsed/ETA: 1604.4s / 687.0s
Train: 81 [3550/5000 ( 71%)]  Loss: 4.64 (4.07)  Time: 0.441s,   45.31/s  (0.458s,   43.66/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1626.7s / 663.8s
Train: 81 [3600/5000 ( 72%)]  Loss: 4.38 (4.07)  Time: 0.446s,   44.88/s  (0.458s,   43.68/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1648.7s / 640.5s
Train: 81 [3650/5000 ( 73%)]  Loss: 3.98 (4.07)  Time: 0.579s,   34.53/s  (0.458s,   43.64/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1673.1s / 618.2s
Train: 81 [3700/5000 ( 74%)]  Loss: 4.54 (4.07)  Time: 0.574s,   34.86/s  (0.459s,   43.57/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1698.8s / 596.3s
Train: 81 [3750/5000 ( 75%)]  Loss: 3.77 (4.07)  Time: 0.441s,   45.37/s  (0.460s,   43.47/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1725.6s / 574.6s
Train: 81 [3800/5000 ( 76%)]  Loss: 3.87 (4.07)  Time: 0.440s,   45.45/s  (0.460s,   43.50/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1747.7s / 551.3s
Train: 81 [3850/5000 ( 77%)]  Loss: 4.82 (4.07)  Time: 0.438s,   45.66/s  (0.460s,   43.52/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1769.8s / 528.0s
Train: 81 [3900/5000 ( 78%)]  Loss: 3.29 (4.07)  Time: 0.447s,   44.78/s  (0.459s,   43.54/s)  LR: 1.802e-05  Data: 0.015 (0.011)  Elapsed/ETA: 1791.9s / 504.8s
Train: 81 [3950/5000 ( 79%)]  Loss: 4.19 (4.07)  Time: 0.443s,   45.17/s  (0.459s,   43.56/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1814.0s / 481.6s
Train: 81 [4000/5000 ( 80%)]  Loss: 3.70 (4.07)  Time: 0.437s,   45.72/s  (0.459s,   43.57/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1836.8s / 458.6s
Train: 81 [4050/5000 ( 81%)]  Loss: 3.57 (4.07)  Time: 0.444s,   45.00/s  (0.459s,   43.58/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1858.9s / 435.5s
Train: 81 [4100/5000 ( 82%)]  Loss: 4.39 (4.07)  Time: 0.476s,   41.98/s  (0.459s,   43.59/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1881.7s / 412.5s
Train: 81 [4150/5000 ( 83%)]  Loss: 4.59 (4.07)  Time: 0.439s,   45.53/s  (0.459s,   43.60/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1904.0s / 389.4s
Train: 81 [4200/5000 ( 84%)]  Loss: 4.62 (4.07)  Time: 0.440s,   45.46/s  (0.459s,   43.62/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1926.2s / 366.4s
Train: 81 [4250/5000 ( 85%)]  Loss: 4.45 (4.07)  Time: 0.441s,   45.32/s  (0.458s,   43.64/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1948.3s / 343.3s
Train: 81 [4300/5000 ( 86%)]  Loss: 4.25 (4.07)  Time: 0.439s,   45.53/s  (0.459s,   43.62/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 1972.1s / 320.5s
Train: 81 [4350/5000 ( 87%)]  Loss: 3.26 (4.07)  Time: 0.443s,   45.16/s  (0.458s,   43.64/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 1994.2s / 297.5s
Train: 81 [4400/5000 ( 88%)]  Loss: 3.80 (4.07)  Time: 0.583s,   34.29/s  (0.459s,   43.62/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 2018.1s / 274.7s
Train: 81 [4450/5000 ( 89%)]  Loss: 3.37 (4.07)  Time: 0.436s,   45.86/s  (0.459s,   43.62/s)  LR: 1.802e-05  Data: 0.010 (0.011)  Elapsed/ETA: 2040.9s / 251.7s
Train: 81 [4500/5000 ( 90%)]  Loss: 4.86 (4.07)  Time: 0.438s,   45.64/s  (0.459s,   43.57/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2066.0s / 229.0s
Train: 81 [4550/5000 ( 91%)]  Loss: 3.50 (4.07)  Time: 0.438s,   45.66/s  (0.459s,   43.59/s)  LR: 1.802e-05  Data: 0.011 (0.011)  Elapsed/ETA: 2087.9s / 206.0s
