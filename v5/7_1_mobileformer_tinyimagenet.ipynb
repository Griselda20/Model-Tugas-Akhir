{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp4-hpoJ0C99",
        "outputId": "408d2c97-d402-49cb-8771-53af33a4b173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-07 02:42:16--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs231n.stanford.edu/tiny-imagenet-200.zip [following]\n",
            "--2025-01-07 02:42:16--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  6.42MB/s    in 39s     \n",
            "\n",
            "2025-01-07 02:42:56 (6.11 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (11.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=3d43abe2aa7ae7a32f1def7d39b6c7925885e09e8b8cb7e7910efad934d60a49\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=0aedde5a525bbaa8f7c569c2e369b5008d90d10f6f537571cad36e77578aab13\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip\n",
        "!pip install fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dVE5NtM2D6V"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import functional as F, ToPILImage\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset, random_split, Subset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import os, glob\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2lUMjFt2wcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71163f29-a57f-4483-e41a-f05a50e09563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "else:\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TeuyttdpeaR"
      },
      "source": [
        "Dna blocks used for Mobile-Former"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xhZNoPD3VfU"
      },
      "outputs": [],
      "source": [
        "\"\"\" Dna blocks used for Mobile-Former\n",
        "\n",
        "A PyTorch impl of Dna blocks\n",
        "\n",
        "Paper: Mobile-Former: Bridging MobileNet and Transformer (CVPR 2022)\n",
        "       https://arxiv.org/abs/2108.05895\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from timm.models.layers import DropPath\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class h_sigmoid(nn.Module):\n",
        "    def __init__(self, inplace=True, h_max=1):\n",
        "        super(h_sigmoid, self).__init__()\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\n",
        "        self.h_max = h_max\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(x + 3) * self.h_max / 6\n",
        "\n",
        "class h_swish(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_swish, self).__init__()\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)\n",
        "\n",
        "class ChannelShuffle(nn.Module):\n",
        "    def __init__(self, groups):\n",
        "        super(ChannelShuffle, self).__init__()\n",
        "        self.groups = groups\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.size()\n",
        "\n",
        "        channels_per_group = c // self.groups\n",
        "\n",
        "        # reshape\n",
        "        x = x.view(b, self.groups, channels_per_group, h, w)\n",
        "\n",
        "        x = torch.transpose(x, 1, 2).contiguous()\n",
        "\n",
        "        # flatten\n",
        "        out = x.view(b, -1, h, w)\n",
        "        return out\n",
        "\n",
        "class DyReLU(nn.Module):\n",
        "    def __init__(self, num_func=2, use_bias=False, scale=2., serelu=False):\n",
        "        \"\"\"\n",
        "        num_func: -1: none\n",
        "                   0: relu\n",
        "                   1: SE\n",
        "                   2: dy-relu\n",
        "        \"\"\"\n",
        "        super(DyReLU, self).__init__()\n",
        "\n",
        "        assert(num_func>=-1 and num_func<=2)\n",
        "        self.num_func = num_func\n",
        "        self.scale = scale\n",
        "\n",
        "        serelu = serelu and num_func == 1\n",
        "        self.act = nn.ReLU6(inplace=True) if num_func == 0 or serelu else nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, tuple):\n",
        "            out, a = x\n",
        "        else:\n",
        "            out = x\n",
        "\n",
        "        out = self.act(out)\n",
        "\n",
        "\n",
        "        if self.num_func == 1:    # SE\n",
        "            a = a * self.scale\n",
        "            out = out * a\n",
        "        elif self.num_func == 2:  # DY-ReLU\n",
        "            _, C, _, _ = a.shape\n",
        "            a1, a2 = torch.split(a, [C//2, C//2], dim=1)\n",
        "            a1 = (a1 - 0.5) * self.scale + 1.0 #  0.0 -- 2.0\n",
        "            a2 = (a2 - 0.5) * self.scale       # -1.0 -- 1.0\n",
        "            out = torch.max(out*a1, out*a2)\n",
        "\n",
        "        return out\n",
        "\n",
        "class HyperFunc(nn.Module):\n",
        "    def __init__(self, token_dim, oup, sel_token_id=0, reduction_ratio=4):\n",
        "        super(HyperFunc, self).__init__()\n",
        "\n",
        "        self.sel_token_id = sel_token_id\n",
        "        squeeze_dim = token_dim // reduction_ratio\n",
        "        self.hyper = nn.Sequential(\n",
        "            nn.Linear(token_dim, squeeze_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(squeeze_dim, oup),\n",
        "            h_sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, tuple):\n",
        "            x, attn = x\n",
        "\n",
        "        if self.sel_token_id == -1:\n",
        "            hp = self.hyper(x).permute(1, 2, 0)         # bs x hyper_dim x T\n",
        "\n",
        "            bs, T, H, W = attn.shape\n",
        "            attn = attn.view(bs, T, H*W)\n",
        "            hp = torch.matmul(hp, attn)                  # bs x hyper_dim x HW\n",
        "            h = hp.view(bs, -1, H, W)\n",
        "        else:\n",
        "            t = x[self.sel_token_id]\n",
        "            h = self.hyper(t)\n",
        "            h = torch.unsqueeze(torch.unsqueeze(h, 2), 3)\n",
        "        return h\n",
        "\n",
        "class MaxDepthConv(nn.Module):\n",
        "    def __init__(self, inp, oup, stride):\n",
        "        super(MaxDepthConv, self).__init__()\n",
        "        self.inp = inp\n",
        "        self.oup = oup\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(inp, oup, (3,1), stride, (1, 0), bias=False, groups=inp),\n",
        "            nn.BatchNorm2d(oup)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(inp, oup, (1,3), stride, (0, 1), bias=False, groups=inp),\n",
        "            nn.BatchNorm2d(oup)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.conv1(x)\n",
        "        y2 = self.conv2(x)\n",
        "\n",
        "        out = torch.max(y1, y2)\n",
        "        return out\n",
        "\n",
        "class Local2GlobalAttn(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp,\n",
        "        token_dim=128,\n",
        "        token_num=6,\n",
        "        inp_res=0,\n",
        "        norm_pos='post',\n",
        "        drop_path_rate=0.\n",
        "    ):\n",
        "        super(Local2GlobalAttn, self).__init__()\n",
        "\n",
        "        num_heads = 2\n",
        "        self.scale = (inp // num_heads) ** -0.5\n",
        "\n",
        "        self.q = nn.Linear(token_dim, inp)\n",
        "        self.proj = nn.Linear(inp, token_dim)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(token_dim)\n",
        "        self.drop_path = DropPath(drop_path_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        features, tokens = x\n",
        "        bs, C, _, _ = features.shape\n",
        "\n",
        "        t = self.q(tokens).permute(1, 0, 2) # from T x bs x Ct to bs x T x Ct\n",
        "        k = features.view(bs, C, -1)        # bs x C x HW\n",
        "        attn = (t @ k) * self.scale\n",
        "\n",
        "        attn_out = attn.softmax(dim=-1)             # bs x T x HW\n",
        "        attn_out = (attn_out @ k.permute(0, 2, 1))  # bs x T x C\n",
        "                                                    # note here: k=v without transform\n",
        "        t = self.proj(attn_out.permute(1, 0, 2))    #T x bs x C\n",
        "\n",
        "        tokens = tokens + self.drop_path(t)\n",
        "        tokens = self.layer_norm(tokens)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "class Local2Global(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            inp,\n",
        "            block_type='mlp',\n",
        "            token_dim=128,\n",
        "            token_num=6,\n",
        "            inp_res=0,\n",
        "            attn_num_heads=2,\n",
        "            use_dynamic=False,\n",
        "            norm_pos='post',\n",
        "            drop_path_rate=0.,\n",
        "            remove_proj_local=True,\n",
        "        ):\n",
        "        super(Local2Global, self).__init__()\n",
        "        print(f'L2G: {attn_num_heads} heads, inp: {inp}, token: {token_dim}')\n",
        "\n",
        "        self.num_heads = attn_num_heads\n",
        "        self.token_num = token_num\n",
        "        self.norm_pos = norm_pos\n",
        "        self.block = block_type\n",
        "        self.use_dynamic = use_dynamic\n",
        "\n",
        "        if self.use_dynamic:\n",
        "            self.alpha_scale = 2.0\n",
        "            self.alpha = nn.Sequential(\n",
        "                nn.Linear(token_dim, inp),\n",
        "                h_sigmoid(),\n",
        "            )\n",
        "\n",
        "\n",
        "        if 'mlp' in block_type:\n",
        "            self.mlp = nn.Linear(inp_res, token_num)\n",
        "\n",
        "        if 'attn' in block_type:\n",
        "            self.scale = (inp // attn_num_heads) ** -0.5\n",
        "            self.q = nn.Linear(token_dim, inp)\n",
        "\n",
        "        self.proj = nn.Linear(inp, token_dim)\n",
        "        self.layer_norm = nn.LayerNorm(token_dim)\n",
        "        self.drop_path = DropPath(drop_path_rate)\n",
        "\n",
        "        self.remove_proj_local = remove_proj_local\n",
        "        if self.remove_proj_local == False:\n",
        "            self.k = nn.Conv2d(inp, inp, 1, 1, 0, bias=False)\n",
        "            self.v = nn.Conv2d(inp, inp, 1, 1, 0, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        features, tokens = x # features: bs x C x H x W\n",
        "                             #   tokens: T x bs x Ct\n",
        "\n",
        "        bs, C, H, W = features.shape\n",
        "        T, _, _ = tokens.shape\n",
        "        attn = None\n",
        "\n",
        "        if 'mlp' in self.block:\n",
        "            t_sum = self.mlp(features.view(bs, C, -1)).permute(2, 0, 1) # T x bs x C\n",
        "\n",
        "        if 'attn' in self.block:\n",
        "            t = self.q(tokens).view(T, bs, self.num_heads, -1).permute(1, 2, 0, 3)  # from T x bs x Ct to bs x N x T x Ct/N\n",
        "            if self.remove_proj_local:\n",
        "                k = features.view(bs, self.num_heads, -1, H*W)                          # bs x N x C/N x HW\n",
        "                attn = (t @ k) * self.scale                                             # bs x N x T x HW\n",
        "\n",
        "                attn_out = attn.softmax(dim=-1)                 # bs x N x T x HW\n",
        "                attn_out = (attn_out @ k.transpose(-1, -2))     # bs x N x T x C/N (k: bs x N x C/N x HW)\n",
        "                                                                # note here: k=v without transform\n",
        "            else:\n",
        "                k = self.k(features).view(bs, self.num_heads, -1, H*W)                          # bs x N x C/N x HW\n",
        "                v = self.v(features).view(bs, self.num_heads, -1, H*W)                          # bs x N x C/N x HW\n",
        "                attn = (t @ k) * self.scale                                             # bs x N x T x HW\n",
        "\n",
        "                attn_out = attn.softmax(dim=-1)                 # bs x N x T x HW\n",
        "                attn_out = (attn_out @ v.transpose(-1, -2))     # bs x N x T x C/N (k: bs x N x C/N x HW)\n",
        "                                                                # note here: k=v without transform\n",
        "\n",
        "            t_a = attn_out.permute(2, 0, 1, 3)              # T x bs x N x C/N\n",
        "            t_a = t_a.reshape(T, bs, -1)\n",
        "\n",
        "            if 'mlp' in self.block:\n",
        "                t_sum = t_sum + t_a\n",
        "            else:\n",
        "                t_sum = t_a\n",
        "\n",
        "        if self.use_dynamic:\n",
        "            alp = self.alpha(tokens) * self.alpha_scale\n",
        "            t_sum = t_sum * alp\n",
        "\n",
        "        t_sum = self.proj(t_sum)\n",
        "        tokens = tokens + self.drop_path(t_sum)\n",
        "        tokens = self.layer_norm(tokens)\n",
        "\n",
        "        if attn is not None:\n",
        "            bs, Nh, Ca, HW = attn.shape\n",
        "            attn = attn.view(bs, Nh, Ca, H, W)\n",
        "\n",
        "        return tokens, attn\n",
        "\n",
        "class GlobalBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block_type='mlp',\n",
        "        token_dim=128,\n",
        "        token_num=6,\n",
        "        mlp_token_exp=4,\n",
        "        attn_num_heads=4,\n",
        "        use_dynamic=False,\n",
        "        use_ffn=False,\n",
        "        norm_pos='post',\n",
        "        drop_path_rate=0.\n",
        "    ):\n",
        "        super(GlobalBlock, self).__init__()\n",
        "\n",
        "        print(f'G2G: {attn_num_heads} heads')\n",
        "\n",
        "        self.block = block_type\n",
        "        self.num_heads = attn_num_heads\n",
        "        self.token_num = token_num\n",
        "        self.norm_pos = norm_pos\n",
        "        self.use_dynamic = use_dynamic\n",
        "        self.use_ffn = use_ffn\n",
        "        self.ffn_exp = 2\n",
        "\n",
        "        if self.use_ffn:\n",
        "            print('use ffn')\n",
        "            self.ffn = nn.Sequential(\n",
        "                nn.Linear(token_dim, token_dim * self.ffn_exp),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(token_dim * self.ffn_exp, token_dim)\n",
        "            )\n",
        "            self.ffn_norm = nn.LayerNorm(token_dim)\n",
        "\n",
        "\n",
        "        if self.use_dynamic:\n",
        "            self.alpha_scale = 2.0\n",
        "            self.alpha = nn.Sequential(\n",
        "                nn.Linear(token_dim, token_dim),\n",
        "                h_sigmoid(),\n",
        "            )\n",
        "\n",
        "\n",
        "        if 'mlp' in self.block:\n",
        "            self.token_mlp = nn.Sequential(\n",
        "                nn.Linear(token_num, token_num*mlp_token_exp),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(token_num*mlp_token_exp, token_num),\n",
        "            )\n",
        "\n",
        "        if 'attn' in self.block:\n",
        "            self.scale = (token_dim // attn_num_heads) ** -0.5\n",
        "            self.q = nn.Linear(token_dim, token_dim)\n",
        "\n",
        "        self.channel_mlp = nn.Linear(token_dim, token_dim)\n",
        "        self.layer_norm = nn.LayerNorm(token_dim)\n",
        "        self.drop_path = DropPath(drop_path_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tokens = x\n",
        "\n",
        "        T, bs, C = tokens.shape\n",
        "\n",
        "        if 'mlp' in self.block:\n",
        "            # use post norm, token.shape: token_num x bs x channel\n",
        "            t = self.token_mlp(tokens.permute(1, 2, 0)) # bs x channel x token_num\n",
        "            t_sum = t.permute(2, 0, 1)                  # token_num x bs x channel\n",
        "\n",
        "        if 'attn' in self.block:\n",
        "            t = self.q(tokens).view(T, bs, self.num_heads, -1).permute(1, 2, 0, 3)  # from T x bs x Ct to bs x N x T x Ct/N\n",
        "            k = tokens.permute(1, 2, 0).view(bs, self.num_heads, -1, T)             # from T x bs x Ct -> bs x Ct x T -> bs x N x Ct/N x T\n",
        "            attn = (t @ k) * self.scale                                             # bs x N x T x T\n",
        "\n",
        "            attn_out = attn.softmax(dim=-1)                 # bs x N x T x T\n",
        "            attn_out = (attn_out @ k.transpose(-1, -2))     # bs x N x T x C/N (k: bs x N x Ct/N x T)\n",
        "                                                            # note here: k=v without transform\n",
        "            t_a = attn_out.permute(2, 0, 1, 3)              # T x bs x N x C/N\n",
        "            t_a = t_a.reshape(T, bs, -1)\n",
        "\n",
        "            t_sum = t_sum + t_a if 'mlp' in self.block else t_a\n",
        "\n",
        "        if self.use_dynamic:\n",
        "            alp = self.alpha(tokens) * self.alpha_scale\n",
        "            t_sum = t_sum * alp\n",
        "\n",
        "        t_sum = self.channel_mlp(t_sum)  # token_num x bs x channel\n",
        "        tokens = tokens + self.drop_path(t_sum)\n",
        "        tokens = self.layer_norm(tokens)\n",
        "\n",
        "        if self.use_ffn:\n",
        "            t_ffn = self.ffn(tokens)\n",
        "            tokens = tokens + t_ffn\n",
        "            tokens = self.ffn_norm(tokens)\n",
        "\n",
        "\n",
        "        return tokens\n",
        "\n",
        "class Global2Local(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp,\n",
        "        inp_res=0,\n",
        "        block_type='mlp',\n",
        "        token_dim=128,\n",
        "        token_num=6,\n",
        "        attn_num_heads=2,\n",
        "        use_dynamic=False,\n",
        "        drop_path_rate=0.,\n",
        "        remove_proj_local=True,\n",
        "    ):\n",
        "        super(Global2Local, self).__init__()\n",
        "        print(f'G2L: {attn_num_heads} heads, inp: {inp}, token: {token_dim}')\n",
        "\n",
        "        self.token_num = token_num\n",
        "        self.num_heads = attn_num_heads\n",
        "        self.block = block_type\n",
        "        self.use_dynamic = use_dynamic\n",
        "\n",
        "        if self.use_dynamic:\n",
        "            self.alpha_scale = 2.0\n",
        "            self.alpha = nn.Sequential(\n",
        "                nn.Linear(token_dim, inp),\n",
        "                h_sigmoid(),\n",
        "            )\n",
        "\n",
        "\n",
        "        if 'mlp' in self.block:\n",
        "            self.mlp = nn.Linear(token_num, inp_res)\n",
        "\n",
        "        if 'attn' in self.block:\n",
        "            self.scale = (inp // attn_num_heads) ** -0.5\n",
        "            self.k = nn.Linear(token_dim, inp)\n",
        "\n",
        "        self.proj = nn.Linear(token_dim, inp)\n",
        "        self.drop_path = DropPath(drop_path_rate)\n",
        "\n",
        "        self.remove_proj_local = remove_proj_local\n",
        "        if self.remove_proj_local == False:\n",
        "            self.q = nn.Conv2d(inp, inp, 1, 1, 0, bias=False)\n",
        "            self.fuse = nn.Conv2d(inp, inp, 1, 1, 0, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, tokens = x\n",
        "\n",
        "        if self.use_dynamic:\n",
        "            alp = self.alpha(tokens) * self.alpha_scale\n",
        "            v = self.proj(tokens)\n",
        "            v = (v * alp).permute(1, 2, 0)\n",
        "        else:\n",
        "            v = self.proj(tokens).permute(1, 2, 0)  # from T x bs x Ct -> T x bs x C -> bs x C x T\n",
        "\n",
        "        bs, C, H, W = out.shape\n",
        "        if 'mlp' in self.block:\n",
        "            g_sum = self.mlp(v).view(bs, C, H, W)       # bs x C x T -> bs x C x H x W\n",
        "\n",
        "        if 'attn' in self.block:\n",
        "            if self.remove_proj_local:\n",
        "                q = out.view(bs, self.num_heads, -1, H*W).transpose(-1, -2)                         # bs x N x HW x C/N\n",
        "            else:\n",
        "                q = self.q(out).view(bs, self.num_heads, -1, H*W).transpose(-1, -2)                         # bs x N x HW x C/N\n",
        "\n",
        "            k = self.k(tokens).permute(1, 2, 0).view(bs, self.num_heads, -1, self.token_num)    # from T x bs x Ct -> bs x C x T -> bs x N x C/N x T\n",
        "            attn = (q @ k) * self.scale                         # bs x N x HW x T\n",
        "\n",
        "            attn_out = attn.softmax(dim=-1)                     # bs x N x HW x T\n",
        "\n",
        "            vh = v.view(bs, self.num_heads, -1, self.token_num) # bs x N x C/N x T\n",
        "            attn_out = (attn_out @ vh.transpose(-1, -2))        # bs x N x HW x C/N\n",
        "                                                                # note here k != v\n",
        "            g_a = attn_out.transpose(-1, -2).reshape(bs, C, H, W)   # bs x C x HW\n",
        "\n",
        "            if self.remove_proj_local == False:\n",
        "                g_a = self.fuse(g_a)\n",
        "\n",
        "            g_sum = g_sum + g_a if 'mlp' in self.block else g_a\n",
        "\n",
        "        out = out + self.drop_path(g_sum)\n",
        "\n",
        "        return out\n",
        "\n",
        "##########################################################################################################\n",
        "# Dna Blocks\n",
        "##########################################################################################################\n",
        "class DnaBlock3(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp,\n",
        "        oup,\n",
        "        stride,\n",
        "        exp_ratios, #(e1, e2)\n",
        "        kernel_size=(3,3),\n",
        "        dw_conv='dw',\n",
        "        group_num=1,\n",
        "        se_flag=[2,0,2,0],\n",
        "        hyper_token_id=0,\n",
        "        hyper_reduction_ratio=4,\n",
        "        token_dim=128,\n",
        "        token_num=6,\n",
        "        inp_res=49,\n",
        "        gbr_type='mlp',\n",
        "        gbr_dynamic=[False, False, False],\n",
        "        gbr_ffn=False,\n",
        "        gbr_before_skip=False,\n",
        "        mlp_token_exp=4,\n",
        "        norm_pos='post',\n",
        "        drop_path_rate=0.,\n",
        "        cnn_drop_path_rate=0.,\n",
        "        attn_num_heads=2,\n",
        "        remove_proj_local=True,\n",
        "    ):\n",
        "        super(DnaBlock3, self).__init__()\n",
        "\n",
        "        print(f'block: {inp_res}, cnn-drop {cnn_drop_path_rate:.4f}, mlp-drop {drop_path_rate:.4f}')\n",
        "        if isinstance(exp_ratios, tuple):\n",
        "            e1, e2 = exp_ratios\n",
        "        else:\n",
        "            e1, e2 = exp_ratios, 4\n",
        "        k1, k2 = kernel_size\n",
        "\n",
        "        self.stride = stride\n",
        "        self.hyper_token_id = hyper_token_id\n",
        "\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "        self.use_conv_alone = False\n",
        "        if e1 == 1 or e2 == 0:\n",
        "            self.use_conv_alone = True\n",
        "            if dw_conv == 'dw':\n",
        "                self.conv = nn.Sequential(\n",
        "                    # dw\n",
        "                    nn.Conv2d(inp, inp*e1, 3, stride, 1, groups=inp, bias=False),\n",
        "                    nn.BatchNorm2d(inp*e1),\n",
        "                    nn.ReLU6(inplace=True),\n",
        "                    ChannelShuffle(inp) if group_num > 1 else nn.Sequential(),\n",
        "                    # pw-linear\n",
        "                    nn.Conv2d(inp*e1, oup, 1, 1, 0, groups=group_num, bias=False),\n",
        "                    nn.BatchNorm2d(oup),\n",
        "                )\n",
        "            elif dw_conv == 'sepdw':\n",
        "                self.conv = nn.Sequential(\n",
        "                    # dw\n",
        "                    nn.Conv2d(inp, inp*e1//2, (3,1), (stride,1), (1,0), groups=inp, bias=False),\n",
        "                    nn.BatchNorm2d(inp*e1//2),\n",
        "                    nn.Conv2d(inp*e1//2, inp*e1, (1,3), (1, stride), (0,1), groups=inp*e1//2, bias=False),\n",
        "                    nn.BatchNorm2d(inp*e1),\n",
        "                    nn.ReLU6(inplace=True),\n",
        "                    ChannelShuffle(inp) if group_num > 1 else nn.Sequential(),\n",
        "                    # pw-linear\n",
        "                    nn.Conv2d(inp*e1, oup, 1, 1, 0, groups=group_num, bias=False),\n",
        "                    nn.BatchNorm2d(oup),\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            # conv (dw->pw->dw->pw)\n",
        "            self.se_flag = se_flag\n",
        "            hidden_dim1 = round(inp * e1)\n",
        "            hidden_dim2 = round(oup * e2)\n",
        "\n",
        "            if dw_conv == 'dw':\n",
        "                self.conv1 = nn.Sequential(\n",
        "                    nn.Conv2d(inp, hidden_dim1, k1, stride, k1//2, groups=inp, bias=False),\n",
        "                    nn.BatchNorm2d(hidden_dim1),\n",
        "                    ChannelShuffle(inp) if group_num > 1 else nn.Sequential()\n",
        "                )\n",
        "            elif dw_conv == 'maxdw':\n",
        "                self.conv1 = nn.Sequential(\n",
        "                    MaxDepthConv(inp, hidden_dim1, stride),\n",
        "                    ChannelShuffle(group_num) if group_num > 1 else nn.Sequential()\n",
        "                )\n",
        "            elif dw_conv == 'sepdw':\n",
        "                self.conv1 = nn.Sequential(\n",
        "                    nn.Conv2d(inp, hidden_dim1//2, (3,1), (stride,1), (1,0), groups=inp, bias=False),\n",
        "                    nn.BatchNorm2d(hidden_dim1//2),\n",
        "                    nn.Conv2d(hidden_dim1//2, hidden_dim1, (1,3), (1, stride), (0,1), groups=hidden_dim1//2, bias=False),\n",
        "                    nn.BatchNorm2d(hidden_dim1),\n",
        "                    ChannelShuffle(inp) if group_num > 1 else nn.Sequential()\n",
        "                )\n",
        "\n",
        "            num_func = se_flag[0]\n",
        "            self.act1 = DyReLU(num_func=num_func, scale=2., serelu=True)\n",
        "            self.hyper1 = HyperFunc(\n",
        "                token_dim,\n",
        "                hidden_dim1 * num_func,\n",
        "                sel_token_id=hyper_token_id,\n",
        "                reduction_ratio=hyper_reduction_ratio\n",
        "            ) if se_flag[0] > 0 else nn.Sequential()\n",
        "\n",
        "\n",
        "            self.conv2 = nn.Sequential(\n",
        "                nn.Conv2d(hidden_dim1, oup, 1, 1, 0, groups=group_num, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "            num_func = -1\n",
        "            #num_func = 1 if se_flag[1] == 1 else -1\n",
        "            self.act2 = DyReLU(num_func=num_func, scale=2.)\n",
        "\n",
        "\n",
        "            if dw_conv == 'dw':\n",
        "                self.conv3 = nn.Sequential(\n",
        "                    nn.Conv2d(oup, hidden_dim2, k2, 1, k2//2, groups=oup, bias=False),\n",
        "                    nn.BatchNorm2d(hidden_dim2),\n",
        "                    ChannelShuffle(oup) if group_num > 1 else nn.Sequential()\n",
        "                )\n",
        "            elif dw_conv == 'maxdw':\n",
        "                self.conv3 = nn.Sequential(\n",
        "                    MaxDepthConv(oup, hidden_dim2, 1),\n",
        "                )\n",
        "            elif dw_conv == 'sepdw':\n",
        "                self.conv3 = nn.Sequential(\n",
        "                    nn.Conv2d(oup, hidden_dim2//2, (3,1), (1,1), (1,0), groups=oup, bias=False),\n",
        "                    nn.BatchNorm2d(hidden_dim2//2),\n",
        "                    nn.Conv2d(hidden_dim2//2, hidden_dim2, (1,3), (1, 1), (0,1), groups=hidden_dim2//2, bias=False),\n",
        "                    nn.BatchNorm2d(hidden_dim2),\n",
        "                    ChannelShuffle(oup) if group_num > 1 else nn.Sequential()\n",
        "                )\n",
        "\n",
        "            num_func = se_flag[2]\n",
        "            self.act3 = DyReLU(num_func=num_func, scale=2., serelu=True)\n",
        "            self.hyper3 = HyperFunc(\n",
        "                token_dim,\n",
        "                hidden_dim2 * num_func,\n",
        "                sel_token_id=hyper_token_id,\n",
        "                reduction_ratio=hyper_reduction_ratio\n",
        "            ) if se_flag[2] > 0 else nn.Sequential()\n",
        "\n",
        "\n",
        "            self.conv4 = nn.Sequential(\n",
        "                nn.Conv2d(hidden_dim2, oup, 1, 1, 0, groups=group_num, bias=False),\n",
        "                nn.BatchNorm2d(oup)\n",
        "            )\n",
        "            num_func = 1 if se_flag[3] == 1 else -1\n",
        "            self.act4 = DyReLU(num_func=num_func, scale=2.)\n",
        "            self.hyper4 = HyperFunc(\n",
        "                token_dim,\n",
        "                oup * num_func,\n",
        "                sel_token_id=hyper_token_id,\n",
        "                reduction_ratio=hyper_reduction_ratio\n",
        "            ) if se_flag[3] > 0 else nn.Sequential()\n",
        "\n",
        "\n",
        "            self.drop_path = DropPath(cnn_drop_path_rate)\n",
        "\n",
        "            # l2g, gb, g2l\n",
        "            self.local_global = Local2Global(\n",
        "                inp,\n",
        "                block_type = gbr_type,\n",
        "                token_dim=token_dim,\n",
        "                token_num=token_num,\n",
        "                inp_res=inp_res,\n",
        "                use_dynamic = gbr_dynamic[0],\n",
        "                norm_pos=norm_pos,\n",
        "                drop_path_rate=drop_path_rate,\n",
        "                attn_num_heads=attn_num_heads,\n",
        "                remove_proj_local=remove_proj_local,\n",
        "            )\n",
        "\n",
        "            self.global_block = GlobalBlock(\n",
        "                block_type=gbr_type,\n",
        "                token_dim=token_dim,\n",
        "                token_num=token_num,\n",
        "                mlp_token_exp=mlp_token_exp,\n",
        "                use_dynamic = gbr_dynamic[1],\n",
        "                use_ffn=gbr_ffn,\n",
        "                norm_pos=norm_pos,\n",
        "                drop_path_rate=drop_path_rate\n",
        "            )\n",
        "\n",
        "            oup_res = inp_res // (stride * stride)\n",
        "\n",
        "            self.global_local = Global2Local(\n",
        "                oup,\n",
        "                oup_res,\n",
        "                block_type=gbr_type,\n",
        "                token_dim=token_dim,\n",
        "                token_num=token_num,\n",
        "                use_dynamic = gbr_dynamic[2],\n",
        "                drop_path_rate=drop_path_rate,\n",
        "                attn_num_heads=attn_num_heads,\n",
        "                remove_proj_local=remove_proj_local,\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features, tokens = x\n",
        "        if self.use_conv_alone:\n",
        "            out = self.conv(features)\n",
        "        else:\n",
        "            # step 1: local to global\n",
        "            tokens, attn = self.local_global((features, tokens))\n",
        "            tokens = self.global_block(tokens)\n",
        "\n",
        "            # step 2: conv1 + conv2\n",
        "            out = self.conv1(features)\n",
        "\n",
        "            # process attn: mean, downsample if stride > 1, and softmax\n",
        "            if self.hyper_token_id == -1:\n",
        "                attn = attn.mean(dim=1) # bs x T x H x W\n",
        "                if self.stride > 1:\n",
        "                    _, _, H, W = out.shape\n",
        "                    attn = F.adaptive_avg_pool2d(attn, (H, W))\n",
        "                attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "            if self.se_flag[0] > 0:\n",
        "                hp = self.hyper1((tokens, attn))\n",
        "                out = self.act1((out, hp))\n",
        "            else:\n",
        "                out = self.act1(out)\n",
        "\n",
        "            out = self.conv2(out)\n",
        "            out = self.act2(out)\n",
        "\n",
        "            # step 4: conv3 + conv 4\n",
        "            out_cp = out\n",
        "            out = self.conv3(out)\n",
        "            if self.se_flag[2] > 0:\n",
        "                hp = self.hyper3((tokens, attn))\n",
        "                out = self.act3((out, hp))\n",
        "            else:\n",
        "                out = self.act3(out)\n",
        "\n",
        "            out = self.conv4(out)\n",
        "            if self.se_flag[3] > 0:\n",
        "                hp = self.hyper4((tokens, attn))\n",
        "                out = self.act4((out, hp))\n",
        "            else:\n",
        "                out = self.act4(out)\n",
        "\n",
        "            out = self.drop_path(out) + out_cp\n",
        "\n",
        "            # step 3: global to local\n",
        "            out = self.global_local((out, tokens))\n",
        "\n",
        "        if self.identity:\n",
        "            out = out + features\n",
        "\n",
        "        return (out, tokens)\n",
        "\n",
        "\n",
        "class DnaBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp,\n",
        "        oup,\n",
        "        stride,\n",
        "        exp_ratios, #(e1, e2)\n",
        "        kernel_size=(3,3),\n",
        "        dw_conv='dw',\n",
        "        group_num=1,\n",
        "        se_flag=[2,0,2,0],\n",
        "        hyper_token_id=0,\n",
        "        hyper_reduction_ratio=4,\n",
        "        token_dim=128,\n",
        "        token_num=6,\n",
        "        inp_res=49,\n",
        "        gbr_type='mlp',\n",
        "        gbr_dynamic=[False, False, False],\n",
        "        gbr_ffn=False,\n",
        "        gbr_before_skip=False,\n",
        "        mlp_token_exp=4,\n",
        "        norm_pos='post',\n",
        "        drop_path_rate=0.,\n",
        "        cnn_drop_path_rate=0.,\n",
        "        attn_num_heads=2,\n",
        "        remove_proj_local=True,\n",
        "    ):\n",
        "        super(DnaBlock, self).__init__()\n",
        "\n",
        "        print(f'block: {inp_res}, cnn-drop {cnn_drop_path_rate:.4f}, mlp-drop {drop_path_rate:.4f}')\n",
        "        if isinstance(exp_ratios, tuple):\n",
        "            e1, e2 = exp_ratios\n",
        "        else:\n",
        "            e1, e2 = exp_ratios, 4\n",
        "        k1, k2 = kernel_size\n",
        "\n",
        "        self.stride = stride\n",
        "        self.hyper_token_id = hyper_token_id\n",
        "\n",
        "        self.gbr_before_skip = gbr_before_skip\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "        self.use_conv_alone = False\n",
        "        if e1 == 1 or e2 == 0:\n",
        "            self.use_conv_alone = True\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(inp, inp*e1, 3, stride, 1, groups=inp, bias=False),\n",
        "                nn.BatchNorm2d(inp*e1),\n",
        "                nn.ReLU6(inplace=True),\n",
        "                ChannelShuffle(inp) if group_num > 1 else nn.Sequential(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(inp*e1, oup, 1, 1, 0, groups=group_num, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            # conv (pw->dw->pw)\n",
        "            self.se_flag = se_flag\n",
        "            hidden_dim = round(inp * e1)\n",
        "\n",
        "            self.conv1 = nn.Sequential(\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, groups=group_num, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                ChannelShuffle(group_num) if group_num > 1 else nn.Sequential()\n",
        "            )\n",
        "\n",
        "            num_func = se_flag[0]\n",
        "            self.act1 = DyReLU(num_func=num_func, scale=2., serelu=True)\n",
        "            self.hyper1 = HyperFunc(\n",
        "                token_dim,\n",
        "                hidden_dim * num_func,\n",
        "                sel_token_id=hyper_token_id,\n",
        "                reduction_ratio=hyper_reduction_ratio\n",
        "            ) if se_flag[0] > 0 else nn.Sequential()\n",
        "\n",
        "\n",
        "            self.conv2 = nn.Sequential(\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, k1, stride, k1//2, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "            )\n",
        "            num_func = se_flag[2] # note here we used index 2 to be consistent with block2\n",
        "            self.act2 = DyReLU(num_func=num_func, scale=2., serelu=True)\n",
        "            self.hyper2 = HyperFunc(\n",
        "                token_dim,\n",
        "                hidden_dim * num_func,\n",
        "                sel_token_id=hyper_token_id,\n",
        "                reduction_ratio=hyper_reduction_ratio\n",
        "            ) if se_flag[2] > 0 else nn.Sequential()\n",
        "\n",
        "\n",
        "            self.conv3 = nn.Sequential(\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, groups=group_num, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "                ChannelShuffle(group_num) if group_num > 1 else nn.Sequential()\n",
        "            )\n",
        "            num_func = 1 if se_flag[3] == 1 else -1\n",
        "            self.act3 = DyReLU(num_func=num_func, scale=2.)\n",
        "            self.hyper3 = HyperFunc(\n",
        "                token_dim,\n",
        "                oup * num_func,\n",
        "                sel_token_id=hyper_token_id,\n",
        "                reduction_ratio=hyper_reduction_ratio\n",
        "            ) if se_flag[3] > 0 else nn.Sequential()\n",
        "\n",
        "\n",
        "            self.drop_path = DropPath(cnn_drop_path_rate)\n",
        "\n",
        "            # l2g, gb, g2l\n",
        "            self.local_global = Local2Global(\n",
        "                inp,\n",
        "                block_type = gbr_type,\n",
        "                token_dim=token_dim,\n",
        "                token_num=token_num,\n",
        "                inp_res=inp_res,\n",
        "                use_dynamic = gbr_dynamic[0],\n",
        "                norm_pos=norm_pos,\n",
        "                drop_path_rate=drop_path_rate,\n",
        "                attn_num_heads=attn_num_heads,\n",
        "                remove_proj_local=remove_proj_local,\n",
        "            )\n",
        "\n",
        "            self.global_block = GlobalBlock(\n",
        "                block_type=gbr_type,\n",
        "                token_dim=token_dim,\n",
        "                token_num=token_num,\n",
        "                mlp_token_exp=mlp_token_exp,\n",
        "                use_dynamic = gbr_dynamic[1],\n",
        "                use_ffn=gbr_ffn,\n",
        "                norm_pos=norm_pos,\n",
        "                drop_path_rate=drop_path_rate\n",
        "            )\n",
        "\n",
        "            oup_res = inp_res // (stride * stride)\n",
        "\n",
        "            self.global_local = Global2Local(\n",
        "                oup,\n",
        "                oup_res,\n",
        "                block_type=gbr_type,\n",
        "                token_dim=token_dim,\n",
        "                token_num=token_num,\n",
        "                use_dynamic = gbr_dynamic[2],\n",
        "                drop_path_rate=drop_path_rate,\n",
        "                attn_num_heads=attn_num_heads,\n",
        "                remove_proj_local=remove_proj_local,\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features, tokens = x\n",
        "        if self.use_conv_alone:\n",
        "            out = self.conv(features)\n",
        "            if self.identity:\n",
        "                out = self.drop_path(out) + features\n",
        "\n",
        "        else:\n",
        "            # step 1: local to global\n",
        "            tokens, attn = self.local_global((features, tokens))\n",
        "            tokens = self.global_block(tokens)\n",
        "\n",
        "            # step 2: conv1 + conv2 + conv3\n",
        "            out = self.conv1(features)\n",
        "\n",
        "            # process attn: mean, downsample if stride > 1, and softmax\n",
        "            if self.hyper_token_id == -1:\n",
        "                attn = attn.mean(dim=1) # bs x T x H x W\n",
        "                if self.stride > 1:\n",
        "                    _, _, H, W = out.shape\n",
        "                    attn = F.adaptive_avg_pool2d(attn, (H, W))\n",
        "                attn = torch.softmax(attn, dim=1)\n",
        "\n",
        "            if self.se_flag[0] > 0:\n",
        "                hp = self.hyper1((tokens, attn))\n",
        "                out = self.act1((out, hp))\n",
        "            else:\n",
        "                out = self.act1(out)\n",
        "\n",
        "            out = self.conv2(out)\n",
        "            if self.se_flag[2] > 0:\n",
        "                hp = self.hyper2((tokens, attn))\n",
        "                out = self.act2((out, hp))\n",
        "            else:\n",
        "                out = self.act2(out)\n",
        "\n",
        "            out = self.conv3(out)\n",
        "            if self.se_flag[3] > 0:\n",
        "                hp = self.hyper3((tokens, attn))\n",
        "                out = self.act3((out, hp))\n",
        "            else:\n",
        "                out = self.act3(out)\n",
        "\n",
        "            # step 3: global to local and skip\n",
        "            if self.gbr_before_skip == True:\n",
        "                out = self.global_local((out, tokens))\n",
        "                if self.identity:\n",
        "                    out = self.drop_path(out) + features\n",
        "            else:\n",
        "                if self.identity:\n",
        "                    out = self.drop_path(out) + features\n",
        "                out = self.global_local((out, tokens))\n",
        "\n",
        "        return (out, tokens)\n",
        "\n",
        "##########################################################################################################\n",
        "# classifier\n",
        "##########################################################################################################\n",
        "class MergeClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self, inp,\n",
        "        oup=1280,\n",
        "        ch_exp=6,\n",
        "        num_classes=1000,\n",
        "        drop_rate=0.,\n",
        "        drop_branch=[0.0, 0.0],\n",
        "        group_num=1,\n",
        "        token_dim=128,\n",
        "        cls_token_num=1,\n",
        "        last_act='relu',\n",
        "        hyper_token_id=0,\n",
        "        hyper_reduction_ratio=4\n",
        "    ):\n",
        "        super(MergeClassifier, self).__init__()\n",
        "\n",
        "        self.drop_branch=drop_branch\n",
        "        self.cls_token_num = cls_token_num\n",
        "\n",
        "        hidden_dim = inp * ch_exp\n",
        "        self.conv = nn.Sequential(\n",
        "            ChannelShuffle(group_num) if group_num > 1 else nn.Sequential(),\n",
        "            nn.Conv2d(inp, hidden_dim, 1, 1, 0, groups=group_num, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.last_act = last_act\n",
        "        num_func = 2 if last_act == 'dyrelu' else 0\n",
        "        self.act = DyReLU(num_func=num_func, scale=2.)\n",
        "\n",
        "        self.hyper = HyperFunc(\n",
        "            token_dim,\n",
        "            hidden_dim * num_func,\n",
        "            sel_token_id=hyper_token_id,\n",
        "            reduction_ratio=hyper_reduction_ratio\n",
        "        ) if last_act == 'dyrelu' else nn.Sequential()\n",
        "\n",
        "        self.avgpool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            h_swish()\n",
        "        )\n",
        "\n",
        "        if cls_token_num > 0:\n",
        "            cat_token_dim = token_dim * cls_token_num\n",
        "        elif cls_token_num == 0:\n",
        "            cat_token_dim = token_dim\n",
        "        else:\n",
        "            cat_token_dim = 0\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim + cat_token_dim, oup),\n",
        "            nn.BatchNorm1d(oup),\n",
        "            h_swish()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "           nn.Dropout(drop_rate),\n",
        "           nn.Linear(oup, num_classes)\n",
        "       )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features, tokens = x\n",
        "\n",
        "        x = self.conv(features)\n",
        "\n",
        "        if self.last_act == 'dyrelu':\n",
        "            hp = self.hyper(tokens)\n",
        "            x = self.act((x, hp))\n",
        "        else:\n",
        "            x = self.act(x)\n",
        "\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        ps = [x]\n",
        "\n",
        "        if self.cls_token_num == 0:\n",
        "            avg_token = torch.mean(F.relu6(tokens), dim=0)\n",
        "            ps.append(avg_token)\n",
        "        elif self.cls_token_num < 0:\n",
        "            pass\n",
        "        else:\n",
        "            for i in range(self.cls_token_num):\n",
        "                ps.append(tokens[i])\n",
        "\n",
        "        # drop branch\n",
        "        if self.training and self.drop_branch[0] + self.drop_branch[1] > 1e-8:\n",
        "            rd = torch.rand((x.shape[0], 1), dtype=x.dtype, device=x.device)\n",
        "            keep_local = 1 - self.drop_branch[0]\n",
        "            keep_global = 1 - self.drop_branch[1]\n",
        "            rd_local = (keep_local + rd).floor_()\n",
        "            rd_global = -((rd - keep_global).floor_())\n",
        "            ps[0] = ps[0].div(keep_local) * rd_local\n",
        "            ps[1] = ps[1].div(keep_global) * rd_global\n",
        "\n",
        "        x = torch.cat(ps, dim=1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOQnfAo9ptCr"
      },
      "source": [
        "HELPERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAhewG3tpm9r"
      },
      "outputs": [],
      "source": [
        "\"\"\" Model creation / weight loading / state_dict helpers\n",
        "\n",
        "Hacked together by / Copyright 2020 Ross Wightman\n",
        "\"\"\"\n",
        "import logging\n",
        "import os\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "from copy import deepcopy\n",
        "from typing import Any, Callable, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "from timm.models.features import FeatureListNet, FeatureDictNet, FeatureHookNet\n",
        "from timm.models.hub import has_hf_hub, download_cached_file, load_state_dict_from_hf\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from timm.models.layers import Conv2dSame, Linear\n",
        "\n",
        "\n",
        "_logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def load_state_dict(checkpoint_path, use_ema=False):\n",
        "    if checkpoint_path and os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        state_dict_key = 'state_dict'\n",
        "        if isinstance(checkpoint, dict):\n",
        "            if use_ema and 'state_dict_ema' in checkpoint:\n",
        "                state_dict_key = 'state_dict_ema'\n",
        "        if state_dict_key and state_dict_key in checkpoint:\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in checkpoint[state_dict_key].items():\n",
        "                # strip `module.` prefix\n",
        "                name = k[7:] if k.startswith('module') else k\n",
        "                new_state_dict[name] = v\n",
        "            state_dict = new_state_dict\n",
        "        else:\n",
        "            state_dict = checkpoint\n",
        "        _logger.info(\"Loaded {} from checkpoint '{}'\".format(state_dict_key, checkpoint_path))\n",
        "        return state_dict\n",
        "    else:\n",
        "        _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n",
        "        raise FileNotFoundError()\n",
        "\n",
        "\n",
        "def load_checkpoint(model, checkpoint_path, use_ema=False, strict=True):\n",
        "    state_dict = load_state_dict(checkpoint_path, use_ema)\n",
        "    model.load_state_dict(state_dict, strict=strict)\n",
        "\n",
        "\n",
        "def resume_checkpoint(model, checkpoint_path, optimizer=None, loss_scaler=None, log_info=True):\n",
        "    resume_epoch = None\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
        "            if log_info:\n",
        "                _logger.info('Restoring model state from checkpoint...')\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in checkpoint['state_dict'].items():\n",
        "                name = k[7:] if k.startswith('module') else k\n",
        "                new_state_dict[name] = v\n",
        "            model.load_state_dict(new_state_dict)\n",
        "\n",
        "            if optimizer is not None and 'optimizer' in checkpoint:\n",
        "                if log_info:\n",
        "                    _logger.info('Restoring optimizer state from checkpoint...')\n",
        "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "            if loss_scaler is not None and loss_scaler.state_dict_key in checkpoint:\n",
        "                if log_info:\n",
        "                    _logger.info('Restoring AMP loss scaler state from checkpoint...')\n",
        "                loss_scaler.load_state_dict(checkpoint[loss_scaler.state_dict_key])\n",
        "\n",
        "            if 'epoch' in checkpoint:\n",
        "                resume_epoch = checkpoint['epoch']\n",
        "                if 'version' in checkpoint and checkpoint['version'] > 1:\n",
        "                    resume_epoch += 1  # start at the next epoch, old checkpoints incremented before save\n",
        "\n",
        "            if log_info:\n",
        "                _logger.info(\"Loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, checkpoint['epoch']))\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint)\n",
        "            if log_info:\n",
        "                _logger.info(\"Loaded checkpoint '{}'\".format(checkpoint_path))\n",
        "        return resume_epoch\n",
        "    else:\n",
        "        _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n",
        "        raise FileNotFoundError()\n",
        "\n",
        "\n",
        "def load_custom_pretrained(model, default_cfg=None, load_fn=None, progress=False, check_hash=False):\n",
        "    r\"\"\"Loads a custom (read non .pth) weight file\n",
        "\n",
        "    Downloads checkpoint file into cache-dir like torch.hub based loaders, but calls\n",
        "    a passed in custom load fun, or the `load_pretrained` model member fn.\n",
        "\n",
        "    If the object is already present in `model_dir`, it's deserialized and returned.\n",
        "    The default value of `model_dir` is ``<hub_dir>/checkpoints`` where\n",
        "    `hub_dir` is the directory returned by :func:`~torch.hub.get_dir`.\n",
        "\n",
        "    Args:\n",
        "        model: The instantiated model to load weights into\n",
        "        default_cfg (dict): Default pretrained model cfg\n",
        "        load_fn: An external stand alone fn that loads weights into provided model, otherwise a fn named\n",
        "            'laod_pretrained' on the model will be called if it exists\n",
        "        progress (bool, optional): whether or not to display a progress bar to stderr. Default: False\n",
        "        check_hash(bool, optional): If True, the filename part of the URL should follow the naming convention\n",
        "            ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more\n",
        "            digits of the SHA256 hash of the contents of the file. The hash is used to\n",
        "            ensure unique names and to verify the contents of the file. Default: False\n",
        "    \"\"\"\n",
        "    default_cfg = default_cfg or getattr(model, 'default_cfg', None) or {}\n",
        "    pretrained_url = default_cfg.get('url', None)\n",
        "    if not pretrained_url:\n",
        "        _logger.warning(\"No pretrained weights exist for this model. Using random initialization.\")\n",
        "        return\n",
        "    cached_file = download_cached_file(default_cfg['url'], check_hash=check_hash, progress=progress)\n",
        "\n",
        "    if load_fn is not None:\n",
        "        load_fn(model, cached_file)\n",
        "    elif hasattr(model, 'load_pretrained'):\n",
        "        model.load_pretrained(cached_file)\n",
        "    else:\n",
        "        _logger.warning(\"Valid function to load pretrained weights is not available, using random initialization.\")\n",
        "\n",
        "\n",
        "def adapt_input_conv(in_chans, conv_weight):\n",
        "    conv_type = conv_weight.dtype\n",
        "    conv_weight = conv_weight.float()  # Some weights are in torch.half, ensure it's float for sum on CPU\n",
        "    O, I, J, K = conv_weight.shape\n",
        "    if in_chans == 1:\n",
        "        if I > 3:\n",
        "            assert conv_weight.shape[1] % 3 == 0\n",
        "            # For models with space2depth stems\n",
        "            conv_weight = conv_weight.reshape(O, I // 3, 3, J, K)\n",
        "            conv_weight = conv_weight.sum(dim=2, keepdim=False)\n",
        "        else:\n",
        "            conv_weight = conv_weight.sum(dim=1, keepdim=True)\n",
        "    elif in_chans != 3:\n",
        "        if I != 3:\n",
        "            raise NotImplementedError('Weight format not supported by conversion.')\n",
        "        else:\n",
        "            # NOTE this strategy should be better than random init, but there could be other combinations of\n",
        "            # the original RGB input layer weights that'd work better for specific cases.\n",
        "            repeat = int(math.ceil(in_chans / 3))\n",
        "            conv_weight = conv_weight.repeat(1, repeat, 1, 1)[:, :in_chans, :, :]\n",
        "            conv_weight *= (3 / float(in_chans))\n",
        "    conv_weight = conv_weight.to(conv_type)\n",
        "    return conv_weight\n",
        "\n",
        "\n",
        "def load_pretrained(model, default_cfg=None, num_classes=1000, in_chans=3, filter_fn=None, strict=True, progress=False):\n",
        "    \"\"\" Load pretrained checkpoint\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module) : PyTorch model module\n",
        "        default_cfg (Optional[Dict]): default configuration for pretrained weights / target dataset\n",
        "        num_classes (int): num_classes for model\n",
        "        in_chans (int): in_chans for model\n",
        "        filter_fn (Optional[Callable]): state_dict filter fn for load (takes state_dict, model as args)\n",
        "        strict (bool): strict load of checkpoint\n",
        "        progress (bool): enable progress bar for weight download\n",
        "\n",
        "    \"\"\"\n",
        "    default_cfg = default_cfg or getattr(model, 'default_cfg', None) or {}\n",
        "    pretrained_url = default_cfg.get('url', None)\n",
        "    hf_hub_id = default_cfg.get('hf_hub', None)\n",
        "    if not pretrained_url and not hf_hub_id:\n",
        "        _logger.warning(\"No pretrained weights exist for this model. Using random initialization.\")\n",
        "        return\n",
        "    if hf_hub_id and has_hf_hub(necessary=not pretrained_url):\n",
        "        _logger.info(f'Loading pretrained weights from Hugging Face hub ({hf_hub_id})')\n",
        "        state_dict = load_state_dict_from_hf(hf_hub_id)\n",
        "    else:\n",
        "        _logger.info(f'Loading pretrained weights from url ({pretrained_url})')\n",
        "        state_dict = load_state_dict_from_url(pretrained_url, progress=progress, map_location='cpu')\n",
        "    if filter_fn is not None:\n",
        "        # for backwards compat with filter fn that take one arg, try one first, the two\n",
        "        try:\n",
        "            state_dict = filter_fn(state_dict)\n",
        "        except TypeError:\n",
        "            state_dict = filter_fn(state_dict, model)\n",
        "\n",
        "    input_convs = default_cfg.get('first_conv', None)\n",
        "    if input_convs is not None and in_chans != 3:\n",
        "        if isinstance(input_convs, str):\n",
        "            input_convs = (input_convs,)\n",
        "        for input_conv_name in input_convs:\n",
        "            weight_name = input_conv_name + '.weight'\n",
        "            try:\n",
        "                state_dict[weight_name] = adapt_input_conv(in_chans, state_dict[weight_name])\n",
        "                _logger.info(\n",
        "                    f'Converted input conv {input_conv_name} pretrained weights from 3 to {in_chans} channel(s)')\n",
        "            except NotImplementedError as e:\n",
        "                del state_dict[weight_name]\n",
        "                strict = False\n",
        "                _logger.warning(\n",
        "                    f'Unable to convert pretrained {input_conv_name} weights, using random init for this layer.')\n",
        "\n",
        "    classifiers = default_cfg.get('classifier', None)\n",
        "    label_offset = default_cfg.get('label_offset', 0)\n",
        "    if classifiers is not None:\n",
        "        if isinstance(classifiers, str):\n",
        "            classifiers = (classifiers,)\n",
        "        if num_classes != default_cfg['num_classes']:\n",
        "            for classifier_name in classifiers:\n",
        "                # completely discard fully connected if model num_classes doesn't match pretrained weights\n",
        "                del state_dict[classifier_name + '.weight']\n",
        "                del state_dict[classifier_name + '.bias']\n",
        "            strict = False\n",
        "        elif label_offset > 0:\n",
        "            for classifier_name in classifiers:\n",
        "                # special case for pretrained weights with an extra background class in pretrained weights\n",
        "                classifier_weight = state_dict[classifier_name + '.weight']\n",
        "                state_dict[classifier_name + '.weight'] = classifier_weight[label_offset:]\n",
        "                classifier_bias = state_dict[classifier_name + '.bias']\n",
        "                state_dict[classifier_name + '.bias'] = classifier_bias[label_offset:]\n",
        "\n",
        "    model.load_state_dict(state_dict, strict=strict)\n",
        "\n",
        "\n",
        "def extract_layer(model, layer):\n",
        "    layer = layer.split('.')\n",
        "    module = model\n",
        "    if hasattr(model, 'module') and layer[0] != 'module':\n",
        "        module = model.module\n",
        "    if not hasattr(model, 'module') and layer[0] == 'module':\n",
        "        layer = layer[1:]\n",
        "    for l in layer:\n",
        "        if hasattr(module, l):\n",
        "            if not l.isdigit():\n",
        "                module = getattr(module, l)\n",
        "            else:\n",
        "                module = module[int(l)]\n",
        "        else:\n",
        "            return module\n",
        "    return module\n",
        "\n",
        "\n",
        "def set_layer(model, layer, val):\n",
        "    layer = layer.split('.')\n",
        "    module = model\n",
        "    if hasattr(model, 'module') and layer[0] != 'module':\n",
        "        module = model.module\n",
        "    lst_index = 0\n",
        "    module2 = module\n",
        "    for l in layer:\n",
        "        if hasattr(module2, l):\n",
        "            if not l.isdigit():\n",
        "                module2 = getattr(module2, l)\n",
        "            else:\n",
        "                module2 = module2[int(l)]\n",
        "            lst_index += 1\n",
        "    lst_index -= 1\n",
        "    for l in layer[:lst_index]:\n",
        "        if not l.isdigit():\n",
        "            module = getattr(module, l)\n",
        "        else:\n",
        "            module = module[int(l)]\n",
        "    l = layer[lst_index]\n",
        "    setattr(module, l, val)\n",
        "\n",
        "\n",
        "def adapt_model_from_string(parent_module, model_string):\n",
        "    separator = '***'\n",
        "    state_dict = {}\n",
        "    lst_shape = model_string.split(separator)\n",
        "    for k in lst_shape:\n",
        "        k = k.split(':')\n",
        "        key = k[0]\n",
        "        shape = k[1][1:-1].split(',')\n",
        "        if shape[0] != '':\n",
        "            state_dict[key] = [int(i) for i in shape]\n",
        "\n",
        "    new_module = deepcopy(parent_module)\n",
        "    for n, m in parent_module.named_modules():\n",
        "        old_module = extract_layer(parent_module, n)\n",
        "        if isinstance(old_module, nn.Conv2d) or isinstance(old_module, Conv2dSame):\n",
        "            if isinstance(old_module, Conv2dSame):\n",
        "                conv = Conv2dSame\n",
        "            else:\n",
        "                conv = nn.Conv2d\n",
        "            s = state_dict[n + '.weight']\n",
        "            in_channels = s[1]\n",
        "            out_channels = s[0]\n",
        "            g = 1\n",
        "            if old_module.groups > 1:\n",
        "                in_channels = out_channels\n",
        "                g = in_channels\n",
        "            new_conv = conv(\n",
        "                in_channels=in_channels, out_channels=out_channels, kernel_size=old_module.kernel_size,\n",
        "                bias=old_module.bias is not None, padding=old_module.padding, dilation=old_module.dilation,\n",
        "                groups=g, stride=old_module.stride)\n",
        "            set_layer(new_module, n, new_conv)\n",
        "        if isinstance(old_module, nn.BatchNorm2d):\n",
        "            new_bn = nn.BatchNorm2d(\n",
        "                num_features=state_dict[n + '.weight'][0], eps=old_module.eps, momentum=old_module.momentum,\n",
        "                affine=old_module.affine, track_running_stats=True)\n",
        "            set_layer(new_module, n, new_bn)\n",
        "        if isinstance(old_module, nn.Linear):\n",
        "            # FIXME extra checks to ensure this is actually the FC classifier layer and not a diff Linear layer?\n",
        "            num_features = state_dict[n + '.weight'][1]\n",
        "            new_fc = Linear(\n",
        "                in_features=num_features, out_features=old_module.out_features, bias=old_module.bias is not None)\n",
        "            set_layer(new_module, n, new_fc)\n",
        "            if hasattr(new_module, 'num_features'):\n",
        "                new_module.num_features = num_features\n",
        "    new_module.eval()\n",
        "    parent_module.eval()\n",
        "\n",
        "    return new_module\n",
        "\n",
        "\n",
        "def adapt_model_from_file(parent_module, model_variant):\n",
        "    adapt_file = os.path.join(os.path.dirname(__file__), 'pruned', model_variant + '.txt')\n",
        "    with open(adapt_file, 'r') as f:\n",
        "        return adapt_model_from_string(parent_module, f.read().strip())\n",
        "\n",
        "\n",
        "def default_cfg_for_features(default_cfg):\n",
        "    default_cfg = deepcopy(default_cfg)\n",
        "    # remove default pretrained cfg fields that don't have much relevance for feature backbone\n",
        "    to_remove = ('num_classes', 'crop_pct', 'classifier', 'global_pool')  # add default final pool size?\n",
        "    for tr in to_remove:\n",
        "        default_cfg.pop(tr, None)\n",
        "    return default_cfg\n",
        "\n",
        "\n",
        "def overlay_external_default_cfg(default_cfg, kwargs):\n",
        "    \"\"\" Overlay 'external_default_cfg' in kwargs on top of default_cfg arg.\n",
        "    \"\"\"\n",
        "    external_default_cfg = kwargs.pop('external_default_cfg', None)\n",
        "    if external_default_cfg:\n",
        "        default_cfg.pop('url', None)  # url should come from external cfg\n",
        "        default_cfg.pop('hf_hub', None)  # hf hub id should come from external cfg\n",
        "        default_cfg.update(external_default_cfg)\n",
        "\n",
        "\n",
        "def set_default_kwargs(kwargs, names, default_cfg):\n",
        "    for n in names:\n",
        "        # for legacy reasons, model __init__args uses img_size + in_chans as separate args while\n",
        "        # default_cfg has one input_size=(C, H ,W) entry\n",
        "        if n == 'img_size':\n",
        "            input_size = default_cfg.get('input_size', None)\n",
        "            if input_size is not None:\n",
        "                assert len(input_size) == 3\n",
        "                kwargs.setdefault(n, input_size[-2:])\n",
        "        elif n == 'in_chans':\n",
        "            input_size = default_cfg.get('input_size', None)\n",
        "            if input_size is not None:\n",
        "                assert len(input_size) == 3\n",
        "                kwargs.setdefault(n, input_size[0])\n",
        "        else:\n",
        "            default_val = default_cfg.get(n, None)\n",
        "            if default_val is not None:\n",
        "                kwargs.setdefault(n, default_cfg[n])\n",
        "\n",
        "\n",
        "def filter_kwargs(kwargs, names):\n",
        "    if not kwargs or not names:\n",
        "        return\n",
        "    for n in names:\n",
        "        kwargs.pop(n, None)\n",
        "\n",
        "\n",
        "def update_default_cfg_and_kwargs(default_cfg, kwargs, kwargs_filter):\n",
        "    \"\"\" Update the default_cfg and kwargs before passing to model\n",
        "\n",
        "    FIXME this sequence of overlay default_cfg, set default kwargs, filter kwargs\n",
        "    could/should be replaced by an improved configuration mechanism\n",
        "\n",
        "    Args:\n",
        "        default_cfg: input default_cfg (updated in-place)\n",
        "        kwargs: keyword args passed to model build fn (updated in-place)\n",
        "        kwargs_filter: keyword arg keys that must be removed before model __init__\n",
        "    \"\"\"\n",
        "    # Overlay default cfg values from `external_default_cfg` if it exists in kwargs\n",
        "    overlay_external_default_cfg(default_cfg, kwargs)\n",
        "    # Set model __init__ args that can be determined by default_cfg (if not already passed as kwargs)\n",
        "    default_kwarg_names = ('num_classes', 'global_pool', 'in_chans')\n",
        "    if default_cfg.get('fixed_input_size', False):\n",
        "        # if fixed_input_size exists and is True, model takes an img_size arg that fixes its input size\n",
        "        default_kwarg_names += ('img_size',)\n",
        "    set_default_kwargs(kwargs, names=default_kwarg_names, default_cfg=default_cfg)\n",
        "    # Filter keyword args for task specific model variants (some 'features only' models, etc.)\n",
        "    filter_kwargs(kwargs, names=kwargs_filter)\n",
        "\n",
        "\n",
        "def build_model_with_cfg(\n",
        "        model_cls: Callable,\n",
        "        variant: str,\n",
        "        pretrained: bool,\n",
        "        default_cfg: dict,\n",
        "        model_cfg: Optional[Any] = None,\n",
        "        feature_cfg: Optional[dict] = None,\n",
        "        pretrained_strict: bool = True,\n",
        "        pretrained_filter_fn: Optional[Callable] = None,\n",
        "        pretrained_custom_load: bool = False,\n",
        "        kwargs_filter: Optional[Tuple[str]] = None,\n",
        "        **kwargs):\n",
        "    \"\"\" Build model with specified default_cfg and optional model_cfg\n",
        "\n",
        "    This helper fn aids in the construction of a model including:\n",
        "      * handling default_cfg and associated pretained weight loading\n",
        "      * passing through optional model_cfg for models with config based arch spec\n",
        "      * features_only model adaptation\n",
        "      * pruning config / model adaptation\n",
        "\n",
        "    Args:\n",
        "        model_cls (nn.Module): model class\n",
        "        variant (str): model variant name\n",
        "        pretrained (bool): load pretrained weights\n",
        "        default_cfg (dict): model's default pretrained/task config\n",
        "        model_cfg (Optional[Dict]): model's architecture config\n",
        "        feature_cfg (Optional[Dict]: feature extraction adapter config\n",
        "        pretrained_strict (bool): load pretrained weights strictly\n",
        "        pretrained_filter_fn (Optional[Callable]): filter callable for pretrained weights\n",
        "        pretrained_custom_load (bool): use custom load fn, to load numpy or other non PyTorch weights\n",
        "        kwargs_filter (Optional[Tuple]): kwargs to filter before passing to model\n",
        "        **kwargs: model args passed through to model __init__\n",
        "    \"\"\"\n",
        "    pruned = kwargs.pop('pruned', False)\n",
        "    features = False\n",
        "    feature_cfg = feature_cfg or {}\n",
        "    default_cfg = deepcopy(default_cfg) if default_cfg else {}\n",
        "    update_default_cfg_and_kwargs(default_cfg, kwargs, kwargs_filter)\n",
        "    default_cfg.setdefault('architecture', variant)\n",
        "\n",
        "    # Setup for feature extraction wrapper done at end of this fn\n",
        "    if kwargs.pop('features_only', False):\n",
        "        features = True\n",
        "        feature_cfg.setdefault('out_indices', (0, 1, 2, 3, 4))\n",
        "        if 'out_indices' in kwargs:\n",
        "            feature_cfg['out_indices'] = kwargs.pop('out_indices')\n",
        "\n",
        "    # Build the model\n",
        "    model = model_cls(**kwargs) if model_cfg is None else model_cls(cfg=model_cfg, **kwargs)\n",
        "    model.default_cfg = default_cfg\n",
        "\n",
        "    if pruned:\n",
        "        model = adapt_model_from_file(model, variant)\n",
        "\n",
        "    # For classification models, check class attr, then kwargs, then default to 1k, otherwise 0 for feats\n",
        "    num_classes_pretrained = 0 if features else getattr(model, 'num_classes', kwargs.get('num_classes', 1000))\n",
        "    if pretrained:\n",
        "        if pretrained_custom_load:\n",
        "            load_custom_pretrained(model)\n",
        "        else:\n",
        "            load_pretrained(\n",
        "                model,\n",
        "                num_classes=num_classes_pretrained,\n",
        "                in_chans=kwargs.get('in_chans', 3),\n",
        "                filter_fn=pretrained_filter_fn,\n",
        "                strict=pretrained_strict)\n",
        "\n",
        "    # Wrap the model in a feature extraction module if enabled\n",
        "    if features:\n",
        "        feature_cls = FeatureListNet\n",
        "        if 'feature_cls' in feature_cfg:\n",
        "            feature_cls = feature_cfg.pop('feature_cls')\n",
        "            if isinstance(feature_cls, str):\n",
        "                feature_cls = feature_cls.lower()\n",
        "                if 'hook' in feature_cls:\n",
        "                    feature_cls = FeatureHookNet\n",
        "                else:\n",
        "                    assert False, f'Unknown feature class {feature_cls}'\n",
        "        model = feature_cls(model, **feature_cfg)\n",
        "        model.default_cfg = default_cfg_for_features(default_cfg)  # add back default_cfg\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_parameters(model, exclude_head=False):\n",
        "    if exclude_head:\n",
        "        # FIXME this a bit of a quick and dirty hack to skip classifier head params based on ordering\n",
        "        return [p for p in model.parameters()][:-2]\n",
        "    else:\n",
        "        return model.parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSzLJnc4p7hv"
      },
      "source": [
        "REGISTRY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZjlG78sp79H"
      },
      "outputs": [],
      "source": [
        "\"\"\" Model Registry\n",
        "Hacked together by / Copyright 2020 Ross Wightman\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import re\n",
        "import fnmatch\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "\n",
        "__all__ = ['list_models', 'is_model', 'model_entrypoint', 'list_modules', 'is_model_in_modules',\n",
        "           'is_model_default_key', 'has_model_default_key', 'get_model_default_value', 'is_model_pretrained']\n",
        "\n",
        "_module_to_models = defaultdict(set)  # dict of sets to check membership of model in module\n",
        "_model_to_module = {}  # mapping of model names to module names\n",
        "_model_entrypoints = {}  # mapping of model names to entrypoint fns\n",
        "_model_has_pretrained = set()  # set of model names that have pretrained weight url present\n",
        "_model_default_cfgs = dict()  # central repo for model default_cfgs\n",
        "\n",
        "\n",
        "def register_model(fn):\n",
        "    # lookup containing module\n",
        "    mod = sys.modules[fn.__module__]\n",
        "    module_name_split = fn.__module__.split('.')\n",
        "    module_name = module_name_split[-1] if len(module_name_split) else ''\n",
        "\n",
        "    # add model to __all__ in module\n",
        "    model_name = fn.__name__\n",
        "    if hasattr(mod, '__all__'):\n",
        "        mod.__all__.append(model_name)\n",
        "    else:\n",
        "        mod.__all__ = [model_name]\n",
        "\n",
        "    # add entries to registry dict/sets\n",
        "    _model_entrypoints[model_name] = fn\n",
        "    _model_to_module[model_name] = module_name\n",
        "    _module_to_models[module_name].add(model_name)\n",
        "    has_pretrained = False  # check if model has a pretrained url to allow filtering on this\n",
        "    if hasattr(mod, 'default_cfgs') and model_name in mod.default_cfgs:\n",
        "        # this will catch all models that have entrypoint matching cfg key, but miss any aliasing\n",
        "        # entrypoints or non-matching combos\n",
        "        has_pretrained = 'url' in mod.default_cfgs[model_name] and 'http' in mod.default_cfgs[model_name]['url']\n",
        "        _model_default_cfgs[model_name] = deepcopy(mod.default_cfgs[model_name])\n",
        "    if has_pretrained:\n",
        "        _model_has_pretrained.add(model_name)\n",
        "    return fn\n",
        "\n",
        "\n",
        "def _natural_key(string_):\n",
        "    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_.lower())]\n",
        "\n",
        "\n",
        "def list_models(filter='', module='', pretrained=False, exclude_filters='', name_matches_cfg=False):\n",
        "    \"\"\" Return list of available model names, sorted alphabetically\n",
        "\n",
        "    Args:\n",
        "        filter (str) - Wildcard filter string that works with fnmatch\n",
        "        module (str) - Limit model selection to a specific sub-module (ie 'gen_efficientnet')\n",
        "        pretrained (bool) - Include only models with pretrained weights if True\n",
        "        exclude_filters (str or list[str]) - Wildcard filters to exclude models after including them with filter\n",
        "        name_matches_cfg (bool) - Include only models w/ model_name matching default_cfg name (excludes some aliases)\n",
        "\n",
        "    Example:\n",
        "        model_list('gluon_resnet*') -- returns all models starting with 'gluon_resnet'\n",
        "        model_list('*resnext*, 'resnet') -- returns all models with 'resnext' in 'resnet' module\n",
        "    \"\"\"\n",
        "    if module:\n",
        "        models = list(_module_to_models[module])\n",
        "    else:\n",
        "        models = _model_entrypoints.keys()\n",
        "    if filter:\n",
        "        models = fnmatch.filter(models, filter)  # include these models\n",
        "    if exclude_filters:\n",
        "        if not isinstance(exclude_filters, (tuple, list)):\n",
        "            exclude_filters = [exclude_filters]\n",
        "        for xf in exclude_filters:\n",
        "            exclude_models = fnmatch.filter(models, xf)  # exclude these models\n",
        "            if len(exclude_models):\n",
        "                models = set(models).difference(exclude_models)\n",
        "    if pretrained:\n",
        "        models = _model_has_pretrained.intersection(models)\n",
        "    if name_matches_cfg:\n",
        "        models = set(_model_default_cfgs).intersection(models)\n",
        "    return list(sorted(models, key=_natural_key))\n",
        "\n",
        "\n",
        "def is_model(model_name):\n",
        "    \"\"\" Check if a model name exists\n",
        "    \"\"\"\n",
        "    return model_name in _model_entrypoints\n",
        "\n",
        "\n",
        "def model_entrypoint(model_name):\n",
        "    \"\"\"Fetch a model entrypoint for specified model name\n",
        "    \"\"\"\n",
        "    return _model_entrypoints[model_name]\n",
        "\n",
        "\n",
        "def list_modules():\n",
        "    \"\"\" Return list of module names that contain models / model entrypoints\n",
        "    \"\"\"\n",
        "    modules = _module_to_models.keys()\n",
        "    return list(sorted(modules))\n",
        "\n",
        "\n",
        "def is_model_in_modules(model_name, module_names):\n",
        "    \"\"\"Check if a model exists within a subset of modules\n",
        "    Args:\n",
        "        model_name (str) - name of model to check\n",
        "        module_names (tuple, list, set) - names of modules to search in\n",
        "    \"\"\"\n",
        "    assert isinstance(module_names, (tuple, list, set))\n",
        "    return any(model_name in _module_to_models[n] for n in module_names)\n",
        "\n",
        "\n",
        "def has_model_default_key(model_name, cfg_key):\n",
        "    \"\"\" Query model default_cfgs for existence of a specific key.\n",
        "    \"\"\"\n",
        "    if model_name in _model_default_cfgs and cfg_key in _model_default_cfgs[model_name]:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def is_model_default_key(model_name, cfg_key):\n",
        "    \"\"\" Return truthy value for specified model default_cfg key, False if does not exist.\n",
        "    \"\"\"\n",
        "    if model_name in _model_default_cfgs and _model_default_cfgs[model_name].get(cfg_key, False):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_model_default_value(model_name, cfg_key):\n",
        "    \"\"\" Get a specific model default_cfg value by key. None if it doesn't exist.\n",
        "    \"\"\"\n",
        "    if model_name in _model_default_cfgs:\n",
        "        return _model_default_cfgs[model_name].get(cfg_key, None)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def is_model_pretrained(model_name):\n",
        "    return model_name in _model_has_pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFCJECMaqCsi"
      },
      "source": [
        "MOBILE - FORMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgo-7glIqEij"
      },
      "outputs": [],
      "source": [
        "\"\"\"Mobile-Former V1\n",
        "\n",
        "A PyTorch impl of MobileFromer-V1.\n",
        "\n",
        "Paper: Mobile-Former: Bridging MobileNet and Transformer (CVPR 2022)\n",
        "       https://arxiv.org/abs/2108.05895\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
        "\n",
        "__all__ = ['MobileFormer']\n",
        "\n",
        "def _cfg(url='', **kwargs):\n",
        "    return {\n",
        "        'url': url, 'num_classes': 1000, 'input_size': (3, 64, 64), 'pool_size': (1, 1),\n",
        "        'crop_pct': 0.875, 'interpolation': 'bilinear',\n",
        "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
        "        'first_conv': 'conv_stem', 'classifier': 'classifier',\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "default_cfgs = {\n",
        "    'default': _cfg(url=''),\n",
        "}\n",
        "\n",
        "class MobileFormer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block_args,\n",
        "        num_classes=1000,\n",
        "        img_size=64,\n",
        "        width_mult=1.,\n",
        "        in_chans=3,\n",
        "        stem_chs=16,\n",
        "        num_features=1280,\n",
        "        dw_conv='dw',\n",
        "        kernel_size=(3,3),\n",
        "        cnn_exp=(6,4),\n",
        "        group_num=1,\n",
        "        se_flag=[2,0,2,0],\n",
        "        hyper_token_id=0,\n",
        "        hyper_reduction_ratio=4,\n",
        "        token_dim=128,\n",
        "        token_num=6,\n",
        "        cls_token_num=1,\n",
        "        last_act='relu',\n",
        "        last_exp=6,\n",
        "        gbr_type='mlp',\n",
        "        gbr_dynamic=[False, False, False],\n",
        "        gbr_norm='post',\n",
        "        gbr_ffn=False,\n",
        "        gbr_before_skip=False,\n",
        "        gbr_drop=[0.0, 0.0],\n",
        "        mlp_token_exp=4,\n",
        "        drop_rate=0.,\n",
        "        drop_path_rate=0.,\n",
        "        cnn_drop_path_rate=0.,\n",
        "        attn_num_heads = 2,\n",
        "        remove_proj_local=True,\n",
        "        ):\n",
        "\n",
        "        super(MobileFormer, self).__init__()\n",
        "\n",
        "        cnn_drop_path_rate = drop_path_rate\n",
        "        mdiv = 8 if width_mult > 1.01 else 4\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        #global tokens\n",
        "        self.tokens = nn.Embedding(token_num, token_dim)\n",
        "\n",
        "        # Stem\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, stem_chs, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(stem_chs),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "        input_channel = stem_chs\n",
        "\n",
        "        # blocks\n",
        "        layer_num = len(block_args)\n",
        "        inp_res = img_size * img_size // 4\n",
        "        layers = []\n",
        "        for idx, val in enumerate(block_args):\n",
        "            b, t, c, n, s, t2 = val # t2 for block2 the second expand\n",
        "            block = eval(b)\n",
        "\n",
        "            t = (t, t2)\n",
        "            output_channel = _make_divisible(c * width_mult, mdiv) if idx > 0 else _make_divisible(c * width_mult, 4)\n",
        "\n",
        "            drop_path_prob = drop_path_rate * (idx+1) / layer_num\n",
        "            cnn_drop_path_prob = cnn_drop_path_rate * (idx+1) / layer_num\n",
        "\n",
        "            layers.append(block(\n",
        "                input_channel,\n",
        "                output_channel,\n",
        "                s,\n",
        "                t,\n",
        "                dw_conv=dw_conv,\n",
        "                kernel_size=kernel_size,\n",
        "                group_num=group_num,\n",
        "                se_flag=se_flag,\n",
        "                hyper_token_id=hyper_token_id,\n",
        "                hyper_reduction_ratio=hyper_reduction_ratio,\n",
        "                token_dim=token_dim,\n",
        "                token_num=token_num,\n",
        "                inp_res=inp_res,\n",
        "                gbr_type=gbr_type,\n",
        "                gbr_dynamic=gbr_dynamic,\n",
        "                gbr_ffn=gbr_ffn,\n",
        "                gbr_before_skip=gbr_before_skip,\n",
        "                mlp_token_exp=mlp_token_exp,\n",
        "                norm_pos=gbr_norm,\n",
        "                drop_path_rate=drop_path_prob,\n",
        "                cnn_drop_path_rate=cnn_drop_path_prob,\n",
        "                attn_num_heads=attn_num_heads,\n",
        "                remove_proj_local=remove_proj_local,\n",
        "            ))\n",
        "            input_channel = output_channel\n",
        "\n",
        "            if s == 2:\n",
        "                inp_res = inp_res // 4\n",
        "\n",
        "            for i in range(1, n):\n",
        "                layers.append(block(\n",
        "                    input_channel,\n",
        "                    output_channel,\n",
        "                    1,\n",
        "                    t,\n",
        "                    dw_conv=dw_conv,\n",
        "                    kernel_size=kernel_size,\n",
        "                    group_num=group_num,\n",
        "                    se_flag=se_flag,\n",
        "                    hyper_token_id=hyper_token_id,\n",
        "                    hyper_reduction_ratio=hyper_reduction_ratio,\n",
        "                    token_dim=token_dim,\n",
        "                    token_num=token_num,\n",
        "                    inp_res=inp_res,\n",
        "                    gbr_type=gbr_type,\n",
        "                    gbr_dynamic=gbr_dynamic,\n",
        "                    gbr_ffn=gbr_ffn,\n",
        "                    gbr_before_skip=gbr_before_skip,\n",
        "                    mlp_token_exp=mlp_token_exp,\n",
        "                    norm_pos=gbr_norm,\n",
        "                    drop_path_rate=drop_path_prob,\n",
        "                    cnn_drop_path_rate=cnn_drop_path_prob,\n",
        "                    attn_num_heads=attn_num_heads,\n",
        "                    remove_proj_local=remove_proj_local,\n",
        "                ))\n",
        "                input_channel = output_channel\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # last layer of local to global\n",
        "        self.local_global = Local2Global(\n",
        "            input_channel,\n",
        "            block_type = gbr_type,\n",
        "            token_dim=token_dim,\n",
        "            token_num=token_num,\n",
        "            inp_res=inp_res,\n",
        "            use_dynamic = gbr_dynamic[0],\n",
        "            norm_pos=gbr_norm,\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            attn_num_heads=attn_num_heads\n",
        "        )\n",
        "\n",
        "        # classifer\n",
        "        self.classifier = MergeClassifier(\n",
        "            input_channel,\n",
        "            oup=num_features,\n",
        "            ch_exp=last_exp,\n",
        "            num_classes=num_classes,\n",
        "            drop_rate=drop_rate,\n",
        "            drop_branch=gbr_drop,\n",
        "            group_num=group_num,\n",
        "            token_dim=token_dim,\n",
        "            cls_token_num=cls_token_num,\n",
        "            last_act = last_act,\n",
        "            hyper_token_id=hyper_token_id,\n",
        "            hyper_reduction_ratio=hyper_reduction_ratio\n",
        "        )\n",
        "\n",
        "        #initialize\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # setup tokens\n",
        "        bs, _, _, _ = x.shape\n",
        "        z = self.tokens.weight\n",
        "        tokens = z[None].repeat(bs, 1, 1).clone()\n",
        "        tokens = tokens.permute(1, 0, 2)\n",
        "\n",
        "        # stem -> features -> classifier\n",
        "        x = self.stem(x)\n",
        "        x, tokens = self.features((x, tokens))\n",
        "        tokens, attn = self.local_global((x, tokens))\n",
        "        y = self.classifier((x, tokens))\n",
        "\n",
        "        return y\n",
        "\n",
        "def _create_mobile_former(variant, pretrained=False, **kwargs):\n",
        "    model = build_model_with_cfg(\n",
        "        MobileFormer,\n",
        "        variant,\n",
        "        pretrained,\n",
        "        default_cfg=default_cfgs['default'],\n",
        "        **kwargs)\n",
        "    print(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "common_model_kwargs = dict(\n",
        "    cnn_drop_path_rate = 0.1,\n",
        "    dw_conv = 'dw',\n",
        "    kernel_size=(3, 3),\n",
        "    cnn_exp = (6, 4),\n",
        "    cls_token_num = 1,\n",
        "    hyper_token_id = 0,\n",
        "    hyper_reduction_ratio = 4,\n",
        "    attn_num_heads = 2,\n",
        "    gbr_norm = 'post',\n",
        "    mlp_token_exp = 4,\n",
        "    gbr_before_skip = False,\n",
        "    gbr_drop = [0., 0.],\n",
        "    last_act = 'relu',\n",
        "    remove_proj_local = True,\n",
        ")\n",
        "\n",
        "@register_model\n",
        "def mobile_former_508m(pretrained=False, **kwargs):\n",
        "\n",
        "    #stem = 24\n",
        "    dna_blocks = [\n",
        "        #b, e1,  c, n, s, e2\n",
        "        ['DnaBlock3', 2,  24, 1, 1, 0], #1 112x112 (1)\n",
        "        ['DnaBlock3', 6,  40, 1, 2, 4], #2 56x56 (2)\n",
        "        ['DnaBlock',  3,  40, 1, 1, 3], #3\n",
        "        ['DnaBlock3', 6,  72, 1, 2, 4], #4 28x28 (2)\n",
        "        ['DnaBlock',  3,  72, 1, 1, 3], #5\n",
        "        ['DnaBlock3', 6, 128, 1, 2, 4], #6 14x14 (4)\n",
        "        ['DnaBlock',  4, 128, 1, 1, 4], #7\n",
        "        ['DnaBlock',  6, 176, 1, 1, 4], #8\n",
        "        ['DnaBlock',  6, 176, 1, 1, 4], #9\n",
        "        ['DnaBlock3', 6, 240, 1, 2, 4], #10 7x7 (3)\n",
        "        ['DnaBlock',  6, 240, 1, 1, 4], #11\n",
        "        ['DnaBlock',  6, 240, 1, 1, 4], #12\n",
        "    ]\n",
        "\n",
        "    model_kwargs = dict(\n",
        "        block_args = dna_blocks,\n",
        "        width_mult = 1.0,\n",
        "        se_flag = [2,0,2,0],\n",
        "        group_num = 1,\n",
        "        gbr_type = 'attn',\n",
        "        gbr_dynamic = [True, False, False],\n",
        "        gbr_ffn = True,\n",
        "        num_features = 1920,\n",
        "        stem_chs = 24,\n",
        "        token_num = 6,\n",
        "        token_dim = 192,\n",
        "        **common_model_kwargs,\n",
        "        **kwargs,\n",
        "    )\n",
        "    model = _create_mobile_former(\"mobile_former_508m\", pretrained, **model_kwargs)\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def mobile_former_294m(pretrained=False, **kwargs):\n",
        "\n",
        "    #stem = 16\n",
        "    dna_blocks = [\n",
        "        #b, e1,  c, n, s, e2\n",
        "        ['DnaBlock3', 2,  16, 1, 1, 0], #1 112x112 (1)\n",
        "        ['DnaBlock3', 6,  24, 1, 2, 4], #2 56x56 (2)\n",
        "        ['DnaBlock',  4,  24, 1, 1, 4], #3\n",
        "        ['DnaBlock3', 6,  48, 1, 2, 4], #4 28x28 (2)\n",
        "        ['DnaBlock',  4,  48, 1, 1, 4], #5\n",
        "        ['DnaBlock3', 6,  96, 1, 2, 4], #6 14x14 (4)\n",
        "        ['DnaBlock',  4,  96, 1, 1, 4], #7\n",
        "        ['DnaBlock',  6, 128, 1, 1, 4], #8\n",
        "        ['DnaBlock',  6, 128, 1, 1, 4], #9\n",
        "        ['DnaBlock3', 6, 192, 1, 2, 4], #10 7x7 (3)\n",
        "        ['DnaBlock',  6, 192, 1, 1, 4], #11\n",
        "        ['DnaBlock',  6, 192, 1, 1, 4], #12\n",
        "    ]\n",
        "\n",
        "    model_kwargs = dict(\n",
        "        block_args = dna_blocks,\n",
        "        width_mult = 1.0,\n",
        "        se_flag = [2,0,2,0],\n",
        "        group_num = 1,\n",
        "        gbr_type = 'attn',\n",
        "        gbr_dynamic = [True, False, False],\n",
        "        gbr_ffn = True,\n",
        "        num_features = 1920,\n",
        "        stem_chs = 16,\n",
        "        token_num = 6,\n",
        "        token_dim = 192,\n",
        "        **common_model_kwargs,\n",
        "        **kwargs,\n",
        "    )\n",
        "    model = _create_mobile_former(\"mobile_former_294m\", pretrained, **model_kwargs)\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def mobile_former_214m(pretrained=False, **kwargs):\n",
        "\n",
        "    #stem = 12\n",
        "    dna_blocks = [\n",
        "        #b, e1,  c, n, s, e2\n",
        "        ['DnaBlock3', 2,  12, 1, 1, 0], #1 112x112 (1)\n",
        "        ['DnaBlock3', 6,  20, 1, 2, 4], #2 56x56 (2)\n",
        "        ['DnaBlock',  3,  20, 1, 1, 4], #3\n",
        "        ['DnaBlock3', 6,  40, 1, 2, 4], #4 28x28 (2)\n",
        "        ['DnaBlock',  4,  40, 1, 1, 4], #5\n",
        "        ['DnaBlock3', 6,  80, 1, 2, 4], #6 14x14 (4)\n",
        "        ['DnaBlock',  4,  80, 1, 1, 4], #7\n",
        "        ['DnaBlock',  6, 112, 1, 1, 4], #8\n",
        "        ['DnaBlock',  6, 112, 1, 1, 4], #9\n",
        "        ['DnaBlock3', 6, 160, 1, 2, 4], #10 7x7 (3)\n",
        "        ['DnaBlock',  6, 160, 1, 1, 4], #11\n",
        "        ['DnaBlock',  6, 160, 1, 1, 4], #12\n",
        "    ]\n",
        "\n",
        "\n",
        "    model_kwargs = dict(\n",
        "        block_args = dna_blocks,\n",
        "        width_mult = 1.0,\n",
        "        se_flag = [2,0,2,0],\n",
        "        group_num = 1,\n",
        "        gbr_type = 'attn',\n",
        "        gbr_dynamic = [True, False, False],\n",
        "        gbr_ffn = True,\n",
        "        num_features = 1600,\n",
        "        stem_chs = 12,\n",
        "        token_num = 6,\n",
        "        token_dim = 192,\n",
        "        **common_model_kwargs,\n",
        "        **kwargs,\n",
        "    )\n",
        "    model = _create_mobile_former(\"mobile_former_214m\", pretrained, **model_kwargs)\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def mobile_former_151m(pretrained=False, **kwargs):\n",
        "\n",
        "    #stem = 12\n",
        "    dna_blocks = [\n",
        "        #b, e1,  c, n, s, e2\n",
        "        ['DnaBlock3', 2,  12, 1, 1, 0], #1 112x112 (1)\n",
        "        ['DnaBlock3', 6,  16, 1, 2, 4], #2 56x56 (2)\n",
        "        ['DnaBlock',  3,  16, 1, 1, 3], #3\n",
        "        ['DnaBlock3', 6,  32, 1, 2, 4], #4 28x28 (2)\n",
        "        ['DnaBlock',  3,  32, 1, 1, 3], #5\n",
        "        ['DnaBlock3', 6,  64, 1, 2, 4], #6 14x14 (4)\n",
        "        ['DnaBlock',  4,  64, 1, 1, 4], #7\n",
        "        ['DnaBlock',  6,  88, 1, 1, 4], #8\n",
        "        ['DnaBlock',  6,  88, 1, 1, 4], #9\n",
        "        ['DnaBlock3', 6, 128, 1, 2, 4], #10 7x7 (3)\n",
        "        ['DnaBlock',  6, 128, 1, 1, 4], #11\n",
        "        ['DnaBlock',  6, 128, 1, 1, 4], #12\n",
        "    ]\n",
        "\n",
        "    model_kwargs = dict(\n",
        "        block_args = dna_blocks,\n",
        "        width_mult = 1.0,\n",
        "        se_flag = [2,0,2,0],\n",
        "        group_num = 1,\n",
        "        gbr_type = 'attn',\n",
        "        gbr_dynamic = [True, False, False],\n",
        "        gbr_ffn = True,\n",
        "        num_features = 1280,\n",
        "        stem_chs = 12,\n",
        "        token_num = 6,\n",
        "        token_dim = 192,\n",
        "        **common_model_kwargs,\n",
        "        **kwargs,\n",
        "    )\n",
        "    model = _create_mobile_former(\"mobile_former_151m\", pretrained, **model_kwargs)\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def mobile_former_96m(pretrained=False, **kwargs):\n",
        "\n",
        "    #stem = 12\n",
        "    dna_blocks = [\n",
        "        #b, e1,  c, n, s, e2\n",
        "        ['DnaBlock3', 2,  12, 1, 1, 0], #1 112x112 (1)\n",
        "        ['DnaBlock3', 6,  16, 1, 2, 4], #2 56x56 (1)\n",
        "        ['DnaBlock3', 6,  32, 1, 2, 4], #3 28x28 (2)\n",
        "        ['DnaBlock',  3,  32, 1, 1, 3], #4\n",
        "        ['DnaBlock3', 6,  64, 1, 2, 4], #5 14x14 (3)\n",
        "        ['DnaBlock',  4,  64, 1, 1, 4], #6\n",
        "        ['DnaBlock',  6,  88, 1, 1, 4], #7\n",
        "        ['DnaBlock3', 6, 128, 1, 2, 4], #8 7x7 (2)\n",
        "        ['DnaBlock',  6, 128, 1, 1, 4], #9\n",
        "    ]\n",
        "\n",
        "\n",
        "    model_kwargs = dict(\n",
        "        block_args = dna_blocks,\n",
        "        width_mult = 1.0,\n",
        "        se_flag = [2,0,2,0],\n",
        "        group_num = 1,\n",
        "        gbr_type = 'attn',\n",
        "        gbr_dynamic = [True, False, False],\n",
        "        gbr_ffn = True,\n",
        "        num_features = 1280,\n",
        "        stem_chs = 12,\n",
        "        token_num = 4,\n",
        "        token_dim = 128,\n",
        "        **common_model_kwargs,\n",
        "        **kwargs,\n",
        "    )\n",
        "    model = _create_mobile_former(\"mobile_former_96m\", pretrained, **model_kwargs)\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def mobile_former_52m(pretrained=False, **kwargs):\n",
        "\n",
        "    #stem = 8\n",
        "    dna_blocks = [\n",
        "        #b, e1,  c, n, s, e2\n",
        "        ['DnaBlock3', 3,  12, 1, 2, 0], #1 56x56 (2)\n",
        "        ['DnaBlock',  3,  12, 1, 1, 3], #2\n",
        "        ['DnaBlock3', 6,  24, 1, 2, 4], #3 28x28 (2)\n",
        "        ['DnaBlock',  3,  24, 1, 1, 3], #4\n",
        "        ['DnaBlock3', 6,  48, 1, 2, 4], #5 14x14 (3)\n",
        "        ['DnaBlock',  4,  48, 1, 1, 4], #6\n",
        "        ['DnaBlock',  6,  64, 1, 1, 4], #7\n",
        "        ['DnaBlock3', 6,  96, 1, 2, 4], #8 7x7 (2)\n",
        "        ['DnaBlock',  6,  96, 1, 1, 4], #9\n",
        "    ]\n",
        "\n",
        "    model_kwargs = dict(\n",
        "        block_args = dna_blocks,\n",
        "        width_mult = 1.0,\n",
        "        se_flag = [2,0,2,0],\n",
        "        group_num = 1,\n",
        "        gbr_type = 'attn',\n",
        "        gbr_dynamic = [True, False, False],\n",
        "        gbr_ffn = True,\n",
        "        num_features = 1024,\n",
        "        stem_chs = 8,\n",
        "        token_num = 3,\n",
        "        token_dim = 128,\n",
        "        **common_model_kwargs,\n",
        "        **kwargs,\n",
        "    )\n",
        "    model = _create_mobile_former(\"mobile_former_52m\", pretrained, **model_kwargs)\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def mobile_former_26m(pretrained=False, **kwargs):\n",
        "\n",
        "    #stem = 8\n",
        "    dna_blocks = [\n",
        "        #b, e1,  c, n, s, e2\n",
        "        ['DnaBlock3', 3,  12, 1, 2, 0], #1 56x56 (2)\n",
        "        ['DnaBlock',  3,  12, 1, 1, 3], #2\n",
        "        ['DnaBlock3', 6,  24, 1, 2, 4], #3 28x28 (2)\n",
        "        ['DnaBlock',  3,  24, 1, 1, 3], #4\n",
        "        ['DnaBlock3', 6,  48, 1, 2, 4], #5 14x14 (3)\n",
        "        ['DnaBlock',  4,  48, 1, 1, 4], #6\n",
        "        ['DnaBlock',  6,  64, 1, 1, 4], #7\n",
        "        ['DnaBlock3', 6,  96, 1, 2, 4], #8 7x7 (2)\n",
        "        ['DnaBlock',  6,  96, 1, 1, 4], #9\n",
        "    ]\n",
        "\n",
        "    model_kwargs = dict(\n",
        "        block_args = dna_blocks,\n",
        "        width_mult = 1.0,\n",
        "        se_flag = [2,0,2,0],\n",
        "        group_num = 4,\n",
        "        gbr_type = 'attn',\n",
        "        gbr_dynamic = [True, False, False],\n",
        "        gbr_ffn = True,\n",
        "        num_features = 1024,\n",
        "        stem_chs = 8,\n",
        "        token_num = 3,\n",
        "        token_dim = 128,\n",
        "        **common_model_kwargs,\n",
        "        **kwargs,\n",
        "    )\n",
        "    model = _create_mobile_former(\"mobile_former_26m\", pretrained, **model_kwargs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 1024\n",
        "test_batch_size = 1024\n",
        "id_dict = {}\n",
        "for i, line in enumerate(open('/content/tiny-imagenet-200/wnids.txt', 'r')):\n",
        "  id_dict[line.replace('\\n', '')] = i\n",
        "\n",
        "class TrainTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, id, transform=None):\n",
        "        self.filenames = glob.glob(\"/content/tiny-imagenet-200/train/*/*/*.JPEG\")\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = read_image(img_path)\n",
        "        if image.shape[0] == 1:\n",
        "          image = read_image(img_path,ImageReadMode.RGB)\n",
        "        label = self.id_dict[img_path.split('/')[4]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image.type(torch.FloatTensor))\n",
        "        return image, label\n",
        "\n",
        "class TestTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, id, transform=None):\n",
        "        self.filenames = glob.glob(\"/content/tiny-imagenet-200/val/images/*.JPEG\")\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "        self.cls_dic = {}\n",
        "        for i, line in enumerate(open('/content/tiny-imagenet-200/val/val_annotations.txt', 'r')):\n",
        "            a = line.split('\\t')\n",
        "            img, cls_id = a[0],a[1]\n",
        "            self.cls_dic[img] = self.id_dict[cls_id]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = read_image(img_path)\n",
        "        if image.shape[0] == 1:\n",
        "          image = read_image(img_path,ImageReadMode.RGB)\n",
        "        label = self.cls_dic[img_path.split('/')[-1]]\n",
        "        if self.transform:\n",
        "            image = self.transform(image.type(torch.FloatTensor))\n",
        "        return image, label\n",
        "\n",
        "transform = transforms.Normalize((122.4786, 114.2755, 101.3963), (70.4924, 68.5679, 71.8127))\n",
        "\n",
        "train_dataset = TrainTinyImageNetDataset(id=id_dict, transform = transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
        "# batch size untuk train dibuat 2 karena ga bisa 1\n",
        "\n",
        "test_dataset = TestTinyImageNetDataset(id=id_dict, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "g8I5W6O0bS8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dhYKFzqzhRS",
        "outputId": "8dbe3275-c29d-4cc7-b5c4-0f73a5fc25be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block: 1024, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "block: 1024, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 16, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 24, token: 192\n",
            "block: 256, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 24, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 24, token: 192\n",
            "block: 256, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 24, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 48, token: 192\n",
            "block: 64, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 48, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 48, token: 192\n",
            "block: 64, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 48, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 96, token: 192\n",
            "block: 16, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 96, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 96, token: 192\n",
            "block: 16, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 96, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 128, token: 192\n",
            "block: 16, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 128, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 128, token: 192\n",
            "block: 16, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 128, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 192, token: 192\n",
            "block: 4, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 192, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 192, token: 192\n",
            "block: 4, cnn-drop 0.0000, mlp-drop 0.0000\n",
            "L2G: 2 heads, inp: 192, token: 192\n",
            "G2G: 4 heads\n",
            "use ffn\n",
            "G2L: 2 heads, inp: 192, token: 192\n",
            "L2G: 2 heads, inp: 192, token: 192\n",
            "MobileFormer(\n",
            "  (tokens): Embedding(6, 192)\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU6(inplace=True)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): DnaBlock3(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "        (3): Sequential()\n",
            "        (4): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DnaBlock3(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(16, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
            "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=192, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
            "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=192, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv4): Sequential(\n",
            "        (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act4): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper4): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=16, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=16, bias=True)\n",
            "        (proj): Linear(in_features=16, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=24, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=24, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "    (2): DnaBlock(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=192, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
            "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper2): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=192, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=24, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=24, bias=True)\n",
            "        (proj): Linear(in_features=24, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=24, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=24, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "    (3): DnaBlock3(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(24, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
            "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=288, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=384, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv4): Sequential(\n",
            "        (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act4): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper4): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=24, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=24, bias=True)\n",
            "        (proj): Linear(in_features=24, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=48, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=48, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "    (4): DnaBlock(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=384, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper2): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=384, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=48, bias=True)\n",
            "        (proj): Linear(in_features=48, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=48, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=48, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "    (5): DnaBlock3(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(48, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
            "        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=576, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=768, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv4): Sequential(\n",
            "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act4): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper4): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=48, bias=True)\n",
            "        (proj): Linear(in_features=48, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=96, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=96, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "    (6): DnaBlock(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=768, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper2): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=768, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=96, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=96, bias=True)\n",
            "        (proj): Linear(in_features=96, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=96, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=96, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "    (7): DnaBlock(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=1152, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
            "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper2): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=1152, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=96, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=96, bias=True)\n",
            "        (proj): Linear(in_features=96, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=128, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=128, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "    (8): DnaBlock(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=1536, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper2): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=1536, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=128, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=128, bias=True)\n",
            "        (proj): Linear(in_features=128, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=128, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=128, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "    (9): DnaBlock3(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(128, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=1536, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(192, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=1536, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv4): Sequential(\n",
            "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act4): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper4): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=128, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=128, bias=True)\n",
            "        (proj): Linear(in_features=128, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "    (10): DnaBlock(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=2304, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
            "        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper2): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=2304, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "    (11): DnaBlock(\n",
            "      (conv1): Sequential(\n",
            "        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act1): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper1): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=2304, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv2): Sequential(\n",
            "        (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
            "        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (act2): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper2): HyperFunc(\n",
            "        (hyper): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=48, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=48, out_features=2304, bias=True)\n",
            "          (3): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv3): Sequential(\n",
            "        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): Sequential()\n",
            "      )\n",
            "      (act3): DyReLU(\n",
            "        (act): Sequential()\n",
            "      )\n",
            "      (hyper3): Sequential()\n",
            "      (drop_path): DropPath(drop_prob=0.000)\n",
            "      (local_global): Local2Global(\n",
            "        (alpha): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (1): h_sigmoid(\n",
            "            (relu): ReLU6(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_block): GlobalBlock(\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=384, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        )\n",
            "        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "      (global_local): Global2Local(\n",
            "        (k): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "        (drop_path): DropPath(drop_prob=0.000)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (local_global): Local2Global(\n",
            "    (alpha): Sequential(\n",
            "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
            "      (1): h_sigmoid(\n",
            "        (relu): ReLU6(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (q): Linear(in_features=192, out_features=192, bias=True)\n",
            "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "    (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    (drop_path): DropPath(drop_prob=0.000)\n",
            "  )\n",
            "  (classifier): MergeClassifier(\n",
            "    (conv): Sequential(\n",
            "      (0): Sequential()\n",
            "      (1): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (act): DyReLU(\n",
            "      (act): ReLU6(inplace=True)\n",
            "    )\n",
            "    (hyper): Sequential()\n",
            "    (avgpool): Sequential(\n",
            "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "      (1): h_swish(\n",
            "        (sigmoid): h_sigmoid(\n",
            "          (relu): ReLU6(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fc): Sequential(\n",
            "      (0): Linear(in_features=1344, out_features=1920, bias=True)\n",
            "      (1): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): h_swish(\n",
            "        (sigmoid): h_sigmoid(\n",
            "          (relu): ReLU6(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Dropout(p=0.0, inplace=False)\n",
            "      (1): Linear(in_features=1920, out_features=1000, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "num_classes = 200\n",
        "\n",
        "model = mobile_former_294m()\n",
        "model.num_classes = num_classes\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "# # Sesuaikan dengan ukuran gambar dataset 64 or 224\n",
        "# x = torch.randn(train_batch_size, 3, 64, 64)\n",
        "# # pakai 2 karena kalau mode train harus 2 ga bisa 1\n",
        "\n",
        "# x = x.to(device)\n",
        "# output = model(x)\n",
        "\n",
        "# print(output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esxiXaiRd83T",
        "outputId": "49d44e7d-f6df-4597-dd31-7fe00d7f7997"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:03<00:00,  1.53it/s, loss=3.91]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:04<00:00,  2.44it/s, loss=4.1]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 4.0936, Train Acc: 12.40%, Val Loss: 4.0172, Val Acc: 13.33%\n",
            "Validation loss improved. Saving model to best_mobile_former_model.pth.\n",
            "\n",
            "Epoch 2/450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:04<00:00,  1.52it/s, loss=3.55]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:03<00:00,  2.56it/s, loss=3.88]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6094, Train Acc: 19.12%, Val Loss: 3.7131, Val Acc: 18.11%\n",
            "Validation loss improved. Saving model to best_mobile_former_model.pth.\n",
            "\n",
            "Epoch 3/450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:03<00:00,  1.55it/s, loss=3.11]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:04<00:00,  2.30it/s, loss=3.65]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.2769, Train Acc: 24.58%, Val Loss: 3.5473, Val Acc: 20.94%\n",
            "Validation loss improved. Saving model to best_mobile_former_model.pth.\n",
            "\n",
            "Epoch 4/450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:04<00:00,  1.52it/s, loss=3.03]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:03<00:00,  2.55it/s, loss=3.52]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.0009, Train Acc: 29.50%, Val Loss: 3.4133, Val Acc: 24.70%\n",
            "Validation loss improved. Saving model to best_mobile_former_model.pth.\n",
            "\n",
            "Epoch 5/450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:04<00:00,  1.52it/s, loss=2.71]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:05<00:00,  1.95it/s, loss=3.37]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.7647, Train Acc: 33.96%, Val Loss: 3.2261, Val Acc: 27.21%\n",
            "Validation loss improved. Saving model to best_mobile_former_model.pth.\n",
            "\n",
            "Epoch 6/450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:03<00:00,  1.54it/s, loss=2.58]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:03<00:00,  2.51it/s, loss=3.35]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.5449, Train Acc: 38.15%, Val Loss: 3.2417, Val Acc: 27.75%\n",
            "No improvement in validation loss. Counter: 1/10\n",
            "\n",
            "Epoch 7/450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:03<00:00,  1.55it/s, loss=2.37]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:05<00:00,  1.79it/s, loss=3.5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.3364, Train Acc: 42.25%, Val Loss: 3.3225, Val Acc: 27.15%\n",
            "No improvement in validation loss. Counter: 2/10\n",
            "\n",
            "Epoch 8/450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:02<00:00,  1.56it/s, loss=2.21]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:03<00:00,  2.59it/s, loss=3.4]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.1158, Train Acc: 46.51%, Val Loss: 3.2782, Val Acc: 28.45%\n",
            "No improvement in validation loss. Counter: 3/10\n",
            "\n",
            "Epoch 9/450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:02<00:00,  1.56it/s, loss=2.01]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:05<00:00,  1.90it/s, loss=3.68]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.9040, Train Acc: 50.88%, Val Loss: 3.5559, Val Acc: 27.63%\n",
            "No improvement in validation loss. Counter: 4/10\n",
            "\n",
            "Epoch 10/450\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:02<00:00,  1.57it/s, loss=1.33]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:03<00:00,  2.51it/s, loss=3.76]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.2642, Train Acc: 66.69%, Val Loss: 3.5302, Val Acc: 29.53%\n",
            "No improvement in validation loss. Counter: 5/10\n",
            "\n",
            "Epoch 11/450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:02<00:00,  1.56it/s, loss=0.978]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:04<00:00,  2.43it/s, loss=4.05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8445, Train Acc: 77.43%, Val Loss: 3.8544, Val Acc: 29.79%\n",
            "No improvement in validation loss. Counter: 6/10\n",
            "\n",
            "Epoch 12/450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:02<00:00,  1.56it/s, loss=0.746]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:04<00:00,  2.22it/s, loss=4.53]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5614, Train Acc: 85.09%, Val Loss: 4.3381, Val Acc: 27.63%\n",
            "No improvement in validation loss. Counter: 7/10\n",
            "\n",
            "Epoch 13/450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:03<00:00,  1.54it/s, loss=0.434]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s, loss=4.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3552, Train Acc: 90.82%, Val Loss: 4.5903, Val Acc: 27.19%\n",
            "No improvement in validation loss. Counter: 8/10\n",
            "\n",
            "Epoch 14/450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:03<00:00,  1.55it/s, loss=0.0943]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:04<00:00,  2.19it/s, loss=4.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1230, Train Acc: 98.12%, Val Loss: 4.5819, Val Acc: 28.64%\n",
            "No improvement in validation loss. Counter: 9/10\n",
            "\n",
            "Epoch 15/450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Progress: 100%|██████████| 98/98 [01:03<00:00,  1.54it/s, loss=0.0376]\n",
            "Validation Progress: 100%|██████████| 10/10 [00:03<00:00,  2.55it/s, loss=4.87]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0355, Train Acc: 99.93%, Val Loss: 4.6588, Val Acc: 28.29%\n",
            "No improvement in validation loss. Counter: 10/10\n",
            "Early stopping triggered!\n",
            "Training complete. Best model loaded from best_mobile_former_model.pth.\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 450\n",
        "lr_val = 0.001\n",
        "patience = 10\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr_val, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "best_model_path = 'best_mobile_former_model.pth'\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "train_losses, train_acc, val_losses, val_acc = [], [], [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=\"Train Progress\", leave=True)\n",
        "\n",
        "    for inputs, labels in train_loader_tqdm:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct_train += predicted.eq(labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    test_loader_tqdm = tqdm(test_loader, desc=\"Validation Progress\", leave=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader_tqdm:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct_val += predicted.eq(labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "            test_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "    val_loss /= len(test_loader.dataset)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_acc.append(train_accuracy)\n",
        "    val_acc.append(val_accuracy)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Validation loss improved. Saving model to {best_model_path}.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement in validation loss. Counter: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print(f\"Training complete. Best model loaded from {best_model_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBrJczYh5sye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e00aaf-45ca-465a-d4a4-bb1996eb3a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([784, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "print(inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7S02k1QCGwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ad56a7-e02d-4193-9bd4-ad1188f29f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test Progress: 100%|██████████| 10/10 [00:04<00:00,  2.29it/s, loss=3.37]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 3.2261, Top-1 Accuracy: 27.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::repeat encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::clone encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::hardtanh_ encountered 39 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 114 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 207 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::softmax encountered 34 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 36 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 11 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 44 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "classifier.conv.0, classifier.hyper, features.0.conv.3, features.1.act1.act, features.1.act2, features.1.act2.act, features.1.act3.act, features.1.act4, features.1.act4.act, features.1.conv1.2, features.1.conv3.2, features.1.drop_path, features.1.global_block.drop_path, features.1.global_local.drop_path, features.1.hyper4, features.1.local_global.drop_path, features.10.act1.act, features.10.act2.act, features.10.act3, features.10.act3.act, features.10.conv1.2, features.10.conv3.2, features.10.drop_path, features.10.global_block.drop_path, features.10.global_local.drop_path, features.10.hyper3, features.10.local_global.drop_path, features.11.act1.act, features.11.act2.act, features.11.act3, features.11.act3.act, features.11.conv1.2, features.11.conv3.2, features.11.drop_path, features.11.global_block.drop_path, features.11.global_local.drop_path, features.11.hyper3, features.11.local_global.drop_path, features.2.act1.act, features.2.act2.act, features.2.act3, features.2.act3.act, features.2.conv1.2, features.2.conv3.2, features.2.drop_path, features.2.global_block.drop_path, features.2.global_local.drop_path, features.2.hyper3, features.2.local_global.drop_path, features.3.act1.act, features.3.act2, features.3.act2.act, features.3.act3.act, features.3.act4, features.3.act4.act, features.3.conv1.2, features.3.conv3.2, features.3.drop_path, features.3.global_block.drop_path, features.3.global_local.drop_path, features.3.hyper4, features.3.local_global.drop_path, features.4.act1.act, features.4.act2.act, features.4.act3, features.4.act3.act, features.4.conv1.2, features.4.conv3.2, features.4.drop_path, features.4.global_block.drop_path, features.4.global_local.drop_path, features.4.hyper3, features.4.local_global.drop_path, features.5.act1.act, features.5.act2, features.5.act2.act, features.5.act3.act, features.5.act4, features.5.act4.act, features.5.conv1.2, features.5.conv3.2, features.5.drop_path, features.5.global_block.drop_path, features.5.global_local.drop_path, features.5.hyper4, features.5.local_global.drop_path, features.6.act1.act, features.6.act2.act, features.6.act3, features.6.act3.act, features.6.conv1.2, features.6.conv3.2, features.6.drop_path, features.6.global_block.drop_path, features.6.global_local.drop_path, features.6.hyper3, features.6.local_global.drop_path, features.7.act1.act, features.7.act2.act, features.7.act3, features.7.act3.act, features.7.conv1.2, features.7.conv3.2, features.7.drop_path, features.7.global_block.drop_path, features.7.global_local.drop_path, features.7.hyper3, features.7.local_global.drop_path, features.8.act1.act, features.8.act2.act, features.8.act3, features.8.act3.act, features.8.conv1.2, features.8.conv3.2, features.8.drop_path, features.8.global_block.drop_path, features.8.global_local.drop_path, features.8.hyper3, features.8.local_global.drop_path, features.9.act1.act, features.9.act2, features.9.act2.act, features.9.act3.act, features.9.act4, features.9.act4.act, features.9.conv1.2, features.9.conv3.2, features.9.drop_path, features.9.global_block.drop_path, features.9.global_local.drop_path, features.9.hyper4, features.9.local_global.drop_path, local_global.drop_path, tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 50583306240\n",
            ": 50583306240\n",
            "tokens: 0\n",
            "stem: 486539264\n",
            "stem.0: 452984832\n",
            "stem.1: 33554432\n",
            "stem.2: 0\n",
            "features: 43869405184\n",
            "features.0: 939524096\n",
            "features.0.conv: 939524096\n",
            "features.0.conv.0: 301989888\n",
            "features.0.conv.1: 67108864\n",
            "features.0.conv.2: 0\n",
            "features.0.conv.3: 0\n",
            "features.0.conv.4: 536870912\n",
            "features.0.conv.5: 33554432\n",
            "features.1: 3605397504\n",
            "features.1.conv1: 276824064\n",
            "features.1.conv1.0: 226492416\n",
            "features.1.conv1.1: 50331648\n",
            "features.1.conv1.2: 0\n",
            "features.1.act1: 0\n",
            "features.1.act1.act: 0\n",
            "features.1.hyper1: 18874368\n",
            "features.1.hyper1.hyper: 18874368\n",
            "features.1.hyper1.hyper.0: 9437184\n",
            "features.1.hyper1.hyper.1: 0\n",
            "features.1.hyper1.hyper.2: 9437184\n",
            "features.1.hyper1.hyper.3: 0\n",
            "features.1.hyper1.hyper.3.relu: 0\n",
            "features.1.conv2: 616562688\n",
            "features.1.conv2.0: 603979776\n",
            "features.1.conv2.1: 12582912\n",
            "features.1.act2: 0\n",
            "features.1.act2.act: 0\n",
            "features.1.conv3: 276824064\n",
            "features.1.conv3.0: 226492416\n",
            "features.1.conv3.1: 50331648\n",
            "features.1.conv3.2: 0\n",
            "features.1.act3: 0\n",
            "features.1.act3.act: 0\n",
            "features.1.hyper3: 18874368\n",
            "features.1.hyper3.hyper: 18874368\n",
            "features.1.hyper3.hyper.0: 9437184\n",
            "features.1.hyper3.hyper.1: 0\n",
            "features.1.hyper3.hyper.2: 9437184\n",
            "features.1.hyper3.hyper.3: 0\n",
            "features.1.hyper3.hyper.3.relu: 0\n",
            "features.1.conv4: 616562688\n",
            "features.1.conv4.0: 603979776\n",
            "features.1.conv4.1: 12582912\n",
            "features.1.act4: 0\n",
            "features.1.act4.act: 0\n",
            "features.1.hyper4: 0\n",
            "features.1.drop_path: 0\n",
            "features.1.local_global: 263847936\n",
            "features.1.local_global.alpha: 18874368\n",
            "features.1.local_global.alpha.0: 18874368\n",
            "features.1.local_global.alpha.1: 0\n",
            "features.1.local_global.alpha.1.relu: 0\n",
            "features.1.local_global.q: 18874368\n",
            "features.1.local_global.proj: 18874368\n",
            "features.1.local_global.layer_norm: 5898240\n",
            "features.1.local_global.drop_path: 0\n",
            "features.1.global_block: 1384906752\n",
            "features.1.global_block.ffn: 905969664\n",
            "features.1.global_block.ffn.0: 452984832\n",
            "features.1.global_block.ffn.1: 0\n",
            "features.1.global_block.ffn.2: 452984832\n",
            "features.1.global_block.ffn_norm: 5898240\n",
            "features.1.global_block.q: 226492416\n",
            "features.1.global_block.channel_mlp: 226492416\n",
            "features.1.global_block.layer_norm: 5898240\n",
            "features.1.global_block.drop_path: 0\n",
            "features.1.global_local: 132120576\n",
            "features.1.global_local.k: 28311552\n",
            "features.1.global_local.proj: 28311552\n",
            "features.1.global_local.drop_path: 0\n",
            "features.2: 3268804608\n",
            "features.2.conv1: 654311424\n",
            "features.2.conv1.0: 603979776\n",
            "features.2.conv1.1: 50331648\n",
            "features.2.conv1.2: 0\n",
            "features.2.act1: 0\n",
            "features.2.act1.act: 0\n",
            "features.2.hyper1: 18874368\n",
            "features.2.hyper1.hyper: 18874368\n",
            "features.2.hyper1.hyper.0: 9437184\n",
            "features.2.hyper1.hyper.1: 0\n",
            "features.2.hyper1.hyper.2: 9437184\n",
            "features.2.hyper1.hyper.3: 0\n",
            "features.2.hyper1.hyper.3.relu: 0\n",
            "features.2.conv2: 276824064\n",
            "features.2.conv2.0: 226492416\n",
            "features.2.conv2.1: 50331648\n",
            "features.2.act2: 0\n",
            "features.2.act2.act: 0\n",
            "features.2.hyper2: 18874368\n",
            "features.2.hyper2.hyper: 18874368\n",
            "features.2.hyper2.hyper.0: 9437184\n",
            "features.2.hyper2.hyper.1: 0\n",
            "features.2.hyper2.hyper.2: 9437184\n",
            "features.2.hyper2.hyper.3: 0\n",
            "features.2.hyper2.hyper.3.relu: 0\n",
            "features.2.conv3: 616562688\n",
            "features.2.conv3.0: 603979776\n",
            "features.2.conv3.1: 12582912\n",
            "features.2.conv3.2: 0\n",
            "features.2.act3: 0\n",
            "features.2.act3.act: 0\n",
            "features.2.hyper3: 0\n",
            "features.2.drop_path: 0\n",
            "features.2.local_global: 166330368\n",
            "features.2.local_global.alpha: 28311552\n",
            "features.2.local_global.alpha.0: 28311552\n",
            "features.2.local_global.alpha.1: 0\n",
            "features.2.local_global.alpha.1.relu: 0\n",
            "features.2.local_global.q: 28311552\n",
            "features.2.local_global.proj: 28311552\n",
            "features.2.local_global.layer_norm: 5898240\n",
            "features.2.local_global.drop_path: 0\n",
            "features.2.global_block: 1384906752\n",
            "features.2.global_block.ffn: 905969664\n",
            "features.2.global_block.ffn.0: 452984832\n",
            "features.2.global_block.ffn.1: 0\n",
            "features.2.global_block.ffn.2: 452984832\n",
            "features.2.global_block.ffn_norm: 5898240\n",
            "features.2.global_block.q: 226492416\n",
            "features.2.global_block.channel_mlp: 226492416\n",
            "features.2.global_block.layer_norm: 5898240\n",
            "features.2.global_block.drop_path: 0\n",
            "features.2.global_local: 132120576\n",
            "features.2.global_local.k: 28311552\n",
            "features.2.global_local.proj: 28311552\n",
            "features.2.global_local.drop_path: 0\n",
            "features.3: 3065905152\n",
            "features.3.conv1: 103809024\n",
            "features.3.conv1.0: 84934656\n",
            "features.3.conv1.1: 18874368\n",
            "features.3.conv1.2: 0\n",
            "features.3.act1: 0\n",
            "features.3.act1.act: 0\n",
            "features.3.hyper1: 23592960\n",
            "features.3.hyper1.hyper: 23592960\n",
            "features.3.hyper1.hyper.0: 9437184\n",
            "features.3.hyper1.hyper.1: 0\n",
            "features.3.hyper1.hyper.2: 14155776\n",
            "features.3.hyper1.hyper.3: 0\n",
            "features.3.hyper1.hyper.3.relu: 0\n",
            "features.3.conv2: 459276288\n",
            "features.3.conv2.0: 452984832\n",
            "features.3.conv2.1: 6291456\n",
            "features.3.act2: 0\n",
            "features.3.act2.act: 0\n",
            "features.3.conv3: 138412032\n",
            "features.3.conv3.0: 113246208\n",
            "features.3.conv3.1: 25165824\n",
            "features.3.conv3.2: 0\n",
            "features.3.act3: 0\n",
            "features.3.act3.act: 0\n",
            "features.3.hyper3: 28311552\n",
            "features.3.hyper3.hyper: 28311552\n",
            "features.3.hyper3.hyper.0: 9437184\n",
            "features.3.hyper3.hyper.1: 0\n",
            "features.3.hyper3.hyper.2: 18874368\n",
            "features.3.hyper3.hyper.3: 0\n",
            "features.3.hyper3.hyper.3.relu: 0\n",
            "features.3.conv4: 610271232\n",
            "features.3.conv4.0: 603979776\n",
            "features.3.conv4.1: 6291456\n",
            "features.3.act4: 0\n",
            "features.3.act4.act: 0\n",
            "features.3.hyper4: 0\n",
            "features.3.drop_path: 0\n",
            "features.3.local_global: 166330368\n",
            "features.3.local_global.alpha: 28311552\n",
            "features.3.local_global.alpha.0: 28311552\n",
            "features.3.local_global.alpha.1: 0\n",
            "features.3.local_global.alpha.1.relu: 0\n",
            "features.3.local_global.q: 28311552\n",
            "features.3.local_global.proj: 28311552\n",
            "features.3.local_global.layer_norm: 5898240\n",
            "features.3.local_global.drop_path: 0\n",
            "features.3.global_block: 1384906752\n",
            "features.3.global_block.ffn: 905969664\n",
            "features.3.global_block.ffn.0: 452984832\n",
            "features.3.global_block.ffn.1: 0\n",
            "features.3.global_block.ffn.2: 452984832\n",
            "features.3.global_block.ffn_norm: 5898240\n",
            "features.3.global_block.q: 226492416\n",
            "features.3.global_block.channel_mlp: 226492416\n",
            "features.3.global_block.layer_norm: 5898240\n",
            "features.3.global_block.drop_path: 0\n",
            "features.3.global_local: 150994944\n",
            "features.3.global_local.k: 56623104\n",
            "features.3.global_local.proj: 56623104\n",
            "features.3.global_local.drop_path: 0\n",
            "features.4: 3183869952\n",
            "features.4.conv1: 629145600\n",
            "features.4.conv1.0: 603979776\n",
            "features.4.conv1.1: 25165824\n",
            "features.4.conv1.2: 0\n",
            "features.4.act1: 0\n",
            "features.4.act1.act: 0\n",
            "features.4.hyper1: 28311552\n",
            "features.4.hyper1.hyper: 28311552\n",
            "features.4.hyper1.hyper.0: 9437184\n",
            "features.4.hyper1.hyper.1: 0\n",
            "features.4.hyper1.hyper.2: 18874368\n",
            "features.4.hyper1.hyper.3: 0\n",
            "features.4.hyper1.hyper.3.relu: 0\n",
            "features.4.conv2: 138412032\n",
            "features.4.conv2.0: 113246208\n",
            "features.4.conv2.1: 25165824\n",
            "features.4.act2: 0\n",
            "features.4.act2.act: 0\n",
            "features.4.hyper2: 28311552\n",
            "features.4.hyper2.hyper: 28311552\n",
            "features.4.hyper2.hyper.0: 9437184\n",
            "features.4.hyper2.hyper.1: 0\n",
            "features.4.hyper2.hyper.2: 18874368\n",
            "features.4.hyper2.hyper.3: 0\n",
            "features.4.hyper2.hyper.3.relu: 0\n",
            "features.4.conv3: 610271232\n",
            "features.4.conv3.0: 603979776\n",
            "features.4.conv3.1: 6291456\n",
            "features.4.conv3.2: 0\n",
            "features.4.act3: 0\n",
            "features.4.act3.act: 0\n",
            "features.4.hyper3: 0\n",
            "features.4.drop_path: 0\n",
            "features.4.local_global: 213516288\n",
            "features.4.local_global.alpha: 56623104\n",
            "features.4.local_global.alpha.0: 56623104\n",
            "features.4.local_global.alpha.1: 0\n",
            "features.4.local_global.alpha.1.relu: 0\n",
            "features.4.local_global.q: 56623104\n",
            "features.4.local_global.proj: 56623104\n",
            "features.4.local_global.layer_norm: 5898240\n",
            "features.4.local_global.drop_path: 0\n",
            "features.4.global_block: 1384906752\n",
            "features.4.global_block.ffn: 905969664\n",
            "features.4.global_block.ffn.0: 452984832\n",
            "features.4.global_block.ffn.1: 0\n",
            "features.4.global_block.ffn.2: 452984832\n",
            "features.4.global_block.ffn_norm: 5898240\n",
            "features.4.global_block.q: 226492416\n",
            "features.4.global_block.channel_mlp: 226492416\n",
            "features.4.global_block.layer_norm: 5898240\n",
            "features.4.global_block.drop_path: 0\n",
            "features.4.global_local: 150994944\n",
            "features.4.global_local.k: 56623104\n",
            "features.4.global_local.proj: 56623104\n",
            "features.4.global_local.drop_path: 0\n",
            "features.5: 3113091072\n",
            "features.5.conv1: 51904512\n",
            "features.5.conv1.0: 42467328\n",
            "features.5.conv1.1: 9437184\n",
            "features.5.conv1.2: 0\n",
            "features.5.act1: 0\n",
            "features.5.act1.act: 0\n",
            "features.5.hyper1: 37748736\n",
            "features.5.hyper1.hyper: 37748736\n",
            "features.5.hyper1.hyper.0: 9437184\n",
            "features.5.hyper1.hyper.1: 0\n",
            "features.5.hyper1.hyper.2: 28311552\n",
            "features.5.hyper1.hyper.3: 0\n",
            "features.5.hyper1.hyper.3.relu: 0\n",
            "features.5.conv2: 456130560\n",
            "features.5.conv2.0: 452984832\n",
            "features.5.conv2.1: 3145728\n",
            "features.5.act2: 0\n",
            "features.5.act2.act: 0\n",
            "features.5.conv3: 69206016\n",
            "features.5.conv3.0: 56623104\n",
            "features.5.conv3.1: 12582912\n",
            "features.5.conv3.2: 0\n",
            "features.5.act3: 0\n",
            "features.5.act3.act: 0\n",
            "features.5.hyper3: 47185920\n",
            "features.5.hyper3.hyper: 47185920\n",
            "features.5.hyper3.hyper.0: 9437184\n",
            "features.5.hyper3.hyper.1: 0\n",
            "features.5.hyper3.hyper.2: 37748736\n",
            "features.5.hyper3.hyper.3: 0\n",
            "features.5.hyper3.hyper.3.relu: 0\n",
            "features.5.conv4: 607125504\n",
            "features.5.conv4.0: 603979776\n",
            "features.5.conv4.1: 3145728\n",
            "features.5.act4: 0\n",
            "features.5.act4.act: 0\n",
            "features.5.hyper4: 0\n",
            "features.5.drop_path: 0\n",
            "features.5.local_global: 213516288\n",
            "features.5.local_global.alpha: 56623104\n",
            "features.5.local_global.alpha.0: 56623104\n",
            "features.5.local_global.alpha.1: 0\n",
            "features.5.local_global.alpha.1.relu: 0\n",
            "features.5.local_global.q: 56623104\n",
            "features.5.local_global.proj: 56623104\n",
            "features.5.local_global.layer_norm: 5898240\n",
            "features.5.local_global.drop_path: 0\n",
            "features.5.global_block: 1384906752\n",
            "features.5.global_block.ffn: 905969664\n",
            "features.5.global_block.ffn.0: 452984832\n",
            "features.5.global_block.ffn.1: 0\n",
            "features.5.global_block.ffn.2: 452984832\n",
            "features.5.global_block.ffn_norm: 5898240\n",
            "features.5.global_block.q: 226492416\n",
            "features.5.global_block.channel_mlp: 226492416\n",
            "features.5.global_block.layer_norm: 5898240\n",
            "features.5.global_block.drop_path: 0\n",
            "features.5.global_local: 245366784\n",
            "features.5.global_local.k: 113246208\n",
            "features.5.global_local.proj: 113246208\n",
            "features.5.global_local.drop_path: 0\n",
            "features.6: 3382050816\n",
            "features.6.conv1: 616562688\n",
            "features.6.conv1.0: 603979776\n",
            "features.6.conv1.1: 12582912\n",
            "features.6.conv1.2: 0\n",
            "features.6.act1: 0\n",
            "features.6.act1.act: 0\n",
            "features.6.hyper1: 47185920\n",
            "features.6.hyper1.hyper: 47185920\n",
            "features.6.hyper1.hyper.0: 9437184\n",
            "features.6.hyper1.hyper.1: 0\n",
            "features.6.hyper1.hyper.2: 37748736\n",
            "features.6.hyper1.hyper.3: 0\n",
            "features.6.hyper1.hyper.3.relu: 0\n",
            "features.6.conv2: 69206016\n",
            "features.6.conv2.0: 56623104\n",
            "features.6.conv2.1: 12582912\n",
            "features.6.act2: 0\n",
            "features.6.act2.act: 0\n",
            "features.6.hyper2: 47185920\n",
            "features.6.hyper2.hyper: 47185920\n",
            "features.6.hyper2.hyper.0: 9437184\n",
            "features.6.hyper2.hyper.1: 0\n",
            "features.6.hyper2.hyper.2: 37748736\n",
            "features.6.hyper2.hyper.3: 0\n",
            "features.6.hyper2.hyper.3.relu: 0\n",
            "features.6.conv3: 607125504\n",
            "features.6.conv3.0: 603979776\n",
            "features.6.conv3.1: 3145728\n",
            "features.6.conv3.2: 0\n",
            "features.6.act3: 0\n",
            "features.6.act3.act: 0\n",
            "features.6.hyper3: 0\n",
            "features.6.drop_path: 0\n",
            "features.6.local_global: 364511232\n",
            "features.6.local_global.alpha: 113246208\n",
            "features.6.local_global.alpha.0: 113246208\n",
            "features.6.local_global.alpha.1: 0\n",
            "features.6.local_global.alpha.1.relu: 0\n",
            "features.6.local_global.q: 113246208\n",
            "features.6.local_global.proj: 113246208\n",
            "features.6.local_global.layer_norm: 5898240\n",
            "features.6.local_global.drop_path: 0\n",
            "features.6.global_block: 1384906752\n",
            "features.6.global_block.ffn: 905969664\n",
            "features.6.global_block.ffn.0: 452984832\n",
            "features.6.global_block.ffn.1: 0\n",
            "features.6.global_block.ffn.2: 452984832\n",
            "features.6.global_block.ffn_norm: 5898240\n",
            "features.6.global_block.q: 226492416\n",
            "features.6.global_block.channel_mlp: 226492416\n",
            "features.6.global_block.layer_norm: 5898240\n",
            "features.6.global_block.drop_path: 0\n",
            "features.6.global_local: 245366784\n",
            "features.6.global_local.k: 113246208\n",
            "features.6.global_local.proj: 113246208\n",
            "features.6.global_local.drop_path: 0\n",
            "features.7: 4449501184\n",
            "features.7.conv1: 924844032\n",
            "features.7.conv1.0: 905969664\n",
            "features.7.conv1.1: 18874368\n",
            "features.7.conv1.2: 0\n",
            "features.7.act1: 0\n",
            "features.7.act1.act: 0\n",
            "features.7.hyper1: 66060288\n",
            "features.7.hyper1.hyper: 66060288\n",
            "features.7.hyper1.hyper.0: 9437184\n",
            "features.7.hyper1.hyper.1: 0\n",
            "features.7.hyper1.hyper.2: 56623104\n",
            "features.7.hyper1.hyper.3: 0\n",
            "features.7.hyper1.hyper.3.relu: 0\n",
            "features.7.conv2: 103809024\n",
            "features.7.conv2.0: 84934656\n",
            "features.7.conv2.1: 18874368\n",
            "features.7.act2: 0\n",
            "features.7.act2.act: 0\n",
            "features.7.hyper2: 66060288\n",
            "features.7.hyper2.hyper: 66060288\n",
            "features.7.hyper2.hyper.0: 9437184\n",
            "features.7.hyper2.hyper.1: 0\n",
            "features.7.hyper2.hyper.2: 56623104\n",
            "features.7.hyper2.hyper.3: 0\n",
            "features.7.hyper2.hyper.3.relu: 0\n",
            "features.7.conv3: 1212153856\n",
            "features.7.conv3.0: 1207959552\n",
            "features.7.conv3.1: 4194304\n",
            "features.7.conv3.2: 0\n",
            "features.7.act3: 0\n",
            "features.7.act3.act: 0\n",
            "features.7.hyper3: 0\n",
            "features.7.drop_path: 0\n",
            "features.7.local_global: 364511232\n",
            "features.7.local_global.alpha: 113246208\n",
            "features.7.local_global.alpha.0: 113246208\n",
            "features.7.local_global.alpha.1: 0\n",
            "features.7.local_global.alpha.1.relu: 0\n",
            "features.7.local_global.q: 113246208\n",
            "features.7.local_global.proj: 113246208\n",
            "features.7.local_global.layer_norm: 5898240\n",
            "features.7.local_global.drop_path: 0\n",
            "features.7.global_block: 1384906752\n",
            "features.7.global_block.ffn: 905969664\n",
            "features.7.global_block.ffn.0: 452984832\n",
            "features.7.global_block.ffn.1: 0\n",
            "features.7.global_block.ffn.2: 452984832\n",
            "features.7.global_block.ffn_norm: 5898240\n",
            "features.7.global_block.q: 226492416\n",
            "features.7.global_block.channel_mlp: 226492416\n",
            "features.7.global_block.layer_norm: 5898240\n",
            "features.7.global_block.drop_path: 0\n",
            "features.7.global_local: 327155712\n",
            "features.7.global_local.k: 150994944\n",
            "features.7.global_local.proj: 150994944\n",
            "features.7.global_local.drop_path: 0\n",
            "features.8: 5754978304\n",
            "features.8.conv1: 1635778560\n",
            "features.8.conv1.0: 1610612736\n",
            "features.8.conv1.1: 25165824\n",
            "features.8.conv1.2: 0\n",
            "features.8.act1: 0\n",
            "features.8.act1.act: 0\n",
            "features.8.hyper1: 84934656\n",
            "features.8.hyper1.hyper: 84934656\n",
            "features.8.hyper1.hyper.0: 9437184\n",
            "features.8.hyper1.hyper.1: 0\n",
            "features.8.hyper1.hyper.2: 75497472\n",
            "features.8.hyper1.hyper.3: 0\n",
            "features.8.hyper1.hyper.3.relu: 0\n",
            "features.8.conv2: 138412032\n",
            "features.8.conv2.0: 113246208\n",
            "features.8.conv2.1: 25165824\n",
            "features.8.act2: 0\n",
            "features.8.act2.act: 0\n",
            "features.8.hyper2: 84934656\n",
            "features.8.hyper2.hyper: 84934656\n",
            "features.8.hyper2.hyper.0: 9437184\n",
            "features.8.hyper2.hyper.1: 0\n",
            "features.8.hyper2.hyper.2: 75497472\n",
            "features.8.hyper2.hyper.3: 0\n",
            "features.8.hyper2.hyper.3.relu: 0\n",
            "features.8.conv3: 1614807040\n",
            "features.8.conv3.0: 1610612736\n",
            "features.8.conv3.1: 4194304\n",
            "features.8.conv3.2: 0\n",
            "features.8.act3: 0\n",
            "features.8.act3.act: 0\n",
            "features.8.hyper3: 0\n",
            "features.8.drop_path: 0\n",
            "features.8.local_global: 484048896\n",
            "features.8.local_global.alpha: 150994944\n",
            "features.8.local_global.alpha.0: 150994944\n",
            "features.8.local_global.alpha.1: 0\n",
            "features.8.local_global.alpha.1.relu: 0\n",
            "features.8.local_global.q: 150994944\n",
            "features.8.local_global.proj: 150994944\n",
            "features.8.local_global.layer_norm: 5898240\n",
            "features.8.local_global.drop_path: 0\n",
            "features.8.global_block: 1384906752\n",
            "features.8.global_block.ffn: 905969664\n",
            "features.8.global_block.ffn.0: 452984832\n",
            "features.8.global_block.ffn.1: 0\n",
            "features.8.global_block.ffn.2: 452984832\n",
            "features.8.global_block.ffn_norm: 5898240\n",
            "features.8.global_block.q: 226492416\n",
            "features.8.global_block.channel_mlp: 226492416\n",
            "features.8.global_block.layer_norm: 5898240\n",
            "features.8.global_block.drop_path: 0\n",
            "features.8.global_local: 327155712\n",
            "features.8.global_local.k: 150994944\n",
            "features.8.global_local.proj: 150994944\n",
            "features.8.global_local.drop_path: 0\n",
            "features.9: 3781558272\n",
            "features.9.conv1: 34603008\n",
            "features.9.conv1.0: 28311552\n",
            "features.9.conv1.1: 6291456\n",
            "features.9.conv1.2: 0\n",
            "features.9.act1: 0\n",
            "features.9.act1.act: 0\n",
            "features.9.hyper1: 84934656\n",
            "features.9.hyper1.hyper: 84934656\n",
            "features.9.hyper1.hyper.0: 9437184\n",
            "features.9.hyper1.hyper.1: 0\n",
            "features.9.hyper1.hyper.2: 75497472\n",
            "features.9.hyper1.hyper.3: 0\n",
            "features.9.hyper1.hyper.3.relu: 0\n",
            "features.9.conv2: 605552640\n",
            "features.9.conv2.0: 603979776\n",
            "features.9.conv2.1: 1572864\n",
            "features.9.act2: 0\n",
            "features.9.act2.act: 0\n",
            "features.9.conv3: 34603008\n",
            "features.9.conv3.0: 28311552\n",
            "features.9.conv3.1: 6291456\n",
            "features.9.conv3.2: 0\n",
            "features.9.act3: 0\n",
            "features.9.act3.act: 0\n",
            "features.9.hyper3: 84934656\n",
            "features.9.hyper3.hyper: 84934656\n",
            "features.9.hyper3.hyper.0: 9437184\n",
            "features.9.hyper3.hyper.1: 0\n",
            "features.9.hyper3.hyper.2: 75497472\n",
            "features.9.hyper3.hyper.3: 0\n",
            "features.9.hyper3.hyper.3.relu: 0\n",
            "features.9.conv4: 605552640\n",
            "features.9.conv4.0: 603979776\n",
            "features.9.conv4.1: 1572864\n",
            "features.9.act4: 0\n",
            "features.9.act4.act: 0\n",
            "features.9.hyper4: 0\n",
            "features.9.drop_path: 0\n",
            "features.9.local_global: 484048896\n",
            "features.9.local_global.alpha: 150994944\n",
            "features.9.local_global.alpha.0: 150994944\n",
            "features.9.local_global.alpha.1: 0\n",
            "features.9.local_global.alpha.1.relu: 0\n",
            "features.9.local_global.q: 150994944\n",
            "features.9.local_global.proj: 150994944\n",
            "features.9.local_global.layer_norm: 5898240\n",
            "features.9.local_global.drop_path: 0\n",
            "features.9.global_block: 1384906752\n",
            "features.9.global_block.ffn: 905969664\n",
            "features.9.global_block.ffn.0: 452984832\n",
            "features.9.global_block.ffn.1: 0\n",
            "features.9.global_block.ffn.2: 452984832\n",
            "features.9.global_block.ffn_norm: 5898240\n",
            "features.9.global_block.q: 226492416\n",
            "features.9.global_block.channel_mlp: 226492416\n",
            "features.9.global_block.layer_norm: 5898240\n",
            "features.9.global_block.drop_path: 0\n",
            "features.9.global_local: 462422016\n",
            "features.9.global_local.k: 226492416\n",
            "features.9.global_local.proj: 226492416\n",
            "features.9.global_local.drop_path: 0\n",
            "features.10: 4662362112\n",
            "features.10.conv1: 915406848\n",
            "features.10.conv1.0: 905969664\n",
            "features.10.conv1.1: 9437184\n",
            "features.10.conv1.2: 0\n",
            "features.10.act1: 0\n",
            "features.10.act1.act: 0\n",
            "features.10.hyper1: 122683392\n",
            "features.10.hyper1.hyper: 122683392\n",
            "features.10.hyper1.hyper.0: 9437184\n",
            "features.10.hyper1.hyper.1: 0\n",
            "features.10.hyper1.hyper.2: 113246208\n",
            "features.10.hyper1.hyper.3: 0\n",
            "features.10.hyper1.hyper.3.relu: 0\n",
            "features.10.conv2: 51904512\n",
            "features.10.conv2.0: 42467328\n",
            "features.10.conv2.1: 9437184\n",
            "features.10.act2: 0\n",
            "features.10.act2.act: 0\n",
            "features.10.hyper2: 122683392\n",
            "features.10.hyper2.hyper: 122683392\n",
            "features.10.hyper2.hyper.0: 9437184\n",
            "features.10.hyper2.hyper.1: 0\n",
            "features.10.hyper2.hyper.2: 113246208\n",
            "features.10.hyper2.hyper.3: 0\n",
            "features.10.hyper2.hyper.3.relu: 0\n",
            "features.10.conv3: 907542528\n",
            "features.10.conv3.0: 905969664\n",
            "features.10.conv3.1: 1572864\n",
            "features.10.conv3.2: 0\n",
            "features.10.act3: 0\n",
            "features.10.act3.act: 0\n",
            "features.10.hyper3: 0\n",
            "features.10.drop_path: 0\n",
            "features.10.local_global: 694812672\n",
            "features.10.local_global.alpha: 226492416\n",
            "features.10.local_global.alpha.0: 226492416\n",
            "features.10.local_global.alpha.1: 0\n",
            "features.10.local_global.alpha.1.relu: 0\n",
            "features.10.local_global.q: 226492416\n",
            "features.10.local_global.proj: 226492416\n",
            "features.10.local_global.layer_norm: 5898240\n",
            "features.10.local_global.drop_path: 0\n",
            "features.10.global_block: 1384906752\n",
            "features.10.global_block.ffn: 905969664\n",
            "features.10.global_block.ffn.0: 452984832\n",
            "features.10.global_block.ffn.1: 0\n",
            "features.10.global_block.ffn.2: 452984832\n",
            "features.10.global_block.ffn_norm: 5898240\n",
            "features.10.global_block.q: 226492416\n",
            "features.10.global_block.channel_mlp: 226492416\n",
            "features.10.global_block.layer_norm: 5898240\n",
            "features.10.global_block.drop_path: 0\n",
            "features.10.global_local: 462422016\n",
            "features.10.global_local.k: 226492416\n",
            "features.10.global_local.proj: 226492416\n",
            "features.10.global_local.drop_path: 0\n",
            "features.11: 4662362112\n",
            "features.11.conv1: 915406848\n",
            "features.11.conv1.0: 905969664\n",
            "features.11.conv1.1: 9437184\n",
            "features.11.conv1.2: 0\n",
            "features.11.act1: 0\n",
            "features.11.act1.act: 0\n",
            "features.11.hyper1: 122683392\n",
            "features.11.hyper1.hyper: 122683392\n",
            "features.11.hyper1.hyper.0: 9437184\n",
            "features.11.hyper1.hyper.1: 0\n",
            "features.11.hyper1.hyper.2: 113246208\n",
            "features.11.hyper1.hyper.3: 0\n",
            "features.11.hyper1.hyper.3.relu: 0\n",
            "features.11.conv2: 51904512\n",
            "features.11.conv2.0: 42467328\n",
            "features.11.conv2.1: 9437184\n",
            "features.11.act2: 0\n",
            "features.11.act2.act: 0\n",
            "features.11.hyper2: 122683392\n",
            "features.11.hyper2.hyper: 122683392\n",
            "features.11.hyper2.hyper.0: 9437184\n",
            "features.11.hyper2.hyper.1: 0\n",
            "features.11.hyper2.hyper.2: 113246208\n",
            "features.11.hyper2.hyper.3: 0\n",
            "features.11.hyper2.hyper.3.relu: 0\n",
            "features.11.conv3: 907542528\n",
            "features.11.conv3.0: 905969664\n",
            "features.11.conv3.1: 1572864\n",
            "features.11.conv3.2: 0\n",
            "features.11.act3: 0\n",
            "features.11.act3.act: 0\n",
            "features.11.hyper3: 0\n",
            "features.11.drop_path: 0\n",
            "features.11.local_global: 694812672\n",
            "features.11.local_global.alpha: 226492416\n",
            "features.11.local_global.alpha.0: 226492416\n",
            "features.11.local_global.alpha.1: 0\n",
            "features.11.local_global.alpha.1.relu: 0\n",
            "features.11.local_global.q: 226492416\n",
            "features.11.local_global.proj: 226492416\n",
            "features.11.local_global.layer_norm: 5898240\n",
            "features.11.local_global.drop_path: 0\n",
            "features.11.global_block: 1384906752\n",
            "features.11.global_block.ffn: 905969664\n",
            "features.11.global_block.ffn.0: 452984832\n",
            "features.11.global_block.ffn.1: 0\n",
            "features.11.global_block.ffn.2: 452984832\n",
            "features.11.global_block.ffn_norm: 5898240\n",
            "features.11.global_block.q: 226492416\n",
            "features.11.global_block.channel_mlp: 226492416\n",
            "features.11.global_block.layer_norm: 5898240\n",
            "features.11.global_block.drop_path: 0\n",
            "features.11.global_local: 462422016\n",
            "features.11.global_local.k: 226492416\n",
            "features.11.global_local.proj: 226492416\n",
            "features.11.global_local.drop_path: 0\n",
            "local_global: 694812672\n",
            "local_global.alpha: 226492416\n",
            "local_global.alpha.0: 226492416\n",
            "local_global.alpha.1: 0\n",
            "local_global.alpha.1.relu: 0\n",
            "local_global.q: 226492416\n",
            "local_global.proj: 226492416\n",
            "local_global.layer_norm: 5898240\n",
            "local_global.drop_path: 0\n",
            "classifier: 5532549120\n",
            "classifier.conv: 915406848\n",
            "classifier.conv.0: 0\n",
            "classifier.conv.1: 905969664\n",
            "classifier.conv.2: 9437184\n",
            "classifier.act: 0\n",
            "classifier.act.act: 0\n",
            "classifier.hyper: 0\n",
            "classifier.avgpool: 4718592\n",
            "classifier.avgpool.0: 4718592\n",
            "classifier.avgpool.1: 0\n",
            "classifier.avgpool.1.sigmoid: 0\n",
            "classifier.avgpool.1.sigmoid.relu: 0\n",
            "classifier.fc: 2646343680\n",
            "classifier.fc.0: 2642411520\n",
            "classifier.fc.1: 3932160\n",
            "classifier.fc.2: 0\n",
            "classifier.fc.2.sigmoid: 0\n",
            "classifier.fc.2.sigmoid.relu: 0\n",
            "classifier.classifier: 1966080000\n",
            "classifier.classifier.0: 0\n",
            "classifier.classifier.1: 1966080000\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "test_loss = 0.0\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "\n",
        "test_loader_tqdm = tqdm(test_loader, desc=\"Test Progress\", leave=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader_tqdm:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct_test += predicted.eq(labels).sum().item()\n",
        "        total_test += labels.size(0)\n",
        "\n",
        "        test_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Top-1 Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "# FLOPs calculation\n",
        "dummy_input = torch.randn(test_batch_size, 3, 64, 64).to(device)\n",
        "\n",
        "flops = FlopCountAnalysis(model, dummy_input)\n",
        "print(f\"FLOPs: {flops.total()}\")\n",
        "\n",
        "# print FLOPs per layer\n",
        "for name, count in flops.by_module().items():\n",
        "    print(f\"{name}: {count}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}